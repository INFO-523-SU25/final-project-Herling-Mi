[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Audio Alchemy",
    "section": "",
    "text": "Music recommendation systems increasingly rely on machine learning to capture the complexity of user preferences, yet existing models struggle to account for language diversity and nuanced audio features in songs. This project applies signal processing, vocal separation (DEMUCS library), and machine learning techniques to develop a framework for classifying both music genres and song languages, integrating these predictions with genre metadata for improved personalization. By combining automated data collection with advanced audio analysis, the system provides a foundation for smarter, more inclusive recommendation platforms that enhance user experience across diverse musical contexts. The focus of the project was: langauge and genre recognition. For language recognition, classical models—including Logistic Regression, Random Forests, and SVMs—were trained on extracted statistical and time-frequency features using 5-fold cross-validation. These models showed modest predictive performance, with accuracy, precision, recall, and F1-scores generally ranging from 10–60%, while vocal features provided stronger signals than instrumental components. Next, KNNs and Random Forests were applied with ‘genre’ as the target variable. Finally, CNNs were applied to Mel spectrogram images—both grayscale and color scale—with train/validation/test splits, early stopping, and hyperparameter sweeps to capture complex audio patterns. While all models had limited performance, CNNs have strong theoretical potential, as reported in the literature, and improved recognition compared to classical models, highlighting the promise of deep learning and feature engineering for future music recommendation and language identification systems."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Audio Alchemy",
    "section": "Abstract",
    "text": "Abstract\nMusic recommendation systems increasingly rely on machine learning to capture the complexity of user preferences, yet existing models struggle to account for language diversity and nuanced audio features in songs. This project applies signal processing, vocal separation (DEMUCS library), and machine learning techniques to develop a framework for classifying both music genres and song languages, integrating these predictions with genre metadata for improved personalization. By combining automated data collection with advanced audio analysis, the system provides a foundation for smarter, more inclusive recommendation platforms that enhance user experience across diverse musical contexts. The focus of the project was: langauge and genre recognition. For language recognition, classical models—including Logistic Regression, Random Forests, and SVMs—were trained on extracted statistical and time-frequency features using 5-fold cross-validation. These models showed modest predictive performance, with accuracy, precision, recall, and F1-scores generally ranging from 10–60%, while vocal features provided stronger signals than instrumental components. Next, KNNs and Random Forests were applied with ‘genre’ as the target variable. Finally, CNNs were applied to Mel spectrogram images—both grayscale and color scale—with train/validation/test splits, early stopping, and hyperparameter sweeps to capture complex audio patterns. While all models had limited performance, CNNs have strong theoretical potential, as reported in the literature, and improved recognition compared to classical models, highlighting the promise of deep learning and feature engineering for future music recommendation and language identification systems."
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "",
    "text": "Our team is developing a machine learning system as part of a larger AI-driven music recommendation service. The primary objectives are to build a model capable of recognizing the language(s) spoken in audio files and assessing whether new songs align with a user’s preferences. Understanding spoken language within music tracks, combined with genre classification, enables more personalized and accurate recommendations. This capability is essential for enhancing user experience by suggesting songs that resonate with individual tastes.\nThe challenge lies in effectively processing real audio data to extract meaningful features, classify genres, and interpret user listening histories. By leveraging signal processing and machine learning techniques, this project aims to automate and improve the recommendation process. Such advancements not only deepen our understanding of audio analysis but also pave the way for smarter, more intuitive music discovery platforms."
  },
  {
    "objectID": "proposal.html#proposal",
    "href": "proposal.html#proposal",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "",
    "text": "Our team is developing a machine learning system as part of a larger AI-driven music recommendation service. The primary objectives are to build a model capable of recognizing the language(s) spoken in audio files and assessing whether new songs align with a user’s preferences. Understanding spoken language within music tracks, combined with genre classification, enables more personalized and accurate recommendations. This capability is essential for enhancing user experience by suggesting songs that resonate with individual tastes.\nThe challenge lies in effectively processing real audio data to extract meaningful features, classify genres, and interpret user listening histories. By leveraging signal processing and machine learning techniques, this project aims to automate and improve the recommendation process. Such advancements not only deepen our understanding of audio analysis but also pave the way for smarter, more intuitive music discovery platforms."
  },
  {
    "objectID": "proposal.html#python-libraries",
    "href": "proposal.html#python-libraries",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "Python libraries",
    "text": "Python libraries\n\nimport os\nimport json\nimport subprocess\nimport numpy as np\nimport pandas as pd\n\n# === Machine Learning & Evaluation ===\nimport sklearn  # Models, preprocessing, cross-validation, metrics\nimport lightgbm as lgb  # Gradient boosting\nimport xgboost as xgb  # Gradient boosting\n#import surprise  # Consider removing if problematic, see alternatives\n\n# === Deep Learning Frameworks ===\nimport torch  # PyTorch (used with Demucs, CNNs, etc.)\nimport tensorflow as tf  # TensorFlow\nfrom tensorflow import keras  # Keras API\n\n# === Audio Processing ===\nimport librosa  # Feature extraction (ZCR, RMS, tempo, etc.)\nimport torchaudio  # Audio I/O and transformations with PyTorch\nfrom demucs.apply import apply_model\nfrom demucs.pretrained import get_model  # Vocal separation\n\n# === Visualization ===\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# === Display & Formatting (for .qmd / Jupyter) ===\nfrom IPython.display import display, HTML\n\n\nVocal track separation with - DEMUCS\nDemucs (Deep Extractor for Music Sources) is an open-source tool developed by Meta AI, designed for high-quality music source separation. Originally introduced to outperform traditional spectral mask methods, it uses a deep learning architecture based on convolutional layers and bidirectional LSTM (Long Short-Term Memory) to separate audio into distinct stems like vocals, drums, bass, and others. Unlike classical techniques that rely on frequency-domain heuristics, Demucs operates directly in the time domain, enabling precise extraction of overlapping audio components. For multilingual analysis, Demucs is especially effective because it isolates vocals based purely on acoustic features — not linguistic content — making it an ideal front-end for tasks like spoken language identification or lyric classification in machine learning pipelines.\n\n\nMinimal scripts - Using System Terminal Commands - DEMUCS\nScripts that run demucs using system commands—typically through Python’s subprocess or os libraries—offer a straightforward way to integrate audio separation tools into Python workflows while interacting with the operating system’s file structure and command-line utilities.\n\nimport os\nimport subprocess\n\n# Path to your input audio file\naudio_file = r\"~\\Gloria_Gaynor_I_Will_Survive.wav\"\n\n# Optional: check if file exists\nif not os.path.exists(audio_file):\n    raise FileNotFoundError(f\"Audio file not found: {audio_file}\")\n\n# Build the Demucs command\n# You can change --two-stems to 'drums' or 'bass' if needed\ncommand = [\n    \"demucs\",\n    \"--two-stems=vocals\",  # Extract vocals only\n    \"--out\", \"demucs_output\",  # Output folder\n    audio_file\n]\n\n# Run the command\nprint(\"🔄 Running Demucs...\")\nsubprocess.run(command)\n\nprint(\"✅ Separation complete. Check the 'demucs_output' folder for results.\")"
  },
  {
    "objectID": "proposal.html#dataset",
    "href": "proposal.html#dataset",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "Dataset",
    "text": "Dataset\n\nData - Provenence\nAll audio data used in this project was sourced from YouTube, following an automated pipeline designed to gather relevant tracks for training and evaluation:\n\nA list of artist names was manually curated.\nFor each artist, their top five songs were identified by scraping Google search results.\nYouTube links corresponding to those songs were retrieved using a custom scraping script.\nAudio from each YouTube video was downloaded and stored for analysis.\n\nIn addition to audio collection, a secondary scraper was developed to gather metadata. This tool extracted the most commonly associated genres and countries of origin for each artist by querying publicly available sources.\n\n\nData - Collection\nThe data collection process involved several custom Python scripts designed to scrape and download the necessary information and audio files:\nartist_5_song_list_scrape.py — Retrieves the top five songs per artist from Google search results.\nartist_genre_scrape.py — Gathers genre metadata for each artist from public sources.\nartist_country_of_origin_scrape.py — Extracts the country of origin for each artist.\naudio_scrape_wav_mp3.py — Downloads audio files from YouTube links in WAV and MP3 formats.\nTogether, these scripts automate the extraction of both audio data and relevant metadata to support training and evaluation of the recommendation system.\n\n\nData - Description\nThis table provides a comprehensive overview of the audio and metadata features considered during project development. It details each feature’s definition, data type, output format, and extraction notes. While some features may not be ultimately used, the table serves as a complete reference of potential inputs for modeling musical and audio characteristics. All features requiring extraction have, at least prima facie, been verified against available Python libraries capable of performing the extraction.\n\n\n\n\n\n\n\nTable 1: Summary of Audio Features (Simplified)\n\n\n\n\n\n \nFeature\nDefinition\nVariable Type\nOutput Type\nNotes\n\n\n\n\n0\nArtist\nName of the performing artist or band.\nString\nSingle string\nUsually human entered or scraped from metadata.\n\n\n1\nSong Title\nTitle of the song or track.\nString\nSingle string\nUsually human entered or scraped from metadata.\n\n\n2\nGenre\nMusical style or category (may be multiple).\nString Array\nList of strings\nOften human-assigned or from databases.\n\n\n3\nMean\nAverage value of an audio feature over time.\nNumeric\nSingle number\nUse numpy/scipy on time-series features.\n\n\n4\nVariance\nSpread or variability of the feature.\nNumeric\nSingle number\nUse numpy.var() on time-varying data.\n\n\n5\nSkewness\nAsymmetry of the feature distribution.\nNumeric\nSingle number\nUse scipy.stats.skew() on feature arrays.\n\n\n6\nKurtosis\nHeaviness of tails in the feature distribution.\nNumeric\nSingle number\nUse scipy.stats.kurtosis() for tail behavior.\n\n\n7\nZero Crossing Rate\nRate at which waveform crosses zero amplitude.\nNumeric\nSingle number\nlibrosa.feature.zero_crossing_rate().\n\n\n8\nRMS Energy\nRoot mean square of amplitude (loudness).\nNumeric\nSingle number\nlibrosa.feature.rms() or similar.\n\n\n9\nLoudness\nPerceived loudness (in dB).\nNumeric\nSingle number\nEstimate via RMS or pydub gain.\n\n\n10\nEnergy\nTotal signal energy over time.\nNumeric\nSingle number\nMean RMS squared or signal power.\n\n\n11\nTempo\nBeats per minute (BPM).\nNumeric\nSingle number\nlibrosa.beat.beat_track().\n\n\n12\nDanceability\nSuitability of the track for dancing.\nNumeric\nApproximate\nEstimated with ML or Essentia.\n\n\n13\nKey / Key Name\nMusical key (e.g., C, F#, A minor).\nCategorical\nSingle string\nFrom chroma feature analysis.\n\n\n14\nMode / Mode Name\nTonality: major or minor.\nCategorical\nSingle string\nDerived from key/chroma analysis.\n\n\n15\nFFT\nAmplitude vs frequency via FFT.\nNumeric Array\n1D Array\nnp.fft.fft() + abs().\n\n\n16\nSTFT\nShort-Time Fourier Transform (time-frequency).\nComplex Array\n2D Matrix\nlibrosa.stft().\n\n\n17\nMel-Spectrogram\nMel-scaled spectrogram representation.\nNumeric Array\n2D Matrix\nlibrosa.feature.melspectrogram().\n\n\n18\nFreq vs dB Spectrum\nSpectrum in decibel scale.\nNumeric Array\n1D/2D Matrix\nlibrosa.power_to_db() on FFT or STFT.\n\n\n19\nDuration\nLength of audio in milliseconds.\nNumeric\nSingle number\nlibrosa.get_duration() * 1000.\n\n\n\n\n\n\n\n\n\n\nMachine Learning Nomenclature\nThis table defines key terms related to the concept of “tokens” in the context of music machine learning. It clarifies how raw audio features, sequences, and categorical metadata can be represented as tokens for model input. Understanding these distinctions is essential for designing effective audio classification and recommendation systems.\n\n\n\n\n\n\n\nTable 2: Token Definitions in Music ML Context\n\n\n\n\n\n \nTerm\nExample token in your project\n\n\n\n\n0\nToken\nSingle time-window feature vector or metadata category\n\n\n1\nSequence\nOrdered series of audio feature vectors (like frames in time)\n\n\n2\nVocabulary\nSet of all possible genres, artists, keys, or discretized audio features\n\n\n\n\n\n\n\n\n\n\nData - Storage\nThe example JSON structure below represents a data entry for an artist and their song metadata, including audio file links, user feedback options, and categorical tags. This format organizes information for easy ingestion by machine learning pipelines or user interfaces, capturing both objective metadata (artist, categories, audio features) and subjective user ratings using Likert scales of varying lengths.\nAudio files and any complex metadata such as multi-dimensional arrays (e.g., spectrograms) will be stored in separate folders, with their file paths referenced in the JSON under the AudioFile node.\nJSON Node Storage Categories\nIn hierarchical data structures like JSON, a node represents a single element in the tree. Nodes can be categorized by the type of data they store and their structural role. Here is a list of the strucured schemea in our JSON:\nAudioFile: Nested object containing lists of links to YouTube, WAV, and MP3 versions of the audio files.\nRegion, User: Lists to hold geographic metadata and user identifiers. The User field, along with the Likert rating arrays, are pre-built placeholders designed to support future features such as user accounts and the collection of personalized likeability or preference ratings.\nData: A comprehensive sub-node capturing detailed metadata about the audio, including:\n\nArtist and song title\nGenre(s)\nQuantitative audio features such as mean, variance, skewness, kurtosis, zero crossing rate, RMS energy, loudness, energy, tempo, danceability\nMusical attributes like key and mode\nComplex audio representations like FFT, STFT, mel-spectrogram, frequency vs dB spectrum\nDuration in milliseconds\n\nLikert_2, Likert_3, Likert_5: Arrays representing different Likert scales, each with entries for score, label, color, description, and selection status to capture nuanced user feedback.\ncategory: An array of genre tags associated with the song.\nThis JSON schema is designed to be flexible and extensible, accommodating rich metadata and user feedback for building and improving recommendation systems. Below is a partially filled Node, with all schema present.\n\n\n\n\n{\n    \"AudioFile\": {\n        \"yt_link\": [],\n        \"wav_link\": [],\n        \"mp3_link\": []\n    },\n    \"Region\": [\n        \"America\"\n    ],\n    \"User\": [\n        \"Nathan\"\n    ],\n    \"Data\": {\n        \"Artist\": \"Lady Gaga\",\n        \"Song Title\": \"\",\n        \"Genre\": [],\n        \"Mean (of features)\": null,\n        \"Variance\": null,\n        \"Skewness\": null,\n        \"Kurtosis\": null,\n        \"Zero Crossing Rate\": null,\n        \"RMS Energy\": null,\n        \"Loudness\": null,\n        \"Energy\": null,\n        \"Tempo\": null,\n        \"Danceability\": null,\n        \"Key / Key Name\": \"\",\n        \"Mode / Mode Name\": \"\",\n        \"FFT (Amplitude vs Frequency)\": null,\n        \"STFT (Short-Time Fourier Transform)\": null,\n        \"Mel-Spectrogram\": null,\n        \"Frequency vs dB Spectrum\": null,\n        \"Duration (ms)\": null\n    },\n    \"Likert_2\": [\n        {\n            \"score\": 1,\n            \"label\": \"No\",\n            \"color\": \"#FF4C4C\",\n            \"selected\": true\n        },\n        {\n            \"score\": 2,\n            \"label\": \"Yes\",\n            \"color\": \"#4CAF50\",\n            \"selected\": false\n        }\n    ],\n    \"Likert_3\": [\n        {\n            \"score\": 1,\n            \"label\": \"Dislike\",\n            \"description\": \"I do not like this\",\n            \"color\": \"#FF4C4C\",\n            \"selected\": false\n        },\n        {\n            \"score\": 2,\n            \"label\": \"Meh\",\n            \"description\": \"Neutral or indifferent\",\n            \"color\": \"#FFD700\",\n            \"selected\": true\n        },\n        {\n            \"score\": 3,\n            \"label\": \"Like\",\n            \"description\": \"I like this\",\n            \"color\": \"#4CAF50\",\n            \"selected\": false\n        }\n    ],\n    \"Likert_5\": [\n        {\n            \"score\": 1,\n            \"label\": \"Strongly Dislike\",\n            \"description\": \"I strongly dislike this genre/song\",\n            \"color\": \"#FF4C4C\",\n            \"selected\": false\n        },\n        {\n            \"score\": 2,\n            \"label\": \"Dislike\",\n            \"description\": \"I don\\u2019t enjoy this genre/song\",\n            \"color\": \"#FF8C00\",\n            \"selected\": false\n        },\n        {\n            \"score\": 3,\n            \"label\": \"Neutral\",\n            \"description\": \"Neither like nor dislike\",\n            \"color\": \"#FFD700\",\n            \"selected\": true\n        },\n        {\n            \"score\": 4,\n            \"label\": \"Like\",\n            \"description\": \"I like this genre/song\",\n            \"color\": \"#90EE90\",\n            \"selected\": false\n        },\n        {\n            \"score\": 5,\n            \"label\": \"Strongly Like\",\n            \"description\": \"I strongly like or love this genre/song\",\n            \"color\": \"#008000\",\n            \"selected\": false\n        }\n    ],\n    \"category\": [\n        \"rock\",\n        \"pop\",\n        \"electronic pop\",\n        \"jazz pop\"\n    ]\n}\n\n\n\n\n\nLikert Scale Feature - indepth explanation\nThe Likert score will most likely be implemented in the second phase of the project. It was included in the initial design of the .JSON file with the idea that incorporating it early would enable potential use in the current phase and support software design aligned with future phases or iterations of the project. The intended use of the Likert score is as a UI/UX feature for users. The initial approach will involve encoding the Likert scores and using them as input features, rather than as target variables. The machine learning component will aim to provide a gradient of score values, allowing users to express preferences (likes/dislikes) with greater precision. The goal is to develop a model capable of handling varying Likert scale gradients and their corresponding precision for more accurate recommendations..\nIn our design, each user will have their own Likert score database (or user-specific rating entries) linked to their unique user ID. This ensures that preference modeling and recommendation outputs are truly personalized. The idea is to allow users to “score” their like or dislike of individual songs and then see how the recommendation model adapts to those inputs. To make the interaction more engaging, users can choose between a 2-, 3-, or 5-point Likert scale.\n\n\nData - Second Tier features\nThe second tier of data features will focus on vocal track extraction, utilizing the DEMUCS library. Currently, the plan is to develop a dedicated script that processes each song’s audio files—both .mp3 and .wav formats—and stores the resulting vocal isolation data in individual, well-organized folders. As additional processing requirements emerge, the structure of the master JSON file may need to be adapted or reorganized to accommodate these new data components seamlessly. This approach ensures flexibility and scalability in handling complex audio feature sets while maintaining clear data management practices.\n\n\nThe Audiophiles custom audiofile ui\nA custom audio player UI that supports .wav and .mp3 files and dynamically renders decibel versus frequency spectrograms for each song. The player features real-time analysis with frequency, time, and dB spectrogram visualizations synchronized to the playback - i.e., a scrolling frequency,time,dB heat map. Additionally, there may be a need to build a JSON reader to facilitate the processing and aggregation of Likert scale scores associated with the songs.\n\n\n\n\n\nFigure 1: ‘Mk_1’ - Custom audio player UI displaying real-time frequency, time, and dB spectrograms synchronized to playback.\n\n\nAdditional Script info, Data Storage, Data Extraction:\nAll additional processing tasks are handled via custom Python scripts. These scripts include tools for downloading .mp3 and .mp4 files, and will be maintained individually by each team member. Due to the volume of data involved—approximately 200 .wav files and 200 .mp3 files—these audio assets will be stored locally on each user’s machine and not uploaded to GitHub.\nOnce metadata has been successfully extracted and organized, it can be safely stored and versioned within the GitHub repository. Similarly, any machine learning models developed throughout the project will be saved in the GitHub repo for reproducibility and collaboration.\nScripts for metadata extraction will be developed by Nathan and distributed to the rest of the team. Additional scripts and processing pipelines required for addressing specific research tasks will be created by individual team members as needed."
  },
  {
    "objectID": "proposal.html#individual-duties",
    "href": "proposal.html#individual-duties",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "Individual Duties",
    "text": "Individual Duties\nNathan – Problem #2: User Song Recommendation\n\nMetadata JSON Schema Design\nDesigned and implemented the nested .json structure to store song metadata, audio links, user ratings, and extracted audio features.\nMetadata Extraction Scripts\nResponsible for writing and maintaining Python scripts to:\n\nScrape artist genres, countries of origin, and song lists\n\nRetrieve YouTube links\n\nOrganize metadata for ingestion and storage\n\nAudio Visualization Interface\nDeveloped a custom .wav/.mp3 player with:\n\nReal-time frequency vs. dB spectrograms\n\nStatic visualizations for spectral energy distribution\n\nInteractive time-frequency-dB spectrograms during playback\n\nDemucs Integration and Vocal Separation\nWriting scripts to:\n\nApply the Demucs library for isolating vocal tracks\n\nStore separated .wav and .mp3 files in structured folders for analysis\n\nUser Interface & Data Display\nBuilding HTML/Quarto-styled displays for rendering .json metadata and user feedback in a human-readable format.\n\n\nYashi – Problem #1: Language Recognition from Audio\n\nMachine Learning Models\nLeading development of:\n\nLanguage classification models using vocal segments\n\nPipelines for training, validation, and testing\n\nAudio Feature Extraction\nResponsible for extracting statistical features such as:\n\nZero Crossing Rate (ZCR), Root Mean Square (RMS), tempo, and FFT\n\nTransforming raw waveform data into usable feature vectors\n\nRecommendation System Components\nAssisting in the design of:\n\nFeature-based comparison systems for future personalized recommendations\n\nMethods for encoding user preferences and behavior patterns\n\nModel Evaluation and Testing\nConducting:\n\nAccuracy and performance evaluations of models\n\nError analysis using tools such as confusion matrices and cross-validation\n\n\n\nJoint Responsibilities\nBoth team members will independently construct their machine learning pipelines. Each pipeline will be trained, tested, evaluated, and iteratively improved based on problem-specific goals. Coordination will ensure that feature extraction and data preprocessing remain compatible across both tasks.\n\n🗂️ Workflow Plan (Final 2.25 Weeks)\n\n\n\n\n\n\n\n\nPhase\nDates\nTasks\n\n\n\n\nPhase 1: Script Finalization & Distribution\nAug 1 – Aug 3\n- Finalize all data scraping & audio processing scripts- Distribute scripts to team members- Confirm runtime & environment setup\n\n\nPhase 2: Data Collection & Organization\nAug 4 – Aug 6\n- Each user runs scripts locally- Collect ~200 .mp3 and .wav files per user- Store audio and metadata in standardized folder structure\n\n\nPhase 3: Metadata Processing\nAug 7 – Aug 8\n- Parse, clean, and validate .json metadata- Integrate new entries into master metadata files- Ensure feature coverage (e.g., genre, tempo, duration)\n\n\nPhase 4: ML Pipeline Construction\nAug 9 – Aug 10\n- Each user builds their custom ML pipeline- Define preprocessing, feature extraction, model architecture\n\n\nPhase 5: ML Testing & Iteration – Round 1\nAug 11 – Aug 13\n- Run training and validation pipelines- Tune hyperparameters- Log and assess intermediate results\n\n\nPhase 6: ML Testing & Iteration – Round 2\nAug 14 – Aug 16\n- Refine pipelines based on feedback- Add second-tier features (e.g., vocal-only inputs)- Evaluate early model performance\n\n\nPhase 7: Final Evaluation & Model Selection\nAug 17 – Aug 18\n- Select best models per user task- Create evaluation reports- Generate confusion matrices, ROC curves, etc.\n\n\nPhase 8: Write-Up & Presentation Prep\nAug 19 – Aug 21\n- Complete final Quarto write-up- Polish visualizations and tables- Build and rehearse project presentation"
  },
  {
    "objectID": "proposal.html#questions",
    "href": "proposal.html#questions",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "Questions",
    "text": "Questions\n\n1. Language Recognition with Separated Vocal & Audio Tracks\nHow can we leverage statistical and time-frequency features extracted from separated vocal and audio tracks to build effective language recognition models? Specifically, how can traditional machine learning methods — ranging from classical classifiers on simple statistical summaries to Gaussian Mixture Models on richer time-frequency features — be applied in this context?\n\nWhat are the key benefits and limitations of these approaches?\n\nHow can careful feature engineering, feature integration, and thorough model evaluation improve the accuracy and robustness of language recognition systems?\n\nHow do model results compare and contrast when using .wav files versus .mp3 files?\n\n\n\n\n2. Recommendation Systems Using Audio Features & User Data\nHow can user interaction data, combined with basic track metadata and simple audio features, be used to build an effective recommendation system using collaborative filtering and traditional machine learning methods?\n\nFurthermore, how can advanced audio features, dimensionality reduction, and clustering techniques improve personalized recommendations by better capturing user preferences and track characteristics from both vocal and non-vocal components?\n\nHow do recommendation model results compare and contrast when using .wav files versus .mp3 files, considering the potential impact of audio quality and compression artifacts on feature extraction and recommendation performance?"
  },
  {
    "objectID": "proposal.html#analysis-plan",
    "href": "proposal.html#analysis-plan",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "Analysis plan",
    "text": "Analysis plan\n\nPreamble: Easy and Medium Paths\nThe Easy Path serves as a minimal, foundational implementation aimed at quickly establishing a baseline for language recognition using straightforward statistical features extracted from separated vocal and audio tracks. It relies on classical machine learning models such as Logistic Regression, Random Forest, and Support Vector Machines, which are simple to train and interpret.\nThe Medium Path provides a more detailed approach that extends beyond simple statistics by incorporating time-frequency features such as Mel-Frequency Cepstral Coefficients (MFCCs), spectral centroid, and bandwidth from both vocal and non-vocal tracks. Instead of deep learning, this path uses classical probabilistic models like Gaussian Mixture Models (GMMs), Hidden Markov Models (HMMs), or advanced classical classifiers trained on aggregated time-frequency features. This allows capturing richer audio characteristics while maintaining interpretability and computational efficiency.\n\n\nAnalysis Plan for Problem 1: Language Recognition\n\n\n1. Data Preparation & Feature Extraction - problem 1\n\nLoad separated vocal and instrumental audio tracks for each sample in both .wav and .mp3 formats.\n\nEasy Path: Extract statistical features (mean, variance, skewness, kurtosis, RMS energy, zero crossing rate, tempo, loudness) separately from vocal and audio tracks.\n\nMedium Path: Extract time-frequency features such as MFCCs, spectral centroid, bandwidth, or STFT for vocal and audio tracks. Aggregate these features by computing summary statistics (mean, variance).\n\nNormalize numerical features (StandardScaler or Min-Max scaling).\n\nEncode categorical metadata if available (e.g., one-hot encoding for language labels or artist).\n\n\n\n2. Model Construction & Training - problem 1\n\nEasy Path: Train classical classifiers — Logistic Regression, Random Forest, Support Vector Machines — on statistical feature vectors.\n\nMedium Path:\n\nApply K-Means clustering on aggregated time-frequency features to group similar audio patterns unsupervised, enhancing feature representation.\n\nTrain classical models suited for time-frequency data — Gaussian Mixture Models (GMMs), Hidden Markov Models (HMMs) (optional), or classical classifiers on combined original and cluster-based features.\n\n\nPerform hyperparameter tuning via grid or random search where applicable.\n\n\n\n3. Validation - problem 1\n\nUse stratified K-fold cross-validation to ensure balanced representation of languages in training and test splits.\n\nEvaluate model performance on validation folds.\n\n\n\n4. Performance Evaluation - problem 1\n\nMetrics: Accuracy, Precision, Recall, F1-score.\n\nGenerate confusion matrices to analyze language-specific errors.\n\nConduct ablation studies comparing vocal-only, audio-only, and combined vocal + audio features.\n\nCompare model performance and feature extraction quality between .wav and .mp3 audio formats to assess the impact of audio compression and quality differences on recognition accuracy.\n\n\n\nPutative Machine Learning Techniques - problem 1\n\nLogistic Regression, Random Forest, Support Vector Machines (SVM)\n\nGaussian Mixture Models (GMM), Hidden Markov Models (HMM) (optional)\n\nK-Means Clustering (unsupervised feature grouping)\n\nFeature normalization/scaling (StandardScaler, Min-Max)\n\nEncoding categorical features (One-hot, Label Encoding)\n\nStratified K-Fold cross-validation\n\nHyperparameter tuning (Grid Search, Random Search)\n\nEvaluation metrics: Accuracy, Precision, Recall, F1-score, Confusion Matrix\n\nAblation analysis for feature contribution\n\n\n\n\nAnalysis Plan for Problem 2: User Song Recommendation\n\n\n1. Data Preparation & Feature Extraction - problem 2\n\nLoad user interaction data combined with track metadata (artist, genre, audio features) in both .wav and .mp3 formats.\n\nEncode categorical metadata (one-hot, label encoding, or embeddings).\n\nNormalize numerical features (Min-Max scaling, StandardScaler).\n\nConstruct user-item interaction matrix from implicit feedback or ratings.\n\n\n\n2. Model Construction - problem 2\n\nEasy Path: Collaborative filtering using K-Nearest Neighbors or Logistic Regression leveraging metadata and user preferences.\n\nMedium Path:\n\nUse K-Means clustering or Hierarchical clustering on track features to identify similar groups of songs or users unsupervised.\n\nBuild tree-based classifiers (Random Forest, Gradient Boosting Machines) on clustered feature groups for content-based filtering.\n\n\n\n\n3. Training & Validation - problem 2\n\nTrain-Test splits or Stratified K-Fold cross-validation.\n\nHyperparameter tuning via grid or random search.\n\n\n\n4. Performance Evaluation - problem 2\n\nMetrics: Precision@K, Recall@K, Accuracy, F1-score.\n\nOffline validation on held-out test sets.\n\nAnalyze recommendation relevance and diversity.\n\nPerform ablation studies comparing models built with vocal-only features vs combined vocal + audio features.\n\nCompare recommendation system performance using features extracted from .wav versus .mp3 files to understand the effect of audio quality and compression artifacts.\n\n\n\nPutative Machine Learning Techniques - problem 2\n\nCollaborative Filtering: K-Nearest Neighbors, Logistic Regression for implicit feedback\n\nContent-Based Filtering: K-Means, Hierarchical Clustering (unsupervised grouping), Random Forest, Gradient Boosting Machines\n\nCross-validation: Train-Test Split, Stratified K-Fold CV\n\nFeature Engineering: One-hot encoding, embeddings, normalization (Min-Max, StandardScaler)\n\nEvaluation Metrics: Precision@K, Recall@K, Accuracy, F1-score\n\nAblation Analysis: Assess impact of vocal vs non-vocal feature inclusion"
  },
  {
    "objectID": "proposal.html#repo-oraganization",
    "href": "proposal.html#repo-oraganization",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "Repo Oraganization",
    "text": "Repo Oraganization\n\n_extra: Houses supplementary project materials such as problem statements, ML library documentation, and feature lists. Serves as a flexible space for reference materials.\n\n0_problem_statements：Contains structured descriptions of the project’s problem statements, including tiered Easy/Medium/Hard pipelines.\nmL_lib_info：Holds reference documents describing the machine learning techniques considered for the project.\naudio_features_mk1.csv：A CSV file listing all audio features to be extracted, including definitions, variable types, and extraction methods.\ncode.qmd：A Quarto markdown file containing core code and documentation for the project.\nexample_web_site.md: Example documentation for a project website setup.\nog_data_vars_defs.py: Python definitions for handling and processing project data variables.\nREADME.md: Main project overview, setup instructions, and usage guidelines.\n\n_freeze: Contains frozen Quarto document builds for reproducibility, organized by document (about, index, presentation, proposal).\ngithub: Contains GitHub-specific configuration, including:\n\nISSUE_TEMPLATE: Templates for creating consistent GitHub issues.\nworkflows: GitHub Actions workflows for automation (e.g., building Quarto site).\n\ndata: Houses datasets and associated documentation:\n\ncustomtheming.scss: Custom SCSS styling for the Quarto output.\nREADME.md: Data usage description.\n\ndocs: Contains rendered Quarto output for deployment (e.g., GitHub Pages):\n\n_extra: Supplementary files included in documentation.\nsite_libs: JavaScript and CSS libraries for the generated site.\nindex.html: Rendered project index page.\nproposal.html: Rendered proposal page.\nsearch.json: Search index for the site.\n\nimages：Contains project image assets, including visualizations, diagrams, and decorative images for presentation\npresentation_files：Stores materials supporting the final presentation, such as figures and supplementary assets.\ngitignore：Specifies files and folders to be excluded from Git version control.\nabout.qmd：A Quarto document providing background on the project purpose and introducing team members.\nindex.qmd：The main Quarto page for the project, containing the core narrative, methodology, code, visualizations, and results.\npresentation.qmd：A Quarto file containing the final presentation slides for the project results.\nproposal.qmd：A Quarto file for the project proposal, including dataset descriptions, problem statements, analysis plan, and weekly plan.\nREADME.md：The main project README file, summarizing project objectives, setup instructions, and usage guidelines.\n\n\n\n\n\n\n\nNote\n\n\n\nNote: Current Proposal 1.0.0. Subject to change.\n(version.feature.patch notation)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project was developed by The AudioPhiles For INFO 523 - Data Mining and Discovery at the University of Arizona, taught by Dr. Greg Chism. The team is comprised of the following team members.\n\nTeam member 1: Yashi Mi - Data Science MS student - year 1.\nTeam member 2: Nathan Herling - Data Science MS student - year 1."
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Project title",
    "section": "",
    "text": "The presentation is created using the Quarto CLI\n## sets the start of a new slide\n\n\n\n\nYou can use plain text\n\n\n\nor bullet points1\n\n\nor in two columns\n\n\n\nlike\nthis\n\n\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.073\nModel:                            OLS   Adj. R-squared:                  0.070\nMethod:                 Least Squares   F-statistic:                     30.59\nDate:                Mon, 28 Jul 2025   Prob (F-statistic):           5.84e-08\nTime:                        07:49:21   Log-Likelihood:                -1346.4\nNo. Observations:                 392   AIC:                             2697.\nDf Residuals:                     390   BIC:                             2705.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         35.8015      2.266     15.800      0.000      31.347      40.257\nspeed       -354.7055     64.129     -5.531      0.000    -480.788    -228.623\n==============================================================================\nOmnibus:                       27.687   Durbin-Watson:                   0.589\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               18.976\nSkew:                           0.420   Prob(JB):                     7.57e-05\nKurtosis:                       2.323   Cond. No.                         169.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome text\ngoes here"
  },
  {
    "objectID": "presentation.html#quarto",
    "href": "presentation.html#quarto",
    "title": "Using Quarto for presentations",
    "section": "Quarto",
    "text": "Quarto\n\nThe presentation is created using the Quarto CLI\n## sets the start of a new slide"
  },
  {
    "objectID": "presentation.html#layouts",
    "href": "presentation.html#layouts",
    "title": "Using Quarto for presentations",
    "section": "Layouts",
    "text": "Layouts\nYou can use plain text\n\n\n\nor bullet points1\n\n\nor in two columns\n\n\nlike\nthis\n\nAnd add footnotes"
  },
  {
    "objectID": "presentation.html#code",
    "href": "presentation.html#code",
    "title": "Using Quarto for presentations",
    "section": "Code",
    "text": "Code\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.073\nModel:                            OLS   Adj. R-squared:                  0.070\nMethod:                 Least Squares   F-statistic:                     30.59\nDate:                Mon, 18 Aug 2025   Prob (F-statistic):           5.84e-08\nTime:                        08:56:08   Log-Likelihood:                -1346.4\nNo. Observations:                 392   AIC:                             2697.\nDf Residuals:                     390   BIC:                             2705.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         35.8015      2.266     15.800      0.000      31.347      40.257\nspeed       -354.7055     64.129     -5.531      0.000    -480.788    -228.623\n==============================================================================\nOmnibus:                       27.687   Durbin-Watson:                   0.589\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               18.976\nSkew:                           0.420   Prob(JB):                     7.57e-05\nKurtosis:                       2.323   Cond. No.                         169.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "presentation.html#plot-and-text",
    "href": "presentation.html#plot-and-text",
    "title": "Using Quarto for presentations",
    "section": "Plot and text",
    "text": "Plot and text\n\n\n\nSome text\ngoes here"
  },
  {
    "objectID": "presentation.html#tables",
    "href": "presentation.html#tables",
    "title": "Using Quarto for presentations",
    "section": "Tables",
    "text": "Tables\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\n\n\n\nisland\n\n\n\nbill_length_mm\n\n\n\nbill_depth_mm\n\n\n\nflipper_length_mm\n\n\n\nbody_mass_g\n\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.1\n\n\n\n18.7\n\n\n\n181.0\n\n\n\n3750.0\n\n\n\nMale\n\n\n\n\n\n\n\n1\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.5\n\n\n\n17.4\n\n\n\n186.0\n\n\n\n3800.0\n\n\n\nFemale\n\n\n\n\n\n\n\n2\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n40.3\n\n\n\n18.0\n\n\n\n195.0\n\n\n\n3250.0\n\n\n\nFemale\n\n\n\n\n\n\n\n4\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n36.7\n\n\n\n19.3\n\n\n\n193.0\n\n\n\n3450.0\n\n\n\nFemale\n\n\n\n\n\n\n\n5\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.3\n\n\n\n20.6\n\n\n\n190.0\n\n\n\n3650.0\n\n\n\nMale"
  },
  {
    "objectID": "presentation.html#images",
    "href": "presentation.html#images",
    "title": "Using Quarto for presentations",
    "section": "Images",
    "text": "Images\n\nImage credit: Danielle Navarro, Percolate."
  },
  {
    "objectID": "presentation.html#math-expressions",
    "href": "presentation.html#math-expressions",
    "title": "Using Quarto for presentations",
    "section": "Math Expressions",
    "text": "Math Expressions\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g. $\\alpha+\\beta$ renders \\(\\alpha + \\beta\\). You can use the display style with double dollar signs:\n$$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n\\[\n\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\]\nLimitations:\n\nThe source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting $$ must appear in the very beginning of a line, followed immediately by a non-space character, and the ending $$ must be at the end of a line, led by a non-space character;\nThere should not be spaces after the opening $ or before the closing $."
  },
  {
    "objectID": "presentation.html#feeling-adventurous",
    "href": "presentation.html#feeling-adventurous",
    "title": "Using Quarto for presentations",
    "section": "Feeling adventurous?",
    "text": "Feeling adventurous?\n\nYou are welcomed to use the default styling of the slides. In fact, that’s what I expect majority of you will do. You will differentiate yourself with the content of your presentation.\nBut some of you might want to play around with slide styling. Some solutions for this can be found at https://quarto.org/docs/presentations/revealjs."
  },
  {
    "objectID": "presentation.html#footnotes",
    "href": "presentation.html#footnotes",
    "title": "Project title",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd add footnotes↩︎"
  },
  {
    "objectID": "presentation.html#plots",
    "href": "presentation.html#plots",
    "title": "Using Quarto for presentations",
    "section": "Plots",
    "text": "Plots"
  },
  {
    "objectID": "Yashi analysis.html",
    "href": "Yashi analysis.html",
    "title": "Load the dataset",
    "section": "",
    "text": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from pandas.plotting import scatter_matrix\nfrom sklearn.impute import SimpleImputer from sklearn.preprocessing import LabelEncoder, StandardScaler from sklearn.feature_selection import SelectKBest, f_regression from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression, Ridge, RidgeCV, LassoCV from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, make_scorer, precision_score, recall_score, f1_score\n\nLoad the dataset\ndf = pd.read_csv(“_extra/Audio_Files/Yashi_s_Music/Y_audio_philes_final_features_mk1_filled_Yashi.csv”)\nprint(df.head()) print(df.info()) print(“Missing values per column:”) print(df.isnull().sum())\n\n\nFeature selection\ndf_non_feature = [“artist”, “country”, “language”, “track_type”] df_number_feature = [c for c in df.select_dtypes(include=[np.number]).columns if c not in df_non_feature] print(“Numerical features used:”, df_number_feature)\n\n\nStandardize numerical features globally\nscaler = StandardScaler() df[df_number_feature] = scaler.fit_transform(df[df_number_feature])\n\n\nEncode target variable\nle = LabelEncoder() y_all = le.fit_transform(df[“language”].astype(str)) label_names = le.classes_\n\n\nDefine track_type groups\ntrack_type_all = {0: “complete_song”, 1: “vocal_only”, 2: “no_vocal”} track_type_specs = {name: (df[“track_type”] == code) for code, name in track_type_all.items()} for name, mask in track_type_specs.items(): print(f”{name}: {int(mask.sum())} samples”)\n\n\nSet up Stratified K-Fold cross-validation\nk = 5 skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n\n\nDefine models\nmodels = { “LogReg”: LogisticRegression(max_iter=500, class_weight=“balanced”, random_state=42), “RandomForest”: RandomForestClassifier( n_estimators=400, class_weight=“balanced_subsample”, random_state=42, n_jobs=-1 ), “SVM_linear”: SVC(kernel=“linear”, class_weight=“balanced”, random_state=42), }\n\n\nDefine scoring dictionary with exact metric names\nscoring = { “accuracy”: make_scorer(accuracy_score), “precision”: make_scorer(precision_score, average=“macro”), “recall”: make_scorer(recall_score, average=“macro”), “f1”: make_scorer(f1_score, average=“macro”), }\nrows = []\n\n\nModel evaluation loop\nfor track_type_code, ablation_name in track_type_all.items(): mask = df[“track_type”] == track_type_code df_eval = df.loc[mask].reset_index(drop=True)\nX = df_eval[df_number_feature].copy()\ny = le.transform(df_eval[\"language\"].astype(str))\n\nfor name, clf in models.items():\n    # Use skf and n_jobs=1 to avoid multiprocessing errors\n    cv_results = cross_validate(\n        clf,\n        X,\n        y,\n        cv=skf,\n        scoring=scoring,\n        n_jobs=1,\n        return_train_score=False,\n    )\n    acc = np.mean(cv_results[\"test_accuracy\"])\n    prec = np.mean(cv_results[\"test_precision\"])\n    rec = np.mean(cv_results[\"test_recall\"])\n    f1 = np.mean(cv_results[\"test_f1\"])\n    rows.append({\n        \"ablation\": ablation_name,\n        \"model\": name,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1,\n    })\n    print(f\"[{ablation_name} - {name}] Acc={acc:.3f} | Prec={prec:.3f} | Rec={rec:.3f} | F1={f1:.3f}\")\n\n\nFinal model selection\nchosen_ablation = “vocal_only” chosen_model = “SVM_linear” print(f”[Final model] Ablation = {chosen_ablation} | Model = {chosen_model}“)\ndf_best = df.loc[track_type_specs[chosen_ablation]].reset_index(drop=True) X_best = df_best[df_number_feature].copy() y_best = le.transform(df_best[“language”].astype(str))\nfinal_clf = SVC(kernel=“linear”, class_weight=“balanced”, random_state=42) final_clf.fit(X_best, y_best)\nprint(f”[Final Train] Done on {chosen_ablation} - {chosen_model} | n_samples = {len(df_best)}“)"
  },
  {
    "objectID": "presentation.html#our-dataset",
    "href": "presentation.html#our-dataset",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-1",
    "href": "presentation.html#our-dataset-1",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-2",
    "href": "presentation.html#our-dataset-2",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-3",
    "href": "presentation.html#our-dataset-3",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-4",
    "href": "presentation.html#our-dataset-4",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-5",
    "href": "presentation.html#our-dataset-5",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-6",
    "href": "presentation.html#our-dataset-6",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-7",
    "href": "presentation.html#our-dataset-7",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-8",
    "href": "presentation.html#our-dataset-8",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-9",
    "href": "presentation.html#our-dataset-9",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-10",
    "href": "presentation.html#our-dataset-10",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-11",
    "href": "presentation.html#our-dataset-11",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-12",
    "href": "presentation.html#our-dataset-12",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-13",
    "href": "presentation.html#our-dataset-13",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-14",
    "href": "presentation.html#our-dataset-14",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#questionsgoals",
    "href": "presentation.html#questionsgoals",
    "title": "",
    "section": "Questions/Goals",
    "text": "Questions/Goals"
  },
  {
    "objectID": "presentation.html#data-filesfeature-extraction",
    "href": "presentation.html#data-filesfeature-extraction",
    "title": "",
    "section": "Data Files/Feature Extraction",
    "text": "Data Files/Feature Extraction\n\n##Feature Scraping\n\n##Q1 overview\n\n##Q1 results [1/3]\n\n##Q1 results [2/3]\n\n##Q1 summary [3/3]\n\n##Q2 overview\n\n##Q2 results [1/3]\n\n##Q2 results [2/3]\n\n##Q2 results [3/3]\n\n##Q2 Summary\n\n##Full Summary\n\n##In closing ## Our Dataset"
  },
  {
    "objectID": "presentation.html#section",
    "href": "presentation.html#section",
    "title": "",
    "section": "",
    "text": "🤖 Project Description & Goals\n\n  Our team is developing a machine learning system for an AI-driven music recommendation service.\n  The main goals are:\n\n  \n    Build a model capable of recognizing the language(s) spoken in audio files.\n    Assess whether new songs align with a user’s musical preferences.\n  \n\n  \n\n  ✦ Question 1 – Language Recognition\n  How can we leverage statistical and time-frequency features from separated vocal and audio tracks to build effective language recognition models?\n  \n    What are the strengths and limitations of classical models (e.g., SVMs, Random Forest, Logistic Regression)?\n    How do feature engineering and validation improve model robustness?\n  \n\n  \n\n  ✦ Question 2 – Recommendation System [Genre Recognition]\n  Starting with genre recognition as part of the user UI/UX, can a user’s preferred genres be learned through feature analysis of audio files?\n  \n    Using classical supervised models: Can audio features be extracted and models trained to recognize a user's preferred genres?\n    Using frequency/time heatmap image extraction: Are classical models capable of determining musical genres from spectrogram-like representations?"
  },
  {
    "objectID": "presentation.html#section-1",
    "href": "presentation.html#section-1",
    "title": "",
    "section": "",
    "text": "🧩 Data Files and Extraction Workflow\n  \n    User Input: Providing initial CSV files containing song names, optionally including genre, language, and artist information.\n    Data Completion: Performing automated web scraping with Python scripts, filling in missing metadata fields such as genre, language, and artist.\n    YouTube Integration: Scraping YouTube with Python scripts, finding matching song URLs, and storing these links and related metadata in a JSON file.\n    Audio Acquisition: Downloading .wav audio files from YouTube URLs for each song with Python scripts.\n    Feature Extraction: Using Python (numpy and librosa libraries) scripts for analyzing audio files and extracting features (e.g., fundamental frequency, MFCCs, tempo).\n    Output Files: Saving extracted audio features as CSV files; saving visualizations such as spectrograms as .png images. From here, constructing machine learning pipelines with Python libraries."
  },
  {
    "objectID": "presentation.html#section-2",
    "href": "presentation.html#section-2",
    "title": "",
    "section": "",
    "text": "🔍 Feature Scraping\n\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nfundamental_freq\nFundamental frequency (mean pitch via librosa.pyin)\n\n\nfreq_e_1\nDominant spectral energy #1 (highest energy frequency bin)\n\n\nfreq_e_2\nDominant spectral energy #2 (2nd highest energy frequency bin)\n\n\nfreq_e_3\nDominant spectral energy #3 (3rd highest energy frequency bin)\n\n\nkey\nEstimated musical key (C, C#, D, …, B) via chroma features\n\n\nduration\nLength of audio in seconds\n\n\nzero_crossing_rate\nAverage zero crossing rate (signal sign changes)\n\n\nmfcc_mean\nMean of 13 MFCC coefficients (timbre features)\n\n\nmfcc_std\nStandard deviation of MFCC coefficients\n\n\ntempo\nEstimated tempo in beats per minute (BPM)\n\n\nrms_energy\nRoot mean square energy (loudness measure)\n\n\ntrack_type\nAudio track type (0=full mix, 1=vocal only, 2=no vocals)\n\n\nmel_spectrogram\nMel-scaled spectrogram representing frequency content over time (human hearing range)"
  },
  {
    "objectID": "presentation.html#section-3",
    "href": "presentation.html#section-3",
    "title": "",
    "section": "",
    "text": "F1 (macro) comparison across ablations and models"
  },
  {
    "objectID": "presentation.html#section-4",
    "href": "presentation.html#section-4",
    "title": "",
    "section": "",
    "text": "Q2 (A/B) Overview - Nathan describe your question and how you implemented mL [libraries, validation]"
  },
  {
    "objectID": "presentation.html#section-5",
    "href": "presentation.html#section-5",
    "title": "",
    "section": "",
    "text": "Q2 (A) Results - Nathan graphs? Learning Curve, ROC, any hyperparameter optimization?"
  },
  {
    "objectID": "presentation.html#section-6",
    "href": "presentation.html#section-6",
    "title": "",
    "section": "",
    "text": "Q2 (A) Results - Nathan Results (% scores) Validation methods."
  },
  {
    "objectID": "presentation.html#section-7",
    "href": "presentation.html#section-7",
    "title": "",
    "section": "",
    "text": "Q2 (B) Results - Nathan. Summarize - what you did/results."
  },
  {
    "objectID": "presentation.html#section-8",
    "href": "presentation.html#section-8",
    "title": "",
    "section": "",
    "text": "Q2 (B) Results [3/3] - Nathan. Summarize - what you did/results."
  },
  {
    "objectID": "presentation.html#section-9",
    "href": "presentation.html#section-9",
    "title": "",
    "section": "",
    "text": "Q2 Summary - Nathan"
  },
  {
    "objectID": "presentation.html#section-10",
    "href": "presentation.html#section-10",
    "title": "",
    "section": "",
    "text": "Full Summary - Nathan talk about both question findings."
  },
  {
    "objectID": "presentation.html#section-11",
    "href": "presentation.html#section-11",
    "title": "",
    "section": "",
    "text": "Thank you for listening\n      \n        Your attention was appreciated.\n        Questions welcome.\n      \n      Team Members:"
  },
  {
    "objectID": "presentation.html#section-12",
    "href": "presentation.html#section-12",
    "title": "",
    "section": "",
    "text": "Full Summary - Nathan talk about both question findings."
  },
  {
    "objectID": "presentation.html#section-13",
    "href": "presentation.html#section-13",
    "title": "",
    "section": "",
    "text": "In Closing - Nathan"
  },
  {
    "objectID": "presentation.html#section-14",
    "href": "presentation.html#section-14",
    "title": "",
    "section": "",
    "text": "In Closing"
  },
  {
    "objectID": "presentation.html#project-overview-research-questions",
    "href": "presentation.html#project-overview-research-questions",
    "title": "",
    "section": "🔍 Project Overview & Research Questions",
    "text": "🔍 Project Overview & Research Questions\n\n\n  🎯 Project Description & Goals\n  \n    Our team is developing a machine learning system for an AI-driven music recommendation service. The main goals are:\n  \n  \n    To build a model capable of recognizing the language(s) spoken in audio files\n    To assess whether new songs align with a user’s musical preferences\n  \n  \n    By separating vocal tracks and analyzing audio features such as tempo, loudness, genre, and language, we aim to create a highly personalized and accurate recommendation system.\n  \n\n  \n\n  ❓ Research Question 1 – Language Recognition\n  How can we leverage statistical and time-frequency features extracted from separated vocal and audio tracks to build effective language recognition models?\n  \n    What are the strengths and limitations of classical methods like SVMs or GMMs in this domain?\n    How do engineered features and model validation techniques impact robustness?\n    Does file format (.wav vs .mp3) affect accuracy due to compression artifacts?\n  \n\n  \n\n  ❓ Research Question 2 – Recommendation System\n  How can user interaction data combined with audio and metadata be used to build an effective recommendation system using collaborative filtering or traditional ML?\n  \n    Can audio features and clustering improve personalization?\n    What impact do vocal vs. full-mix inputs have?\n    Does audio quality (.wav vs .mp3) influence feature extraction and recommendation performance?"
  },
  {
    "objectID": "presentation.html#learning-curve-f1-score",
    "href": "presentation.html#learning-curve-f1-score",
    "title": "",
    "section": "Learning Curve (F1 Score)",
    "text": "Learning Curve (F1 Score)\n\n\nF1 (macro) comparison across ablations and models"
  },
  {
    "objectID": "presentation.html#question-1-summary",
    "href": "presentation.html#question-1-summary",
    "title": "",
    "section": "📌 Question 1-Summary",
    "text": "📌 Question 1-Summary\n\n  \n    What we did:\n      \n        Designed a supervised ML pipeline with feature scaling, label encoding, stratified CV.\n        Evaluated three classifiers on different track-type subsets.\n      \n    \n    Results:\n      \n        Best model = Linear SVM trained on vocal_only data.\n        Achieved ~50% performance across all metrics.\n      \n    \n    Conclusion:\n      \n        Vocal features are sufficient to classify language with strong accuracy.\n        Next: expand dataset, apply hyperparameter optimization, and test on external songs."
  },
  {
    "objectID": "presentation.html#section-39",
    "href": "presentation.html#section-39",
    "title": "",
    "section": "",
    "text": "Q2 Overview - Nathan describe your question and how you implemented mL [libraries, validation]"
  },
  {
    "objectID": "presentation.html#section-40",
    "href": "presentation.html#section-40",
    "title": "",
    "section": "",
    "text": "Q2 Results [1/3] - Nathan graphs? Learning Curve, ROC, any hyperparameter optimization?"
  },
  {
    "objectID": "presentation.html#section-41",
    "href": "presentation.html#section-41",
    "title": "",
    "section": "",
    "text": "Q2 Results [2/3] - Nathan Results (% scores) Validation methods."
  },
  {
    "objectID": "presentation.html#section-42",
    "href": "presentation.html#section-42",
    "title": "",
    "section": "",
    "text": "Q2 Results [3/3] - Nathan. Summarize - what you did/results."
  },
  {
    "objectID": "presentation.html#section-43",
    "href": "presentation.html#section-43",
    "title": "",
    "section": "",
    "text": "Q2 Summary - Nathan"
  },
  {
    "objectID": "presentation.html#section-44",
    "href": "presentation.html#section-44",
    "title": "",
    "section": "",
    "text": "Full Summary - Nathan talk about both question findings."
  },
  {
    "objectID": "presentation.html#section-45",
    "href": "presentation.html#section-45",
    "title": "",
    "section": "",
    "text": "In Closing - Nathan"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Audio Alchemy",
    "section": "Introduction",
    "text": "Introduction\nMusic genre classification is a central task in the field of music information retrieval, combining elements of signal processing, machine learning, and deep learning. Accurate genre identification not only enhances music recommendation systems and streaming platforms but also deepens our understanding of audio structure and human perception of sound. Traditional approaches have relied on handcrafted audio features analyzed with machine learning techniques such as Random Forests and Gaussian Mixture Models, offering interpretable yet limited performance [1]. Recent advances, however, leverage deep learning methods—particularly convolutional neural networks (CNNs)—to extract high-level representations directly from spectrograms, achieving state-of-the-art results [2]. This project explores both paradigms: first applying classical machine learning with 5-fold cross-validation, and then advancing to CNN-based classification on spectrogram heat maps, with results evaluated using standard metrics including accuracy, precision, recall, F1-score, confusion matrices, and ROC curves."
  },
  {
    "objectID": "index.html#questions",
    "href": "index.html#questions",
    "title": "Audio Alchemy",
    "section": "Questions",
    "text": "Questions\n\n1. Language Recognition with Separated Vocal & Audio Tracks\n\nInitial Problem Formulation\nHow can we leverage statistical and time-frequency features extracted from separated vocal and audio tracks to build effective language recognition models? Specifically, how can traditional machine learning methods — ranging from classical classifiers on simple statistical summaries to Gaussian Mixture Models on richer time-frequency features — be applied in this context?\n\nWhat are the key benefits and limitations of these approaches?\n\nHow can careful feature engineering, feature integration, and thorough model evaluation improve the accuracy and robustness of language recognition systems?\n\nHow do model results compare and contrast when using .wav files versus .mp3 files?\n\n\n\nSecondary Problem formulation\nFrom the initial formulation, we refined the question to specifically compare how different ablations of the audio track (complete song, vocal-only, and non-vocal) affect model performance.\n\nHow does model performance differ when predicting song language using features from complete songs, vocal-only tracks, and instrumental-only tracks?\nWhat are the relative strengths and limitations of classical machine learning models (Logistic Regression, Random Forest, SVM) when applied to language recognition?\n\n\n\n\n2. Recommendation Systems Using Audio Features & User Data\n\nInitial Problem Formulation\nHow can user interaction data, combined with basic track metadata and simple audio features, be used to build an effective recommendation system using collaborative filtering and traditional machine learning methods?\n\nFurthermore, how can advanced audio features, dimensionality reduction, and clustering techniques improve personalized recommendations by better capturing user preferences and track characteristics from both vocal and non-vocal components?\n\nHow do recommendation model results compare and contrast when using .wav files versus .mp3 files, considering the potential impact of audio quality and compression artifacts on feature extraction and recommendation performance?\n\n\n\nSecondary Problem Formulation\nWe abandoned the use of .wav versus .mp3 formats for the reasons previously mentioned. Instead, the idea of using heat maps/spectrograms was discovered and pursued. A CNN was built for both grayscale and viridis-scale inputs. As will be discussed in the Problem Analysis and Results section, training outcomes were poor despite what initially seemed to be a solid approach. The current hypothesis is that the dataset is too small to produce a robust model, that the extracted song metrics are insufficient to support effective training, or a combination of both. The Likert scale—with options of ‘Likert 2,’ ‘Likert 3,’ and ‘Likert 5’—was not employed, as it represents a second phase of recommendation by genre, contingent on reliable genre recognition. ## Dataset\n\n\n\ndata provenance\nThe data collection process involved several custom Python scripts designed to scrape and download the necessary information and audio files:\nartist_5_song_list_scrape.py — Retrieves the top five songs per artist from Google search results.\nartist_genre_scrape.py — Gathers genre metadata for each artist from public sources.\nartist_country_of_origin_scrape.py — Extracts the country of origin for each artist.\naudio_scrape_wav_mp3.py — Downloads audio files from YouTube links in WAV and MP3 formats.\nTogether, these scripts automate the extraction of both audio data and relevant metadata to support training and evaluation of the recommendation system.\nFor question 1 A total of 123 songs were scraped, each was turned into the triplicate of (1) complete song, (2) audio only (3) vocal only. Yielding 369 observations to work with. The complete extraction pipeline for question 1 was around 15 hours.\nFor question 2 A total of 20 genres were examined, each with 10 example songs from relevant artists - yielding a set of 200 observations to work with. The complete extraction pipeline for question 2 was around 5 hours - due to the use of parallel threads in a reconfigured software file.\n\n\nsoftware distriubtion\nInitially, the plan was to distribute a software package to both partners so they could each collect their song files and extract the data locally. However, due to the ambitious goals and the multifaceted software requirements needed to accomplish them, the team soon felt as if we were flying the plane while building it. Ultimately, one team member (Nathan) took responsibility for collecting the song file data and generating the features. These features were then distributed to other members, replacing the originally envisioned feature-extraction software suite.\n\n\ndata features\nIn addition to the artist and song name, the features listed in Table 1 were scraped for each track. The final selection of features was guided as much by curiosity—‘I wonder what this will do’—as by deliberate planning. Research was done - but, until you try to build the model yourself you’re not aware of what works under what condtions.\n\n\n\n🔍 Feature Scraping\n\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nfundamental_freq\nFundamental frequency (mean pitch via librosa.pyin)\n\n\nfreq_e_1\nDominant spectral energy #1 (highest energy frequency bin)\n\n\nfreq_e_2\nDominant spectral energy #2 (2nd highest energy frequency bin)\n\n\nfreq_e_3\nDominant spectral energy #3 (3rd highest energy frequency bin)\n\n\nkey\nEstimated musical key (C, C#, D, …, B) via chroma features\n\n\nduration\nLength of audio in seconds\n\n\nzero_crossing_rate\nAverage zero crossing rate (signal sign changes)\n\n\nmfcc_mean\nMean of 13 MFCC coefficients (timbre features)\n\n\nmfcc_std\nStandard deviation of MFCC coefficients\n\n\ntempo\nEstimated tempo in beats per minute (BPM)\n\n\nrms_energy\nRoot mean square energy (loudness measure)\n\n\ntrack_type\nAudio track type (0=full mix, 1=vocal only, 2=no vocals)\n\n\nmel_spectrogram\nMel-scaled spectrogram representing frequency content over time (human hearing range)\n\n\n\n\n\n\nTable 1: Extracted Audio Features\n\n\n\ndata storge\nThis JSON schema, as proposed in the original plan, was refactored as needed. The multiple pipeline components required to gather and merge the information made it more expedient to use a combination of the .json design and .csv files. Shown below is the main .json design used for the project.\n\n\n\n\n{\n    \"AudioFile\": {\n        \"yt_link\": [],\n        \"wav_link\": [],\n        \"mp3_link\": []\n    },\n    \"Region\": [\n        \"America\"\n    ],\n    \"Data\": {\n        \"Artist\": \"Lady Gaga\",\n        \"Song Title\": \"\",\n        \"Genre\": [],\n        \"Mean (of features)\": null,\n        \"Variance\": null,\n        \"Skewness\": null,\n        \"Kurtosis\": null,\n        \"Zero Crossing Rate\": null,\n        \"RMS Energy\": null,\n        \"Loudness\": null,\n        \"Energy\": null,\n        \"Tempo\": null,\n        \"Danceability\": null,\n        \"Key / Key Name\": \"\",\n        \"Mode / Mode Name\": \"\",\n        \"Mel-Spectrogram\": null,\n        \"Duration (ms)\": null\n    }\n}"
  },
  {
    "objectID": "index.html#dataset",
    "href": "index.html#dataset",
    "title": "Audio Alchemy",
    "section": "Dataset",
    "text": "Dataset\n\ndata provenance\nThe data collection process involved several custom Python scripts designed to scrape and download the necessary information and audio files:\nartist_5_song_list_scrape.py — Retrieves the top five songs per artist from Google search results.\nartist_genre_scrape.py — Gathers genre metadata for each artist from public sources.\nartist_country_of_origin_scrape.py — Extracts the country of origin for each artist.\naudio_scrape_wav_mp3.py — Downloads audio files from YouTube links in WAV and MP3 formats.\nTogether, these scripts automate the extraction of both audio data and relevant metadata to support training and evaluation of the recommendation system.\nFor question 1 A total of 123 songs were scraped, each was turned into the triplicate of (1) complete song, (2) audio only (3) vocal only. Yielding 369 observations to work with. The complete extraction pipeline for question 1 was around 15 hours.\nFor question 2 A total of 20 genres were examined, each with 10 example songs from relevant artists - yielding a set of 200 observations to work with. The complete extraction pipeline for question 2 was around 5 hours - due to the use of parallel threads in a reconfigured software file.\n\n\nsoftware distriubtion\nInitially, the plan was to distribute a software package to both partners so they could each collect their song files and extract the data locally. However, due to the ambitious goals and the multifaceted software requirements needed to accomplish them, the team soon felt as if we were flying the plane while building it. Ultimately, one team member (Nathan) took responsibility for collecting the song file data and generating the features. These features were then distributed to other members, replacing the originally envisioned feature-extraction software suite.\n\n\ndata features\nIn addition to the artist and song name, the features listed in Table 1 were scraped for each track. The final selection of features was guided as much by curiosity—‘I wonder what this will do’—as by deliberate planning. Research was done - but, until you try to build the model yourself you’re not aware of what works under what condtions.\n\n\n\n🔍 Feature Scraping\n\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nfundamental_freq\nFundamental frequency (mean pitch via librosa.pyin)\n\n\nfreq_e_1\nDominant spectral energy #1 (highest energy frequency bin)\n\n\nfreq_e_2\nDominant spectral energy #2 (2nd highest energy frequency bin)\n\n\nfreq_e_3\nDominant spectral energy #3 (3rd highest energy frequency bin)\n\n\nkey\nEstimated musical key (C, C#, D, …, B) via chroma features\n\n\nduration\nLength of audio in seconds\n\n\nzero_crossing_rate\nAverage zero crossing rate (signal sign changes)\n\n\nmfcc_mean\nMean of 13 MFCC coefficients (timbre features)\n\n\nmfcc_std\nStandard deviation of MFCC coefficients\n\n\ntempo\nEstimated tempo in beats per minute (BPM)\n\n\nrms_energy\nRoot mean square energy (loudness measure)\n\n\ntrack_type\nAudio track type (0=full mix, 1=vocal only, 2=no vocals)\n\n\nmel_spectrogram\nMel-scaled spectrogram representing frequency content over time (human hearing range)\n\n\n\n\n\n\nTable 1: Extracted Audio Features\n\n\n\ndata storge\nThis JSON schema, as proposed in the original plan, was refactored as needed. The multiple pipeline components required to gather and merge the information made it more expedient to use a combination of the .json design and .csv files. Shown below is the main .json design used for the project.\n\n\n\n\n{\n    \"AudioFile\": {\n        \"yt_link\": [],\n        \"wav_link\": [],\n        \"mp3_link\": []\n    },\n    \"Region\": [\n        \"America\"\n    ],\n    \"Data\": {\n        \"Artist\": \"Lady Gaga\",\n        \"Song Title\": \"\",\n        \"Genre\": [],\n        \"Mean (of features)\": null,\n        \"Variance\": null,\n        \"Skewness\": null,\n        \"Kurtosis\": null,\n        \"Zero Crossing Rate\": null,\n        \"RMS Energy\": null,\n        \"Loudness\": null,\n        \"Energy\": null,\n        \"Tempo\": null,\n        \"Danceability\": null,\n        \"Key / Key Name\": \"\",\n        \"Mode / Mode Name\": \"\",\n        \"Mel-Spectrogram\": null,\n        \"Duration (ms)\": null\n    }\n}"
  },
  {
    "objectID": "index.html#analysis-plan",
    "href": "index.html#analysis-plan",
    "title": "Audio Alchemy",
    "section": "Analysis Plan",
    "text": "Analysis Plan\n\nGeneral\n\n\nQ1 - Yashi\n\nYashi #_to_Do - entire section for your problem\n\n\nput any graphs in this section.\ndo you have learning curve, ROC curves? Feature importance graphs?\n\n\nrestate question\ndata collection - data set size/composition (var types) Data was collected through a series of Python scripts. [see slide 3] You had an extra layer of feature extraction - removing vocal/instrumental tracts.\ndata processing - any PCA, correlation, imputation, outlier removal?\nmodel selection - what models, why? (if any reason), what Python libraries did you use?\nmodel validation - what metrics? why?\nmodel evaluation - what metrics? why? Note: the ‘no vocal’ track - behaved at about 50% accuracy - which is what you’d expect for a control group.\nfuture steps/recommendations\n\n\n\nQ2\n\nNathan #_to_Do\n\n\nput any graphs in this section.\ndo you have learning curve, ROC curves? Feature importance graphs?\n\n\nrestate question\ndata collection - data set size/composition (var types) Data was collected through a series of Python scripts. [see slide 3] You had an extra layer of feature extraction - removing vocal/instrumental tracts.\ndata processing - any PCA, correlation, imputation, outlier removal?\nmodel selection - what models, why? (if any reason), what Python libraries did you use?\nmodel validation - what metrics? why?\nmodel evaluation - what metrics? why?\nfuture steps/recommendations"
  },
  {
    "objectID": "index.html#results-conclusion",
    "href": "index.html#results-conclusion",
    "title": "Audio Alchemy",
    "section": "Results & Conclusion",
    "text": "Results & Conclusion\nThe primary goal of this project was to develop a machine learning system capable of recognizing both the language spoken in audio files and the musical genre, in order to enhance personalization in AI-driven music recommendation platforms. By accurately identifying spoken language within songs and combining this information with genre metadata, the system aims to suggest tracks that more closely align with individual user preferences. The challenge involved processing raw audio data, separating vocal from instrumental components, extracting meaningful statistical and time-frequency features, and applying both classical machine learning models and deep learning architectures to capture the underlying patterns in music.\nTo address these goals, the team experimented with a variety of models. Classical approaches—including Logistic Regression, Random Forests, and Support Vector Machines—were trained on extracted audio features using cross-validation, yielding modest predictive performance with accuracy, precision, recall, and F1-scores generally between 10–60%. K-Nearest Neighbors and Random Forests were applied for genre classification, while Convolutional Neural Networks were trained on Mel spectrogram images (grayscale and color), using early stopping and hyperparameter sweeps to optimize performance. Although the models demonstrated only limited overall accuracy, CNNs showed comparatively stronger results and align with literature on deep learning’s potential for audio analysis. Future improvements may include expanding the dataset size, refining spectrogram preprocessing, exploring deeper or more specialized architectures, and integrating more robust feature engineering to enhance both language and genre recognition for more effective recommendation systems."
  },
  {
    "objectID": "index.html#video-links",
    "href": "index.html#video-links",
    "title": "Audio Alchemy",
    "section": "Video links",
    "text": "Video links\n\nNathan #_to_Do"
  },
  {
    "objectID": "index.html#audio-player-demo",
    "href": "index.html#audio-player-demo",
    "title": "Audio Alchemy",
    "section": "Audio Player Demo",
    "text": "Audio Player Demo\nA demo of the Ui/Ux audio player written for this project. First a few songs are scrolled through to demonstarate the functionality of ‘real time’ generation of dB v. freq. curves for .wav and .mp3. Next a song is played to demonstrat the ‘real time’ audio analysis with the spectrogram (heat map) feature.\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "index.html#team-member-workload",
    "href": "index.html#team-member-workload",
    "title": "Audio Alchemy",
    "section": "Team member workload",
    "text": "Team member workload\nOur project workload followed a structured week-by-week workflow as proposed in the initial proposal, with responsibilities distributed among team members. We began by finalizing and sharing the proposal, followed by the individual collection and organization of ~200 audio files per person. Nathan Herling led the processing and validation of metadata, while each member focused on building machine learning pipelines and conducting iterative testing. The project concluded with a collaborative effort on final model evaluation, report preparation, and presentation development."
  },
  {
    "objectID": "index.html#sources",
    "href": "index.html#sources",
    "title": "Audio Alchemy",
    "section": "sources",
    "text": "sources\n[1] https://link.springer.com/chapter/10.1007/978-981-97-4533-3_6\n[2] https://arxiv.org/html/2411.14474v1"
  },
  {
    "objectID": "index.html#a-note-on-spectographic-features-of-.mp3-vs.-.wav",
    "href": "index.html#a-note-on-spectographic-features-of-.mp3-vs.-.wav",
    "title": "Audio Alchemy",
    "section": "A note on spectographic features of .mp3 vs. .wav",
    "text": "A note on spectographic features of .mp3 vs. .wav\nInitially, it was assumed that the typical size difference between .mp3 and .wav files would reflect meaningful differences in their spectral properties. However, analysis revealed that the differences were minimal, leading us to abandon the idea of using the two file types as comparative baselines for our models. As shown in Figure 1, the typical similarity between .wav and .mp3 files exceeds 98%, rendering the expectation of differing training results between the two data types a moot point.\n\n\n\nFigure 1: Frequency histogram comparison between MP3 and WAV audio files."
  },
  {
    "objectID": "index.html#problem-analysis-and-results",
    "href": "index.html#problem-analysis-and-results",
    "title": "Audio Alchemy",
    "section": "Problem analysis and results",
    "text": "Problem analysis and results\n\nGeneral\nEasy and medium paths, as proposed in the proposal morphed into multiple paths to tackle the problem, some bearing fruit, some not. In Q1, it could be argued that three easy paths were taken in an attempt to explore which may work better, and in Q2 two easy paths and a medium path were explored.\n\n\nQ1 - Yashi\nHow can we leverage audio features from separated vocal and instrumental tracks to improve language recognition in music?\nData Collection: The dataset consisted of ~200 audio files, preprocessed into three ablations: complete songs, vocal-only tracks, and instrumental-only tracks. Features included time-domain statistics (mean, variance, skewness, kurtosis).\nData Processing: All features were standardized using global scaling. Encoded target variable (language) with LabelEncoder.No major imputation was required as missingness was minimal.\nModel Selection: I evaluated three models: Logistic Regression, Random Forest, and Support Vector Machines with linear kernels. These models were chosen for their balance of interpretability, robustness, and suitability for structured feature data. Training and evaluation were conducted using 5-fold stratified cross-validation to ensure reliable performance comparisons across models.\nValidation & Metrics: Evaluation focused on accuracy, precision, recall, and F1-score. Confusion matrices were used to analyze per-class misclassification patterns.\nModel Evaluation:\n\n\n\n\n\n\n\n\n\n\n\nAblation\nModel\nAccuracy\nPrecision\nRecall\nF1\n\n\n\n\ncomplete_song\nLogReg\n0.399\n0.398\n0.447\n0.387\n\n\ncomplete_song\nRandomForest\n0.626\n0.469\n0.410\n0.401\n\n\ncomplete_song\nSVM_linear\n0.432\n0.443\n0.504\n0.427\n\n\nvocal_only\nLogReg\n0.560\n0.531\n0.567\n0.509\n\n\nvocal_only\nRandomForest\n0.552\n0.426\n0.404\n0.385\n\n\nvocal_only\nSVM_linear\n0.544\n0.542\n0.583\n0.514\n\n\nno_vocal\nLogReg\n0.333\n0.371\n0.364\n0.316\n\n\nno_vocal\nRandomForest\n0.577\n0.436\n0.349\n0.328\n\n\nno_vocal\nSVM_linear\n0.366\n0.418\n0.411\n0.347\n\n\nColumn Min\n-\n0.333\n0.371\n0.349\n0.316\n\n\nColumn Max\n-\n0.626\n0.542\n0.583\n0.514\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResults:\n\nVocal-only tracks: Provided the best classification signal, with SVM achieving ~0.51 macro F1, outperforming Random Forest and Logistic Regression.\nComplete songs: Models achieved moderate performance (~0.40 F1), reflecting a mixture of useful vocal cues diluted by instrumental content.\nNon_vocal tracks: Accuracy dropped to ~0.50 (random baseline), validating the expectation that language recognition requires vocal content.\nFuture reommendations: larger data sets and more hyperparameter exploration\n\n\n\nQ2\nHow can we leverage audio features to construct a machine learning model capable of genre recognition?\n\nData Collection: Data collection was performed with python scripts for each feature listed in Table 1. Ten genres were chosen and twenty representative artists for each genre - were both choosen by google search.\nData Processing: All data was present, no imputation was needed. It was decided to not eliminate statistical outliers, since the model design hasn’t been explored thoroughly enough to warrant selecting out data.\nModel Selection: Three supervised machine learning methods were chosen: (1) Knn [classic baseline] (2) Random Forest [with the hope of good baseline results] (3) CNN performed with the numerical dataset and spectograph extracted files.\nModel Validation:\n\n\n  (1) KNN\n  \n    LOOCV: Validates and selects the best hyperparameters.\n    5-Fold CV learning curve: Validates generalization performance as a function of training size.\n  \n\n  (2) Random Forest\n  \n    Cross validation\n    Learning curve\n  \n\n  (3) CNN\n  \n    Early stopping\n  \n\n  Model Metrics\n\n  (1) KNN - hyperparameter sweep:\n  \n    n_neighbors: [1, 3, 5, 7, 9]\n    weights: ['uniform', 'distance']\n    metric: ['euclidean', 'manhattan']\n    p: [1, 2] (only relevant if metric='minkowski')\n  \n\n  (2) Random Forest - hyperparameter sweep:\n  \n    n_estimators: [10, 50, 100, 200]\n    max_depth: [None, 5, 10, 15]\n    min_samples_split: [2, 5, 10]\n    min_samples_leaf: [1, 2, 4]\n  \n\n  (3) CNN - hyperparameter sweep:\n  \n    conv_filters: [[32, 64], [64, 128], [32, 64, 128]]\n    kernel_size: [(3,3), (5,5)]\n    dropout: [0.2, 0.3, 0.5]\n    learning_rate: [0.01, 0.001, 0.0001]"
  },
  {
    "objectID": "Yashi analysis.html#data-loading-overview",
    "href": "Yashi analysis.html#data-loading-overview",
    "title": "Yashi Analysis",
    "section": "",
    "text": "#| label: data-loading import pandas as pd import numpy as np\ndf = pd.read_csv(“_extra/Audio_Files/Yashi_s_Music/Y_audio_philes_final_features_mk1_filled_Yashi.csv”)\nprint(df.head()) print(df.info()) print(“Missing values per column:”) print(df.isnull().sum())\n#| label: preprocessing from sklearn.preprocessing import LabelEncoder, StandardScaler\ndf_non_feature = [“artist”, “country”, “language”, “track_type”] df_number_feature = [ c for c in df.select_dtypes(include=[np.number]).columns if c not in df_non_feature] print(“Numerical features used:”, df_number_feature)"
  },
  {
    "objectID": "index.html#a-note-on-project-software",
    "href": "index.html#a-note-on-project-software",
    "title": "Audio Alchemy",
    "section": "A note on project software",
    "text": "A note on project software\nIt’s generally easier and faster to run Python scrips separately and use the results in the discussion of the results. All scripts used are stored in: Herling-Mi_extra\\0_mL_scripts\\0_p1 Herling-Mi_extra\\0_mL_scripts\\0_p2"
  },
  {
    "objectID": "index.html#data---second-tier-features",
    "href": "index.html#data---second-tier-features",
    "title": "Audio Alchemy",
    "section": "Data - Second Tier Features",
    "text": "Data - Second Tier Features\nAfter the initial .wav data collection, the following '2nd tier' data were generated/collected:\n\n  .mp3\n  Separated vocal track\n  Separated audio track\n  Spectrogram data (viridis scale)\n  Spectrogram data (grey scale)\n\nThe Demucs library turned out to be easy to use and very good at vocal and background track separation. Scripts that run Demucs using system commands—typically through Python’s subprocess or os libraries—offer a straightforward way to integrate audio separation tools into Python workflows while interacting with the operating system’s file structure and command-line utilities. In order to run files in parallel, a main_script.py capable of generating mulitple threads would call a worker_script.py such as the one below.\nBelow are two representatives of spectrogram feature extraction.\n\n  \n    \n    image 1.Queen_we_will_rock_you - spectrograph - grey scale\n  \n\n  \n    \n    image 2.Billie Eilish - bad guy - spectrograph - vridis scale\n  \n\n\n\nimport os\nimport subprocess\n\n# Path to your input audio file\naudio_file = r\"~\\Gloria_Gaynor_I_Will_Survive.wav\"\n\n# Optional: check if file exists\nif not os.path.exists(audio_file):\n    raise FileNotFoundError(f\"Audio file not found: {audio_file}\")\n\n# Build the Demucs command\n# You can change --two-stems to 'drums' or 'bass' if needed\ncommand = [\n    \"demucs\",\n    \"--two-stems=vocals\",  # Extract vocals only\n    \"--out\", \"demucs_output\",  # Output folder\n    audio_file\n]\n\n# Run the command\nprint(\"🔄 Running Demucs...\")\nsubprocess.run(command)\n\nprint(\"✅ Separation complete. Check the 'demucs_output' folder for results.\")"
  },
  {
    "objectID": "index.html#metrics-and-results-knn",
    "href": "index.html#metrics-and-results-knn",
    "title": "Audio Alchemy",
    "section": "Metrics and Results (Knn)",
    "text": "Metrics and Results (Knn)\n\n\n  \n    \n    Hyperparameter Swee\n  \n\n  \n    \n    Learning Curve Knn\n  \n\n\n\n\n\nBest Hyperparameters\n\n\n\n\nknn__algorithm\nauto\n\n\nknn__n_neighbors\n15\n\n\nknn__p\n1\n\n\nknn__weights\nuniform\n\n\n\n\n\n\n\n\n\n\n\nMetric\nScore\n\n\n\n\nTest Precision (weighted)\n0.1017\n\n\nTest F1 Score (weighted)\n0.1117\n\n\nLOOCV Accuracy\n0.1350\n\n\n\n\n\n\n\n\n\n\n\nClass\nPrecision\nRecall\nF1-Score\nSupport\n\n\n\n\n0\n0.00\n0.00\n0.00\n4\n\n\n1\n0.25\n0.25\n0.25\n4\n\n\n2\n0.20\n0.25\n0.22\n4\n\n\n3\n0.17\n0.25\n0.20\n4\n\n\n4\n0.00\n0.00\n0.00\n4\n\n\n5\n0.00\n0.00\n0.00\n4\n\n\n6\n0.00\n0.00\n0.00\n4\n\n\n7\n0.20\n0.25\n0.22\n4\n\n\n8\n0.20\n0.25\n0.22\n4\n\n\n9\n0.00\n0.00\n0.00\n4\n\n\nAccuracy\n0.12\n\n\nMacro Avg\n0.11\n\n\nWeighted Avg\n0.11\n\n\n\n\n\n\n  Knn EvaluationThis learning curve reveals a significant gap between training and cross-validation performance for your KNN classifier:\n  \n  🔵 Training Score: The model achieves a perfect F1 score of 1.0 across all training set sizes, which is a strong indicator of overfitting—the model memorizes the training data rather than generalizing from it.\n  \n  🟢 Cross-Validation Score: Starts near 0.0 and only climbs to about 0.2 even with 160 training samples. This suggests the model struggles to generalize and perform well on unseen data.\n  \n  📉 Implication: Despite using the best hyperparameters, the model may be too sensitive to noise or lacks sufficient complexity to capture meaningful patterns. KNN’s reliance on local structure might be failing due to sparse or high-dimensional data."
  },
  {
    "objectID": "index.html#metrics-and-results-random-forest",
    "href": "index.html#metrics-and-results-random-forest",
    "title": "Audio Alchemy",
    "section": "Metrics and Results (Random Forest)",
    "text": "Metrics and Results (Random Forest)\n\n\n  \n    \n    Hyperparameter Sweep - RF\n  \n\n  \n    \n    Learning Curve - RF\n  \n\n\n\n\n\nMetric\nValue\n\n\n\n\nmax_depth\n10.0\n\n\nmin_samples_leaf\n1.0\n\n\nmin_samples_split\n2.0\n\n\nn_estimators\n200.0\n\n\naccuracy\n0.925\n\n\nprecision\n0.927554\n\n\nrecall\n0.925\n\n\nf1_score\n0.924728\n\n\n\n\n  Random Forrest AnalysisYes, this random forest model appears to be overtrained, and here’s why:\n\n  🔍 Key Indicators of Overtraining\n  \n    Training Accuracy = 1.0 across all training set sizes:\n      This suggests the model is memorizing the training data perfectly, which is a classic sign of overfitting.\n    \n    Validation Accuracy starts low (~0.1) and rises to ~0.85:\n      While the validation accuracy improves with more data, the persistent gap between training and validation accuracy indicates poor generalization early on.\n    \n    Even at the largest training size, the model still performs significantly worse on unseen data than on training data.\n  \n\n  📈 What a Healthy Learning Curve Might Look Like\n  \n    Training accuracy should decrease slightly as training size increases (less memorization).\n    Validation accuracy should increase and converge toward training accuracy.\n    A smaller gap between the two curves suggests better generalization.\n  \n\n  🧠 Why Random Forests Can Overfit\n  \n    If the number of trees is too high or if each tree is allowed to grow too deep, the ensemble can overfit.\n    Especially with small datasets, random forests can memorize patterns that don’t generalize."
  },
  {
    "objectID": "index.html#metrics-and-results-cnn---grey-scale",
    "href": "index.html#metrics-and-results-cnn---grey-scale",
    "title": "Audio Alchemy",
    "section": "Metrics and Results (CNN - Grey scale)",
    "text": "Metrics and Results (CNN - Grey scale)\n\n\n  \n    \n    Hyperparameter Sweep CNN - grey scale\n  \n\n  \n    \n    Learning Curve CNN - grey scale\n  \n\n\n\n\n\nConv Layers\nEpochs\nPatience\nAccuracy\nF1\nPrecision\n\n\n\n\n2\n10\n2\n0.0750\n0.0143\n0.0079\n\n\n2\n10\n5\n0.1000\n0.0229\n0.0129\n\n\n3\n10\n2\n0.1000\n0.0182\n0.0100\n\n\n3\n10\n5\n0.1000\n0.0182\n0.0100\n\n\n2\n15\n2\n0.1000\n0.0186\n0.0103\n\n\n2\n15\n5\n0.1000\n0.0186\n0.0103\n\n\n3\n15\n2\n0.1250\n0.0450\n0.0361\n\n\n3\n15\n5\n0.1000\n0.0182\n0.0100\n\n\n2\n30\n2\n0.1000\n0.0182\n0.0100\n\n\n2\n30\n5\n0.2000\n0.0750\n0.0476\n\n\n3\n30\n2\n0.1000\n0.0182\n0.0100\n\n\n3\n30\n5\n0.1000\n0.0182\n0.0100\n\n\n\n\n  CNN Model Assessment - Grayscale Data\n  \n  🚨 Red Flags in the Learning Curve\n  \n    Training Accuracy rises to 1.0 by epoch 5: The model is perfectly memorizing the training data.\n    Validation Accuracy stays flat at ~0.2: The model is not generalizing at all. It is essentially guessing on unseen data.\n  \n  \n  🔍 Possible Causes\n  \n    Data Issues:\n      \n        Grayscale input might lack sufficient contrast or features.\n        Labels could be noisy or mismatched.\n      \n    \n    Model Complexity: The CNN might be too deep or have too many parameters for the dataset size.\n    Overtraining:\n      \n        No regularization (e.g., dropout, weight decay).\n        No early stopping.\n      \n    \n  \n\n  Note: All attempts to reduce overtraining did not work. It is postulated that the dataset needs to be larger for the CNN to learn meaningful patterns."
  },
  {
    "objectID": "index.html#metrics-and-results-cnn---color-scale",
    "href": "index.html#metrics-and-results-cnn---color-scale",
    "title": "Audio Alchemy",
    "section": "Metrics and Results (CNN - Color scale)",
    "text": "Metrics and Results (CNN - Color scale)\n\n\n  \n    \n    Hyperparameter Sweep - Color Spectrogram - CNN\n  \n\n  \n    \n    Learning Curve - Color Spectrogram - CNN\n  \n\n\n\n  Final CNN Model Stats\n  \n  \n    Training Accuracy: 0.1937\n    Training Loss: 2.1730\n    Validation Accuracy: 0.1750\n    Validation Loss: 2.1234\n    Epoch 5: Early stopping triggered\n    Restoring model weights from the best epoch: 1\n  \n\n  Final Best Model Metrics:\n  \n    Accuracy: 0.1000\n    Precision: 0.0100\n    F1 Score: 0.0182\n  \n\n\n  Color Spectrogram CNN Learning Curve\n\n  This graph shows a modestly improving CNN model trained on color spectrogram data, but it’s still underperforming overall.\n\n  📈 What the Learning Curve Shows:\n  \n    Training Accuracy steadily increases from 0.0 to ~0.18 by epoch 4.\n    Validation Accuracy peaks at epoch 2 (~0.20), then slightly declines and flattens.\n  \n\n  🧠 Interpretation:\n  \n    The model is learning, but very slowly.\n    The validation peak at epoch 2 suggests the model briefly generalized well, but then started to overfit.\n    The low overall accuracy (max ~0.20) implies the model is struggling to extract meaningful features from the spectrograms.\n  \n\n  🔍 Possible Issues:\n  \n    Spectrogram preprocessing might be suboptimal (e.g., poor resolution, noisy input).\n    Model architecture may be too shallow or not well-tuned for this type of data.\n    Class imbalance or label noise could be limiting performance.\n    Too few epochs — the model might need more time to converge.\n  \n\n  Note: Epoch 2 was the optimal epoch from the hyperparameter sweep. A postulated fix is to use a larger dataset to improve generalization and model performance.\n\n(7) future steps/recommendations"
  }
]