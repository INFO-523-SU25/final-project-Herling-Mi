[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Audio Alchemy",
    "section": "",
    "text": "Music recommendation systems increasingly rely on machine learning to capture the complexity of user preferences, yet existing models struggle to account for language diversity and nuanced audio features in songs. This project applies signal processing, vocal separation (DEMUCS library), and machine learning techniques to classify song languages and integrate them with genre metadata for improved personalization. By combining automated data collection with advanced audio analysis, the system provides a foundation for smarter, more inclusive recommendation platforms that enhance user experience across diverse musical contexts. The project first applied Random Forests and Gaussian Mixture Models with 5-fold cross-validation for audio genre identification, then advanced to CNNs on spectrogram heat maps validated via a train/validation/test split with early stopping, evaluated through accuracy, precision, recall, F1-score, confusion matrices, and ROC curves.\n\nNathan #_to_Do\n\n(Nathan) - sentence about results.\n\nBy applying statistical and time-frequency features to separated vocal and instrumental tracks, we evaluated the feasibility of machine learning models in song language recognition. Classical approaches such as Logistic Regression, Random Forests, and SVMs were trained with 5-fold cross-validation. Results showed that vocal-only features provided the strongest signals for classification. The analysis demonstrated that language prediction from raw audio is viable when leveraging targeted feature engineering.\n\n(Yashi) - sentence about Part 2. (Yashi) - sentence about results."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Audio Alchemy",
    "section": "",
    "text": "Music recommendation systems increasingly rely on machine learning to capture the complexity of user preferences, yet existing models struggle to account for language diversity and nuanced audio features in songs. This project applies signal processing, vocal separation (DEMUCS library), and machine learning techniques to classify song languages and integrate them with genre metadata for improved personalization. By combining automated data collection with advanced audio analysis, the system provides a foundation for smarter, more inclusive recommendation platforms that enhance user experience across diverse musical contexts. The project first applied Random Forests and Gaussian Mixture Models with 5-fold cross-validation for audio genre identification, then advanced to CNNs on spectrogram heat maps validated via a train/validation/test split with early stopping, evaluated through accuracy, precision, recall, F1-score, confusion matrices, and ROC curves.\n\nNathan #_to_Do\n\n(Nathan) - sentence about results.\n\nBy applying statistical and time-frequency features to separated vocal and instrumental tracks, we evaluated the feasibility of machine learning models in song language recognition. Classical approaches such as Logistic Regression, Random Forests, and SVMs were trained with 5-fold cross-validation. Results showed that vocal-only features provided the strongest signals for classification. The analysis demonstrated that language prediction from raw audio is viable when leveraging targeted feature engineering.\n\n(Yashi) - sentence about Part 2. (Yashi) - sentence about results."
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "",
    "text": "Our team is developing a machine learning system as part of a larger AI-driven music recommendation service. The primary objectives are to build a model capable of recognizing the language(s) spoken in audio files and assessing whether new songs align with a user’s preferences. Understanding spoken language within music tracks, combined with genre classification, enables more personalized and accurate recommendations. This capability is essential for enhancing user experience by suggesting songs that resonate with individual tastes.\nThe challenge lies in effectively processing real audio data to extract meaningful features, classify genres, and interpret user listening histories. By leveraging signal processing and machine learning techniques, this project aims to automate and improve the recommendation process. Such advancements not only deepen our understanding of audio analysis but also pave the way for smarter, more intuitive music discovery platforms."
  },
  {
    "objectID": "proposal.html#proposal",
    "href": "proposal.html#proposal",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "",
    "text": "Our team is developing a machine learning system as part of a larger AI-driven music recommendation service. The primary objectives are to build a model capable of recognizing the language(s) spoken in audio files and assessing whether new songs align with a user’s preferences. Understanding spoken language within music tracks, combined with genre classification, enables more personalized and accurate recommendations. This capability is essential for enhancing user experience by suggesting songs that resonate with individual tastes.\nThe challenge lies in effectively processing real audio data to extract meaningful features, classify genres, and interpret user listening histories. By leveraging signal processing and machine learning techniques, this project aims to automate and improve the recommendation process. Such advancements not only deepen our understanding of audio analysis but also pave the way for smarter, more intuitive music discovery platforms."
  },
  {
    "objectID": "proposal.html#python-libraries",
    "href": "proposal.html#python-libraries",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "Python libraries",
    "text": "Python libraries\n\nimport os\nimport json\nimport subprocess\nimport numpy as np\nimport pandas as pd\n\n# === Machine Learning & Evaluation ===\nimport sklearn  # Models, preprocessing, cross-validation, metrics\nimport lightgbm as lgb  # Gradient boosting\nimport xgboost as xgb  # Gradient boosting\n#import surprise  # Consider removing if problematic, see alternatives\n\n# === Deep Learning Frameworks ===\nimport torch  # PyTorch (used with Demucs, CNNs, etc.)\nimport tensorflow as tf  # TensorFlow\nfrom tensorflow import keras  # Keras API\n\n# === Audio Processing ===\nimport librosa  # Feature extraction (ZCR, RMS, tempo, etc.)\nimport torchaudio  # Audio I/O and transformations with PyTorch\nfrom demucs.apply import apply_model\nfrom demucs.pretrained import get_model  # Vocal separation\n\n# === Visualization ===\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# === Display & Formatting (for .qmd / Jupyter) ===\nfrom IPython.display import display, HTML\n\n\nVocal track separation with - DEMUCS\nDemucs (Deep Extractor for Music Sources) is an open-source tool developed by Meta AI, designed for high-quality music source separation. Originally introduced to outperform traditional spectral mask methods, it uses a deep learning architecture based on convolutional layers and bidirectional LSTM (Long Short-Term Memory) to separate audio into distinct stems like vocals, drums, bass, and others. Unlike classical techniques that rely on frequency-domain heuristics, Demucs operates directly in the time domain, enabling precise extraction of overlapping audio components. For multilingual analysis, Demucs is especially effective because it isolates vocals based purely on acoustic features — not linguistic content — making it an ideal front-end for tasks like spoken language identification or lyric classification in machine learning pipelines.\n\n\nMinimal scripts - Using System Terminal Commands - DEMUCS\nScripts that run demucs using system commands—typically through Python’s subprocess or os libraries—offer a straightforward way to integrate audio separation tools into Python workflows while interacting with the operating system’s file structure and command-line utilities.\n\nimport os\nimport subprocess\n\n# Path to your input audio file\naudio_file = r\"~\\Gloria_Gaynor_I_Will_Survive.wav\"\n\n# Optional: check if file exists\nif not os.path.exists(audio_file):\n    raise FileNotFoundError(f\"Audio file not found: {audio_file}\")\n\n# Build the Demucs command\n# You can change --two-stems to 'drums' or 'bass' if needed\ncommand = [\n    \"demucs\",\n    \"--two-stems=vocals\",  # Extract vocals only\n    \"--out\", \"demucs_output\",  # Output folder\n    audio_file\n]\n\n# Run the command\nprint(\"🔄 Running Demucs...\")\nsubprocess.run(command)\n\nprint(\"✅ Separation complete. Check the 'demucs_output' folder for results.\")"
  },
  {
    "objectID": "proposal.html#dataset",
    "href": "proposal.html#dataset",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "Dataset",
    "text": "Dataset\n\nData - Provenence\nAll audio data used in this project was sourced from YouTube, following an automated pipeline designed to gather relevant tracks for training and evaluation:\n\nA list of artist names was manually curated.\nFor each artist, their top five songs were identified by scraping Google search results.\nYouTube links corresponding to those songs were retrieved using a custom scraping script.\nAudio from each YouTube video was downloaded and stored for analysis.\n\nIn addition to audio collection, a secondary scraper was developed to gather metadata. This tool extracted the most commonly associated genres and countries of origin for each artist by querying publicly available sources.\n\n\nData - Collection\nThe data collection process involved several custom Python scripts designed to scrape and download the necessary information and audio files:\nartist_5_song_list_scrape.py — Retrieves the top five songs per artist from Google search results.\nartist_genre_scrape.py — Gathers genre metadata for each artist from public sources.\nartist_country_of_origin_scrape.py — Extracts the country of origin for each artist.\naudio_scrape_wav_mp3.py — Downloads audio files from YouTube links in WAV and MP3 formats.\nTogether, these scripts automate the extraction of both audio data and relevant metadata to support training and evaluation of the recommendation system.\n\n\nData - Description\nThis table provides a comprehensive overview of the audio and metadata features considered during project development. It details each feature’s definition, data type, output format, and extraction notes. While some features may not be ultimately used, the table serves as a complete reference of potential inputs for modeling musical and audio characteristics. All features requiring extraction have, at least prima facie, been verified against available Python libraries capable of performing the extraction.\n\n\n\n\n\n\n\nTable 1: Summary of Audio Features (Simplified)\n\n\n\n\n\n \nFeature\nDefinition\nVariable Type\nOutput Type\nNotes\n\n\n\n\n0\nArtist\nName of the performing artist or band.\nString\nSingle string\nUsually human entered or scraped from metadata.\n\n\n1\nSong Title\nTitle of the song or track.\nString\nSingle string\nUsually human entered or scraped from metadata.\n\n\n2\nGenre\nMusical style or category (may be multiple).\nString Array\nList of strings\nOften human-assigned or from databases.\n\n\n3\nMean\nAverage value of an audio feature over time.\nNumeric\nSingle number\nUse numpy/scipy on time-series features.\n\n\n4\nVariance\nSpread or variability of the feature.\nNumeric\nSingle number\nUse numpy.var() on time-varying data.\n\n\n5\nSkewness\nAsymmetry of the feature distribution.\nNumeric\nSingle number\nUse scipy.stats.skew() on feature arrays.\n\n\n6\nKurtosis\nHeaviness of tails in the feature distribution.\nNumeric\nSingle number\nUse scipy.stats.kurtosis() for tail behavior.\n\n\n7\nZero Crossing Rate\nRate at which waveform crosses zero amplitude.\nNumeric\nSingle number\nlibrosa.feature.zero_crossing_rate().\n\n\n8\nRMS Energy\nRoot mean square of amplitude (loudness).\nNumeric\nSingle number\nlibrosa.feature.rms() or similar.\n\n\n9\nLoudness\nPerceived loudness (in dB).\nNumeric\nSingle number\nEstimate via RMS or pydub gain.\n\n\n10\nEnergy\nTotal signal energy over time.\nNumeric\nSingle number\nMean RMS squared or signal power.\n\n\n11\nTempo\nBeats per minute (BPM).\nNumeric\nSingle number\nlibrosa.beat.beat_track().\n\n\n12\nDanceability\nSuitability of the track for dancing.\nNumeric\nApproximate\nEstimated with ML or Essentia.\n\n\n13\nKey / Key Name\nMusical key (e.g., C, F#, A minor).\nCategorical\nSingle string\nFrom chroma feature analysis.\n\n\n14\nMode / Mode Name\nTonality: major or minor.\nCategorical\nSingle string\nDerived from key/chroma analysis.\n\n\n15\nFFT\nAmplitude vs frequency via FFT.\nNumeric Array\n1D Array\nnp.fft.fft() + abs().\n\n\n16\nSTFT\nShort-Time Fourier Transform (time-frequency).\nComplex Array\n2D Matrix\nlibrosa.stft().\n\n\n17\nMel-Spectrogram\nMel-scaled spectrogram representation.\nNumeric Array\n2D Matrix\nlibrosa.feature.melspectrogram().\n\n\n18\nFreq vs dB Spectrum\nSpectrum in decibel scale.\nNumeric Array\n1D/2D Matrix\nlibrosa.power_to_db() on FFT or STFT.\n\n\n19\nDuration\nLength of audio in milliseconds.\nNumeric\nSingle number\nlibrosa.get_duration() * 1000.\n\n\n\n\n\n\n\n\n\n\nMachine Learning Nomenclature\nThis table defines key terms related to the concept of “tokens” in the context of music machine learning. It clarifies how raw audio features, sequences, and categorical metadata can be represented as tokens for model input. Understanding these distinctions is essential for designing effective audio classification and recommendation systems.\n\n\n\n\n\n\n\nTable 2: Token Definitions in Music ML Context\n\n\n\n\n\n \nTerm\nExample token in your project\n\n\n\n\n0\nToken\nSingle time-window feature vector or metadata category\n\n\n1\nSequence\nOrdered series of audio feature vectors (like frames in time)\n\n\n2\nVocabulary\nSet of all possible genres, artists, keys, or discretized audio features\n\n\n\n\n\n\n\n\n\n\nData - Storage\nThe example JSON structure below represents a data entry for an artist and their song metadata, including audio file links, user feedback options, and categorical tags. This format organizes information for easy ingestion by machine learning pipelines or user interfaces, capturing both objective metadata (artist, categories, audio features) and subjective user ratings using Likert scales of varying lengths.\nAudio files and any complex metadata such as multi-dimensional arrays (e.g., spectrograms) will be stored in separate folders, with their file paths referenced in the JSON under the AudioFile node.\nJSON Node Storage Categories\nIn hierarchical data structures like JSON, a node represents a single element in the tree. Nodes can be categorized by the type of data they store and their structural role. Here is a list of the strucured schemea in our JSON:\nAudioFile: Nested object containing lists of links to YouTube, WAV, and MP3 versions of the audio files.\nRegion, User: Lists to hold geographic metadata and user identifiers. The User field, along with the Likert rating arrays, are pre-built placeholders designed to support future features such as user accounts and the collection of personalized likeability or preference ratings.\nData: A comprehensive sub-node capturing detailed metadata about the audio, including:\n\nArtist and song title\nGenre(s)\nQuantitative audio features such as mean, variance, skewness, kurtosis, zero crossing rate, RMS energy, loudness, energy, tempo, danceability\nMusical attributes like key and mode\nComplex audio representations like FFT, STFT, mel-spectrogram, frequency vs dB spectrum\nDuration in milliseconds\n\nLikert_2, Likert_3, Likert_5: Arrays representing different Likert scales, each with entries for score, label, color, description, and selection status to capture nuanced user feedback.\ncategory: An array of genre tags associated with the song.\nThis JSON schema is designed to be flexible and extensible, accommodating rich metadata and user feedback for building and improving recommendation systems. Below is a partially filled Node, with all schema present.\n\n\n\n\n{\n    \"AudioFile\": {\n        \"yt_link\": [],\n        \"wav_link\": [],\n        \"mp3_link\": []\n    },\n    \"Region\": [\n        \"America\"\n    ],\n    \"User\": [\n        \"Nathan\"\n    ],\n    \"Data\": {\n        \"Artist\": \"Lady Gaga\",\n        \"Song Title\": \"\",\n        \"Genre\": [],\n        \"Mean (of features)\": null,\n        \"Variance\": null,\n        \"Skewness\": null,\n        \"Kurtosis\": null,\n        \"Zero Crossing Rate\": null,\n        \"RMS Energy\": null,\n        \"Loudness\": null,\n        \"Energy\": null,\n        \"Tempo\": null,\n        \"Danceability\": null,\n        \"Key / Key Name\": \"\",\n        \"Mode / Mode Name\": \"\",\n        \"FFT (Amplitude vs Frequency)\": null,\n        \"STFT (Short-Time Fourier Transform)\": null,\n        \"Mel-Spectrogram\": null,\n        \"Frequency vs dB Spectrum\": null,\n        \"Duration (ms)\": null\n    },\n    \"Likert_2\": [\n        {\n            \"score\": 1,\n            \"label\": \"No\",\n            \"color\": \"#FF4C4C\",\n            \"selected\": true\n        },\n        {\n            \"score\": 2,\n            \"label\": \"Yes\",\n            \"color\": \"#4CAF50\",\n            \"selected\": false\n        }\n    ],\n    \"Likert_3\": [\n        {\n            \"score\": 1,\n            \"label\": \"Dislike\",\n            \"description\": \"I do not like this\",\n            \"color\": \"#FF4C4C\",\n            \"selected\": false\n        },\n        {\n            \"score\": 2,\n            \"label\": \"Meh\",\n            \"description\": \"Neutral or indifferent\",\n            \"color\": \"#FFD700\",\n            \"selected\": true\n        },\n        {\n            \"score\": 3,\n            \"label\": \"Like\",\n            \"description\": \"I like this\",\n            \"color\": \"#4CAF50\",\n            \"selected\": false\n        }\n    ],\n    \"Likert_5\": [\n        {\n            \"score\": 1,\n            \"label\": \"Strongly Dislike\",\n            \"description\": \"I strongly dislike this genre/song\",\n            \"color\": \"#FF4C4C\",\n            \"selected\": false\n        },\n        {\n            \"score\": 2,\n            \"label\": \"Dislike\",\n            \"description\": \"I don\\u2019t enjoy this genre/song\",\n            \"color\": \"#FF8C00\",\n            \"selected\": false\n        },\n        {\n            \"score\": 3,\n            \"label\": \"Neutral\",\n            \"description\": \"Neither like nor dislike\",\n            \"color\": \"#FFD700\",\n            \"selected\": true\n        },\n        {\n            \"score\": 4,\n            \"label\": \"Like\",\n            \"description\": \"I like this genre/song\",\n            \"color\": \"#90EE90\",\n            \"selected\": false\n        },\n        {\n            \"score\": 5,\n            \"label\": \"Strongly Like\",\n            \"description\": \"I strongly like or love this genre/song\",\n            \"color\": \"#008000\",\n            \"selected\": false\n        }\n    ],\n    \"category\": [\n        \"rock\",\n        \"pop\",\n        \"electronic pop\",\n        \"jazz pop\"\n    ]\n}\n\n\n\n\n\nLikert Scale Feature - indepth explanation\nThe Likert score will most likely be implemented in the second phase of the project. It was included in the initial design of the .JSON file with the idea that incorporating it early would enable potential use in the current phase and support software design aligned with future phases or iterations of the project. The intended use of the Likert score is as a UI/UX feature for users. The initial approach will involve encoding the Likert scores and using them as input features, rather than as target variables. The machine learning component will aim to provide a gradient of score values, allowing users to express preferences (likes/dislikes) with greater precision. The goal is to develop a model capable of handling varying Likert scale gradients and their corresponding precision for more accurate recommendations..\nIn our design, each user will have their own Likert score database (or user-specific rating entries) linked to their unique user ID. This ensures that preference modeling and recommendation outputs are truly personalized. The idea is to allow users to “score” their like or dislike of individual songs and then see how the recommendation model adapts to those inputs. To make the interaction more engaging, users can choose between a 2-, 3-, or 5-point Likert scale.\n\n\nData - Second Tier features\nThe second tier of data features will focus on vocal track extraction, utilizing the DEMUCS library. Currently, the plan is to develop a dedicated script that processes each song’s audio files—both .mp3 and .wav formats—and stores the resulting vocal isolation data in individual, well-organized folders. As additional processing requirements emerge, the structure of the master JSON file may need to be adapted or reorganized to accommodate these new data components seamlessly. This approach ensures flexibility and scalability in handling complex audio feature sets while maintaining clear data management practices.\n\n\nThe Audiophiles custom audiofile ui\nA custom audio player UI that supports .wav and .mp3 files and dynamically renders decibel versus frequency spectrograms for each song. The player features real-time analysis with frequency, time, and dB spectrogram visualizations synchronized to the playback - i.e., a scrolling frequency,time,dB heat map. Additionally, there may be a need to build a JSON reader to facilitate the processing and aggregation of Likert scale scores associated with the songs.\n\n\n\n\n\nFigure 1: ‘Mk_1’ - Custom audio player UI displaying real-time frequency, time, and dB spectrograms synchronized to playback.\n\n\nAdditional Script info, Data Storage, Data Extraction:\nAll additional processing tasks are handled via custom Python scripts. These scripts include tools for downloading .mp3 and .mp4 files, and will be maintained individually by each team member. Due to the volume of data involved—approximately 200 .wav files and 200 .mp3 files—these audio assets will be stored locally on each user’s machine and not uploaded to GitHub.\nOnce metadata has been successfully extracted and organized, it can be safely stored and versioned within the GitHub repository. Similarly, any machine learning models developed throughout the project will be saved in the GitHub repo for reproducibility and collaboration.\nScripts for metadata extraction will be developed by Nathan and distributed to the rest of the team. Additional scripts and processing pipelines required for addressing specific research tasks will be created by individual team members as needed."
  },
  {
    "objectID": "proposal.html#individual-duties",
    "href": "proposal.html#individual-duties",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "Individual Duties",
    "text": "Individual Duties\nNathan – Problem #2: User Song Recommendation\n\nMetadata JSON Schema Design\nDesigned and implemented the nested .json structure to store song metadata, audio links, user ratings, and extracted audio features.\nMetadata Extraction Scripts\nResponsible for writing and maintaining Python scripts to:\n\nScrape artist genres, countries of origin, and song lists\n\nRetrieve YouTube links\n\nOrganize metadata for ingestion and storage\n\nAudio Visualization Interface\nDeveloped a custom .wav/.mp3 player with:\n\nReal-time frequency vs. dB spectrograms\n\nStatic visualizations for spectral energy distribution\n\nInteractive time-frequency-dB spectrograms during playback\n\nDemucs Integration and Vocal Separation\nWriting scripts to:\n\nApply the Demucs library for isolating vocal tracks\n\nStore separated .wav and .mp3 files in structured folders for analysis\n\nUser Interface & Data Display\nBuilding HTML/Quarto-styled displays for rendering .json metadata and user feedback in a human-readable format.\n\n\nYashi – Problem #1: Language Recognition from Audio\n\nMachine Learning Models\nLeading development of:\n\nLanguage classification models using vocal segments\n\nPipelines for training, validation, and testing\n\nAudio Feature Extraction\nResponsible for extracting statistical features such as:\n\nZero Crossing Rate (ZCR), Root Mean Square (RMS), tempo, and FFT\n\nTransforming raw waveform data into usable feature vectors\n\nRecommendation System Components\nAssisting in the design of:\n\nFeature-based comparison systems for future personalized recommendations\n\nMethods for encoding user preferences and behavior patterns\n\nModel Evaluation and Testing\nConducting:\n\nAccuracy and performance evaluations of models\n\nError analysis using tools such as confusion matrices and cross-validation\n\n\n\nJoint Responsibilities\nBoth team members will independently construct their machine learning pipelines. Each pipeline will be trained, tested, evaluated, and iteratively improved based on problem-specific goals. Coordination will ensure that feature extraction and data preprocessing remain compatible across both tasks.\n\n🗂️ Workflow Plan (Final 2.25 Weeks)\n\n\n\n\n\n\n\n\nPhase\nDates\nTasks\n\n\n\n\nPhase 1: Script Finalization & Distribution\nAug 1 – Aug 3\n- Finalize all data scraping & audio processing scripts- Distribute scripts to team members- Confirm runtime & environment setup\n\n\nPhase 2: Data Collection & Organization\nAug 4 – Aug 6\n- Each user runs scripts locally- Collect ~200 .mp3 and .wav files per user- Store audio and metadata in standardized folder structure\n\n\nPhase 3: Metadata Processing\nAug 7 – Aug 8\n- Parse, clean, and validate .json metadata- Integrate new entries into master metadata files- Ensure feature coverage (e.g., genre, tempo, duration)\n\n\nPhase 4: ML Pipeline Construction\nAug 9 – Aug 10\n- Each user builds their custom ML pipeline- Define preprocessing, feature extraction, model architecture\n\n\nPhase 5: ML Testing & Iteration – Round 1\nAug 11 – Aug 13\n- Run training and validation pipelines- Tune hyperparameters- Log and assess intermediate results\n\n\nPhase 6: ML Testing & Iteration – Round 2\nAug 14 – Aug 16\n- Refine pipelines based on feedback- Add second-tier features (e.g., vocal-only inputs)- Evaluate early model performance\n\n\nPhase 7: Final Evaluation & Model Selection\nAug 17 – Aug 18\n- Select best models per user task- Create evaluation reports- Generate confusion matrices, ROC curves, etc.\n\n\nPhase 8: Write-Up & Presentation Prep\nAug 19 – Aug 21\n- Complete final Quarto write-up- Polish visualizations and tables- Build and rehearse project presentation"
  },
  {
    "objectID": "proposal.html#questions",
    "href": "proposal.html#questions",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "Questions",
    "text": "Questions\n\n1. Language Recognition with Separated Vocal & Audio Tracks\nHow can we leverage statistical and time-frequency features extracted from separated vocal and audio tracks to build effective language recognition models? Specifically, how can traditional machine learning methods — ranging from classical classifiers on simple statistical summaries to Gaussian Mixture Models on richer time-frequency features — be applied in this context?\n\nWhat are the key benefits and limitations of these approaches?\n\nHow can careful feature engineering, feature integration, and thorough model evaluation improve the accuracy and robustness of language recognition systems?\n\nHow do model results compare and contrast when using .wav files versus .mp3 files?\n\n\n\n\n2. Recommendation Systems Using Audio Features & User Data\nHow can user interaction data, combined with basic track metadata and simple audio features, be used to build an effective recommendation system using collaborative filtering and traditional machine learning methods?\n\nFurthermore, how can advanced audio features, dimensionality reduction, and clustering techniques improve personalized recommendations by better capturing user preferences and track characteristics from both vocal and non-vocal components?\n\nHow do recommendation model results compare and contrast when using .wav files versus .mp3 files, considering the potential impact of audio quality and compression artifacts on feature extraction and recommendation performance?"
  },
  {
    "objectID": "proposal.html#analysis-plan",
    "href": "proposal.html#analysis-plan",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "Analysis plan",
    "text": "Analysis plan\n\nPreamble: Easy and Medium Paths\nThe Easy Path serves as a minimal, foundational implementation aimed at quickly establishing a baseline for language recognition using straightforward statistical features extracted from separated vocal and audio tracks. It relies on classical machine learning models such as Logistic Regression, Random Forest, and Support Vector Machines, which are simple to train and interpret.\nThe Medium Path provides a more detailed approach that extends beyond simple statistics by incorporating time-frequency features such as Mel-Frequency Cepstral Coefficients (MFCCs), spectral centroid, and bandwidth from both vocal and non-vocal tracks. Instead of deep learning, this path uses classical probabilistic models like Gaussian Mixture Models (GMMs), Hidden Markov Models (HMMs), or advanced classical classifiers trained on aggregated time-frequency features. This allows capturing richer audio characteristics while maintaining interpretability and computational efficiency.\n\n\nAnalysis Plan for Problem 1: Language Recognition\n\n\n1. Data Preparation & Feature Extraction - problem 1\n\nLoad separated vocal and instrumental audio tracks for each sample in both .wav and .mp3 formats.\n\nEasy Path: Extract statistical features (mean, variance, skewness, kurtosis, RMS energy, zero crossing rate, tempo, loudness) separately from vocal and audio tracks.\n\nMedium Path: Extract time-frequency features such as MFCCs, spectral centroid, bandwidth, or STFT for vocal and audio tracks. Aggregate these features by computing summary statistics (mean, variance).\n\nNormalize numerical features (StandardScaler or Min-Max scaling).\n\nEncode categorical metadata if available (e.g., one-hot encoding for language labels or artist).\n\n\n\n2. Model Construction & Training - problem 1\n\nEasy Path: Train classical classifiers — Logistic Regression, Random Forest, Support Vector Machines — on statistical feature vectors.\n\nMedium Path:\n\nApply K-Means clustering on aggregated time-frequency features to group similar audio patterns unsupervised, enhancing feature representation.\n\nTrain classical models suited for time-frequency data — Gaussian Mixture Models (GMMs), Hidden Markov Models (HMMs) (optional), or classical classifiers on combined original and cluster-based features.\n\n\nPerform hyperparameter tuning via grid or random search where applicable.\n\n\n\n3. Validation - problem 1\n\nUse stratified K-fold cross-validation to ensure balanced representation of languages in training and test splits.\n\nEvaluate model performance on validation folds.\n\n\n\n4. Performance Evaluation - problem 1\n\nMetrics: Accuracy, Precision, Recall, F1-score.\n\nGenerate confusion matrices to analyze language-specific errors.\n\nConduct ablation studies comparing vocal-only, audio-only, and combined vocal + audio features.\n\nCompare model performance and feature extraction quality between .wav and .mp3 audio formats to assess the impact of audio compression and quality differences on recognition accuracy.\n\n\n\nPutative Machine Learning Techniques - problem 1\n\nLogistic Regression, Random Forest, Support Vector Machines (SVM)\n\nGaussian Mixture Models (GMM), Hidden Markov Models (HMM) (optional)\n\nK-Means Clustering (unsupervised feature grouping)\n\nFeature normalization/scaling (StandardScaler, Min-Max)\n\nEncoding categorical features (One-hot, Label Encoding)\n\nStratified K-Fold cross-validation\n\nHyperparameter tuning (Grid Search, Random Search)\n\nEvaluation metrics: Accuracy, Precision, Recall, F1-score, Confusion Matrix\n\nAblation analysis for feature contribution\n\n\n\n\nAnalysis Plan for Problem 2: User Song Recommendation\n\n\n1. Data Preparation & Feature Extraction - problem 2\n\nLoad user interaction data combined with track metadata (artist, genre, audio features) in both .wav and .mp3 formats.\n\nEncode categorical metadata (one-hot, label encoding, or embeddings).\n\nNormalize numerical features (Min-Max scaling, StandardScaler).\n\nConstruct user-item interaction matrix from implicit feedback or ratings.\n\n\n\n2. Model Construction - problem 2\n\nEasy Path: Collaborative filtering using K-Nearest Neighbors or Logistic Regression leveraging metadata and user preferences.\n\nMedium Path:\n\nUse K-Means clustering or Hierarchical clustering on track features to identify similar groups of songs or users unsupervised.\n\nBuild tree-based classifiers (Random Forest, Gradient Boosting Machines) on clustered feature groups for content-based filtering.\n\n\n\n\n3. Training & Validation - problem 2\n\nTrain-Test splits or Stratified K-Fold cross-validation.\n\nHyperparameter tuning via grid or random search.\n\n\n\n4. Performance Evaluation - problem 2\n\nMetrics: Precision@K, Recall@K, Accuracy, F1-score.\n\nOffline validation on held-out test sets.\n\nAnalyze recommendation relevance and diversity.\n\nPerform ablation studies comparing models built with vocal-only features vs combined vocal + audio features.\n\nCompare recommendation system performance using features extracted from .wav versus .mp3 files to understand the effect of audio quality and compression artifacts.\n\n\n\nPutative Machine Learning Techniques - problem 2\n\nCollaborative Filtering: K-Nearest Neighbors, Logistic Regression for implicit feedback\n\nContent-Based Filtering: K-Means, Hierarchical Clustering (unsupervised grouping), Random Forest, Gradient Boosting Machines\n\nCross-validation: Train-Test Split, Stratified K-Fold CV\n\nFeature Engineering: One-hot encoding, embeddings, normalization (Min-Max, StandardScaler)\n\nEvaluation Metrics: Precision@K, Recall@K, Accuracy, F1-score\n\nAblation Analysis: Assess impact of vocal vs non-vocal feature inclusion"
  },
  {
    "objectID": "proposal.html#repo-oraganization",
    "href": "proposal.html#repo-oraganization",
    "title": "Audio Alchemy - with The AudioPhiles",
    "section": "Repo Oraganization",
    "text": "Repo Oraganization\n\n_extra: Houses supplementary project materials such as problem statements, ML library documentation, and feature lists. Serves as a flexible space for reference materials.\n\n0_problem_statements：Contains structured descriptions of the project’s problem statements, including tiered Easy/Medium/Hard pipelines.\nmL_lib_info：Holds reference documents describing the machine learning techniques considered for the project.\naudio_features_mk1.csv：A CSV file listing all audio features to be extracted, including definitions, variable types, and extraction methods.\ncode.qmd：A Quarto markdown file containing core code and documentation for the project.\nexample_web_site.md: Example documentation for a project website setup.\nog_data_vars_defs.py: Python definitions for handling and processing project data variables.\nREADME.md: Main project overview, setup instructions, and usage guidelines.\n\n_freeze: Contains frozen Quarto document builds for reproducibility, organized by document (about, index, presentation, proposal).\ngithub: Contains GitHub-specific configuration, including:\n\nISSUE_TEMPLATE: Templates for creating consistent GitHub issues.\nworkflows: GitHub Actions workflows for automation (e.g., building Quarto site).\n\ndata: Houses datasets and associated documentation:\n\ncustomtheming.scss: Custom SCSS styling for the Quarto output.\nREADME.md: Data usage description.\n\ndocs: Contains rendered Quarto output for deployment (e.g., GitHub Pages):\n\n_extra: Supplementary files included in documentation.\nsite_libs: JavaScript and CSS libraries for the generated site.\nindex.html: Rendered project index page.\nproposal.html: Rendered proposal page.\nsearch.json: Search index for the site.\n\nimages：Contains project image assets, including visualizations, diagrams, and decorative images for presentation\npresentation_files：Stores materials supporting the final presentation, such as figures and supplementary assets.\ngitignore：Specifies files and folders to be excluded from Git version control.\nabout.qmd：A Quarto document providing background on the project purpose and introducing team members.\nindex.qmd：The main Quarto page for the project, containing the core narrative, methodology, code, visualizations, and results.\npresentation.qmd：A Quarto file containing the final presentation slides for the project results.\nproposal.qmd：A Quarto file for the project proposal, including dataset descriptions, problem statements, analysis plan, and weekly plan.\nREADME.md：The main project README file, summarizing project objectives, setup instructions, and usage guidelines.\n\n\n\n\n\n\n\nNote\n\n\n\nNote: Current Proposal 1.0.0. Subject to change.\n(version.feature.patch notation)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project was developed by [Team Name] For INFO 523 - Data Mining and Discovery at the University of Arizona, taught by Dr. Greg Chism. The team is comprised of the following team members.\n\nTeam member 1: Yashi Mi - Data Science MS student - year 1.\nTeam member 2: Nathan Herling - Data Science MS student - year 1."
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Project title",
    "section": "",
    "text": "The presentation is created using the Quarto CLI\n## sets the start of a new slide\n\n\n\n\nYou can use plain text\n\n\n\nor bullet points1\n\n\nor in two columns\n\n\n\nlike\nthis\n\n\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.073\nModel:                            OLS   Adj. R-squared:                  0.070\nMethod:                 Least Squares   F-statistic:                     30.59\nDate:                Mon, 28 Jul 2025   Prob (F-statistic):           5.84e-08\nTime:                        07:49:21   Log-Likelihood:                -1346.4\nNo. Observations:                 392   AIC:                             2697.\nDf Residuals:                     390   BIC:                             2705.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         35.8015      2.266     15.800      0.000      31.347      40.257\nspeed       -354.7055     64.129     -5.531      0.000    -480.788    -228.623\n==============================================================================\nOmnibus:                       27.687   Durbin-Watson:                   0.589\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               18.976\nSkew:                           0.420   Prob(JB):                     7.57e-05\nKurtosis:                       2.323   Cond. No.                         169.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome text\ngoes here"
  },
  {
    "objectID": "presentation.html#quarto",
    "href": "presentation.html#quarto",
    "title": "Using Quarto for presentations",
    "section": "Quarto",
    "text": "Quarto\n\nThe presentation is created using the Quarto CLI\n## sets the start of a new slide"
  },
  {
    "objectID": "presentation.html#layouts",
    "href": "presentation.html#layouts",
    "title": "Using Quarto for presentations",
    "section": "Layouts",
    "text": "Layouts\nYou can use plain text\n\n\n\nor bullet points1\n\n\nor in two columns\n\n\nlike\nthis\n\nAnd add footnotes"
  },
  {
    "objectID": "presentation.html#code",
    "href": "presentation.html#code",
    "title": "Using Quarto for presentations",
    "section": "Code",
    "text": "Code\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.073\nModel:                            OLS   Adj. R-squared:                  0.070\nMethod:                 Least Squares   F-statistic:                     30.59\nDate:                Mon, 18 Aug 2025   Prob (F-statistic):           5.84e-08\nTime:                        08:56:08   Log-Likelihood:                -1346.4\nNo. Observations:                 392   AIC:                             2697.\nDf Residuals:                     390   BIC:                             2705.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         35.8015      2.266     15.800      0.000      31.347      40.257\nspeed       -354.7055     64.129     -5.531      0.000    -480.788    -228.623\n==============================================================================\nOmnibus:                       27.687   Durbin-Watson:                   0.589\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               18.976\nSkew:                           0.420   Prob(JB):                     7.57e-05\nKurtosis:                       2.323   Cond. No.                         169.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "presentation.html#plot-and-text",
    "href": "presentation.html#plot-and-text",
    "title": "Using Quarto for presentations",
    "section": "Plot and text",
    "text": "Plot and text\n\n\n\nSome text\ngoes here"
  },
  {
    "objectID": "presentation.html#tables",
    "href": "presentation.html#tables",
    "title": "Using Quarto for presentations",
    "section": "Tables",
    "text": "Tables\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\n\n\n\nisland\n\n\n\nbill_length_mm\n\n\n\nbill_depth_mm\n\n\n\nflipper_length_mm\n\n\n\nbody_mass_g\n\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.1\n\n\n\n18.7\n\n\n\n181.0\n\n\n\n3750.0\n\n\n\nMale\n\n\n\n\n\n\n\n1\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.5\n\n\n\n17.4\n\n\n\n186.0\n\n\n\n3800.0\n\n\n\nFemale\n\n\n\n\n\n\n\n2\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n40.3\n\n\n\n18.0\n\n\n\n195.0\n\n\n\n3250.0\n\n\n\nFemale\n\n\n\n\n\n\n\n4\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n36.7\n\n\n\n19.3\n\n\n\n193.0\n\n\n\n3450.0\n\n\n\nFemale\n\n\n\n\n\n\n\n5\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.3\n\n\n\n20.6\n\n\n\n190.0\n\n\n\n3650.0\n\n\n\nMale"
  },
  {
    "objectID": "presentation.html#images",
    "href": "presentation.html#images",
    "title": "Using Quarto for presentations",
    "section": "Images",
    "text": "Images\n\nImage credit: Danielle Navarro, Percolate."
  },
  {
    "objectID": "presentation.html#math-expressions",
    "href": "presentation.html#math-expressions",
    "title": "Using Quarto for presentations",
    "section": "Math Expressions",
    "text": "Math Expressions\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g. $\\alpha+\\beta$ renders \\(\\alpha + \\beta\\). You can use the display style with double dollar signs:\n$$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n\\[\n\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\]\nLimitations:\n\nThe source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting $$ must appear in the very beginning of a line, followed immediately by a non-space character, and the ending $$ must be at the end of a line, led by a non-space character;\nThere should not be spaces after the opening $ or before the closing $."
  },
  {
    "objectID": "presentation.html#feeling-adventurous",
    "href": "presentation.html#feeling-adventurous",
    "title": "Using Quarto for presentations",
    "section": "Feeling adventurous?",
    "text": "Feeling adventurous?\n\nYou are welcomed to use the default styling of the slides. In fact, that’s what I expect majority of you will do. You will differentiate yourself with the content of your presentation.\nBut some of you might want to play around with slide styling. Some solutions for this can be found at https://quarto.org/docs/presentations/revealjs."
  },
  {
    "objectID": "presentation.html#footnotes",
    "href": "presentation.html#footnotes",
    "title": "Project title",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd add footnotes↩︎"
  },
  {
    "objectID": "presentation.html#plots",
    "href": "presentation.html#plots",
    "title": "Using Quarto for presentations",
    "section": "Plots",
    "text": "Plots"
  },
  {
    "objectID": "Yashi analysis.html",
    "href": "Yashi analysis.html",
    "title": "Load the dataset",
    "section": "",
    "text": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from pandas.plotting import scatter_matrix\nfrom sklearn.impute import SimpleImputer from sklearn.preprocessing import LabelEncoder, StandardScaler from sklearn.feature_selection import SelectKBest, f_regression from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression, Ridge, RidgeCV, LassoCV from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, make_scorer, precision_score, recall_score, f1_score\n\nLoad the dataset\ndf = pd.read_csv(“_extra/Audio_Files/Yashi_s_Music/Y_audio_philes_final_features_mk1_filled_Yashi.csv”)\nprint(df.head()) print(df.info()) print(“Missing values per column:”) print(df.isnull().sum())\n\n\nFeature selection\ndf_non_feature = [“artist”, “country”, “language”, “track_type”] df_number_feature = [c for c in df.select_dtypes(include=[np.number]).columns if c not in df_non_feature] print(“Numerical features used:”, df_number_feature)\n\n\nStandardize numerical features globally\nscaler = StandardScaler() df[df_number_feature] = scaler.fit_transform(df[df_number_feature])\n\n\nEncode target variable\nle = LabelEncoder() y_all = le.fit_transform(df[“language”].astype(str)) label_names = le.classes_\n\n\nDefine track_type groups\ntrack_type_all = {0: “complete_song”, 1: “vocal_only”, 2: “no_vocal”} track_type_specs = {name: (df[“track_type”] == code) for code, name in track_type_all.items()} for name, mask in track_type_specs.items(): print(f”{name}: {int(mask.sum())} samples”)\n\n\nSet up Stratified K-Fold cross-validation\nk = 5 skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n\n\nDefine models\nmodels = { “LogReg”: LogisticRegression(max_iter=500, class_weight=“balanced”, random_state=42), “RandomForest”: RandomForestClassifier( n_estimators=400, class_weight=“balanced_subsample”, random_state=42, n_jobs=-1 ), “SVM_linear”: SVC(kernel=“linear”, class_weight=“balanced”, random_state=42), }\n\n\nDefine scoring dictionary with exact metric names\nscoring = { “accuracy”: make_scorer(accuracy_score), “precision”: make_scorer(precision_score, average=“macro”), “recall”: make_scorer(recall_score, average=“macro”), “f1”: make_scorer(f1_score, average=“macro”), }\nrows = []\n\n\nModel evaluation loop\nfor track_type_code, ablation_name in track_type_all.items(): mask = df[“track_type”] == track_type_code df_eval = df.loc[mask].reset_index(drop=True)\nX = df_eval[df_number_feature].copy()\ny = le.transform(df_eval[\"language\"].astype(str))\n\nfor name, clf in models.items():\n    # Use skf and n_jobs=1 to avoid multiprocessing errors\n    cv_results = cross_validate(\n        clf,\n        X,\n        y,\n        cv=skf,\n        scoring=scoring,\n        n_jobs=1,\n        return_train_score=False,\n    )\n    acc = np.mean(cv_results[\"test_accuracy\"])\n    prec = np.mean(cv_results[\"test_precision\"])\n    rec = np.mean(cv_results[\"test_recall\"])\n    f1 = np.mean(cv_results[\"test_f1\"])\n    rows.append({\n        \"ablation\": ablation_name,\n        \"model\": name,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1,\n    })\n    print(f\"[{ablation_name} - {name}] Acc={acc:.3f} | Prec={prec:.3f} | Rec={rec:.3f} | F1={f1:.3f}\")\n\n\nFinal model selection\nchosen_ablation = “vocal_only” chosen_model = “SVM_linear” print(f”[Final model] Ablation = {chosen_ablation} | Model = {chosen_model}“)\ndf_best = df.loc[track_type_specs[chosen_ablation]].reset_index(drop=True) X_best = df_best[df_number_feature].copy() y_best = le.transform(df_best[“language”].astype(str))\nfinal_clf = SVC(kernel=“linear”, class_weight=“balanced”, random_state=42) final_clf.fit(X_best, y_best)\nprint(f”[Final Train] Done on {chosen_ablation} - {chosen_model} | n_samples = {len(df_best)}“)"
  },
  {
    "objectID": "presentation.html#our-dataset",
    "href": "presentation.html#our-dataset",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-1",
    "href": "presentation.html#our-dataset-1",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-2",
    "href": "presentation.html#our-dataset-2",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-3",
    "href": "presentation.html#our-dataset-3",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-4",
    "href": "presentation.html#our-dataset-4",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-5",
    "href": "presentation.html#our-dataset-5",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-6",
    "href": "presentation.html#our-dataset-6",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-7",
    "href": "presentation.html#our-dataset-7",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-8",
    "href": "presentation.html#our-dataset-8",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-9",
    "href": "presentation.html#our-dataset-9",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-10",
    "href": "presentation.html#our-dataset-10",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-11",
    "href": "presentation.html#our-dataset-11",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-12",
    "href": "presentation.html#our-dataset-12",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-13",
    "href": "presentation.html#our-dataset-13",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#our-dataset-14",
    "href": "presentation.html#our-dataset-14",
    "title": "",
    "section": "Our Dataset",
    "text": "Our Dataset"
  },
  {
    "objectID": "presentation.html#questionsgoals",
    "href": "presentation.html#questionsgoals",
    "title": "",
    "section": "Questions/Goals",
    "text": "Questions/Goals"
  },
  {
    "objectID": "presentation.html#data-filesfeature-extraction",
    "href": "presentation.html#data-filesfeature-extraction",
    "title": "",
    "section": "Data Files/Feature Extraction",
    "text": "Data Files/Feature Extraction\n\n##Feature Scraping\n\n##Q1 overview\n\n##Q1 results [1/3]\n\n##Q1 results [2/3]\n\n##Q1 summary [3/3]\n\n##Q2 overview\n\n##Q2 results [1/3]\n\n##Q2 results [2/3]\n\n##Q2 results [3/3]\n\n##Q2 Summary\n\n##Full Summary\n\n##In closing ## Our Dataset"
  },
  {
    "objectID": "presentation.html#section",
    "href": "presentation.html#section",
    "title": "",
    "section": "",
    "text": "🤖 Project Description & Goals\n\n  Our team is developing a machine learning system for an AI-driven music recommendation service.\n  The main goals are:\n\n  \n    Build a model capable of recognizing the language(s) spoken in audio files.\n    Assess whether new songs align with a user’s musical preferences.\n  \n\n  \n\n  ✦ Question 1 – Language Recognition\n  How can we leverage statistical and time-frequency features from separated vocal and audio tracks to build effective language recognition models?\n  \n    What are the strengths and limitations of classical models (e.g., SVMs, Random Forest, Logistic Regression)?\n    How do feature engineering and validation improve model robustness?\n  \n\n  \n\n  ✦ Question 2 – Recommendation System [Genre Recognition]\n  Starting with genre recognition as part of the user UI/UX, can a user’s preferred genres be learned through feature analysis of audio files?\n  \n    Using classical supervised models: Can audio features be extracted and models trained to recognize a user's preferred genres?\n    Using frequency/time heatmap image extraction: Are classical models capable of determining musical genres from spectrogram-like representations?"
  },
  {
    "objectID": "presentation.html#section-1",
    "href": "presentation.html#section-1",
    "title": "",
    "section": "",
    "text": "🧩 Data Files and Extraction Workflow\n  \n    User Input: Providing initial CSV files containing song names, optionally including genre, language, and artist information.\n    Data Completion: Performing automated web scraping with Python scripts, filling in missing metadata fields such as genre, language, and artist.\n    YouTube Integration: Scraping YouTube with Python scripts, finding matching song URLs, and storing these links and related metadata in a JSON file.\n    Audio Acquisition: Downloading .wav audio files from YouTube URLs for each song with Python scripts.\n    Feature Extraction: Using Python (numpy and librosa libraries) scripts for analyzing audio files and extracting features (e.g., fundamental frequency, MFCCs, tempo).\n    Output Files: Saving extracted audio features as CSV files; saving visualizations such as spectrograms as .png images. From here, constructing machine learning pipelines with Python libraries."
  },
  {
    "objectID": "presentation.html#section-2",
    "href": "presentation.html#section-2",
    "title": "",
    "section": "",
    "text": "🔍 Feature Scraping\n\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nfundamental_freq\nFundamental frequency (mean pitch via librosa.pyin)\n\n\nfreq_e_1\nDominant spectral energy #1 (highest energy frequency bin)\n\n\nfreq_e_2\nDominant spectral energy #2 (2nd highest energy frequency bin)\n\n\nfreq_e_3\nDominant spectral energy #3 (3rd highest energy frequency bin)\n\n\nkey\nEstimated musical key (C, C#, D, …, B) via chroma features\n\n\nduration\nLength of audio in seconds\n\n\nzero_crossing_rate\nAverage zero crossing rate (signal sign changes)\n\n\nmfcc_mean\nMean of 13 MFCC coefficients (timbre features)\n\n\nmfcc_std\nStandard deviation of MFCC coefficients\n\n\ntempo\nEstimated tempo in beats per minute (BPM)\n\n\nrms_energy\nRoot mean square energy (loudness measure)\n\n\ntrack_type\nAudio track type (0=full mix, 1=vocal only, 2=no vocals)\n\n\nmel_spectrogram\nMel-scaled spectrogram representing frequency content over time (human hearing range)"
  },
  {
    "objectID": "presentation.html#section-3",
    "href": "presentation.html#section-3",
    "title": "",
    "section": "",
    "text": "F1 (macro) comparison across ablations and models"
  },
  {
    "objectID": "presentation.html#section-4",
    "href": "presentation.html#section-4",
    "title": "",
    "section": "",
    "text": "Q2 (A/B) Overview - Nathan describe your question and how you implemented mL [libraries, validation]"
  },
  {
    "objectID": "presentation.html#section-5",
    "href": "presentation.html#section-5",
    "title": "",
    "section": "",
    "text": "Q2 (A) Results - Nathan graphs? Learning Curve, ROC, any hyperparameter optimization?"
  },
  {
    "objectID": "presentation.html#section-6",
    "href": "presentation.html#section-6",
    "title": "",
    "section": "",
    "text": "Q2 (A) Results - Nathan Results (% scores) Validation methods."
  },
  {
    "objectID": "presentation.html#section-7",
    "href": "presentation.html#section-7",
    "title": "",
    "section": "",
    "text": "Q2 (B) Results - Nathan. Summarize - what you did/results."
  },
  {
    "objectID": "presentation.html#section-8",
    "href": "presentation.html#section-8",
    "title": "",
    "section": "",
    "text": "Q2 (B) Results [3/3] - Nathan. Summarize - what you did/results."
  },
  {
    "objectID": "presentation.html#section-9",
    "href": "presentation.html#section-9",
    "title": "",
    "section": "",
    "text": "Q2 Summary - Nathan"
  },
  {
    "objectID": "presentation.html#section-10",
    "href": "presentation.html#section-10",
    "title": "",
    "section": "",
    "text": "Full Summary - Nathan talk about both question findings."
  },
  {
    "objectID": "presentation.html#section-11",
    "href": "presentation.html#section-11",
    "title": "",
    "section": "",
    "text": "Thank you for listening\n      \n        Your attention was appreciated.\n        Questions welcome.\n      \n      Team Members:"
  },
  {
    "objectID": "presentation.html#section-12",
    "href": "presentation.html#section-12",
    "title": "",
    "section": "",
    "text": "Full Summary - Nathan talk about both question findings."
  },
  {
    "objectID": "presentation.html#section-13",
    "href": "presentation.html#section-13",
    "title": "",
    "section": "",
    "text": "In Closing - Nathan"
  },
  {
    "objectID": "presentation.html#section-14",
    "href": "presentation.html#section-14",
    "title": "",
    "section": "",
    "text": "In Closing"
  },
  {
    "objectID": "presentation.html#project-overview-research-questions",
    "href": "presentation.html#project-overview-research-questions",
    "title": "",
    "section": "🔍 Project Overview & Research Questions",
    "text": "🔍 Project Overview & Research Questions\n\n\n  🎯 Project Description & Goals\n  \n    Our team is developing a machine learning system for an AI-driven music recommendation service. The main goals are:\n  \n  \n    To build a model capable of recognizing the language(s) spoken in audio files\n    To assess whether new songs align with a user’s musical preferences\n  \n  \n    By separating vocal tracks and analyzing audio features such as tempo, loudness, genre, and language, we aim to create a highly personalized and accurate recommendation system.\n  \n\n  \n\n  ❓ Research Question 1 – Language Recognition\n  How can we leverage statistical and time-frequency features extracted from separated vocal and audio tracks to build effective language recognition models?\n  \n    What are the strengths and limitations of classical methods like SVMs or GMMs in this domain?\n    How do engineered features and model validation techniques impact robustness?\n    Does file format (.wav vs .mp3) affect accuracy due to compression artifacts?\n  \n\n  \n\n  ❓ Research Question 2 – Recommendation System\n  How can user interaction data combined with audio and metadata be used to build an effective recommendation system using collaborative filtering or traditional ML?\n  \n    Can audio features and clustering improve personalization?\n    What impact do vocal vs. full-mix inputs have?\n    Does audio quality (.wav vs .mp3) influence feature extraction and recommendation performance?"
  },
  {
    "objectID": "presentation.html#learning-curve-f1-score",
    "href": "presentation.html#learning-curve-f1-score",
    "title": "",
    "section": "Learning Curve (F1 Score)",
    "text": "Learning Curve (F1 Score)\n\n\nF1 (macro) comparison across ablations and models"
  },
  {
    "objectID": "presentation.html#question-1-summary",
    "href": "presentation.html#question-1-summary",
    "title": "",
    "section": "📌 Question 1-Summary",
    "text": "📌 Question 1-Summary\n\n  \n    What we did:\n      \n        Designed a supervised ML pipeline with feature scaling, label encoding, stratified CV.\n        Evaluated three classifiers on different track-type subsets.\n      \n    \n    Results:\n      \n        Best model = Linear SVM trained on vocal_only data.\n        Achieved ~50% performance across all metrics.\n      \n    \n    Conclusion:\n      \n        Vocal features are sufficient to classify language with strong accuracy.\n        Next: expand dataset, apply hyperparameter optimization, and test on external songs."
  },
  {
    "objectID": "presentation.html#section-39",
    "href": "presentation.html#section-39",
    "title": "",
    "section": "",
    "text": "Q2 Overview - Nathan describe your question and how you implemented mL [libraries, validation]"
  },
  {
    "objectID": "presentation.html#section-40",
    "href": "presentation.html#section-40",
    "title": "",
    "section": "",
    "text": "Q2 Results [1/3] - Nathan graphs? Learning Curve, ROC, any hyperparameter optimization?"
  },
  {
    "objectID": "presentation.html#section-41",
    "href": "presentation.html#section-41",
    "title": "",
    "section": "",
    "text": "Q2 Results [2/3] - Nathan Results (% scores) Validation methods."
  },
  {
    "objectID": "presentation.html#section-42",
    "href": "presentation.html#section-42",
    "title": "",
    "section": "",
    "text": "Q2 Results [3/3] - Nathan. Summarize - what you did/results."
  },
  {
    "objectID": "presentation.html#section-43",
    "href": "presentation.html#section-43",
    "title": "",
    "section": "",
    "text": "Q2 Summary - Nathan"
  },
  {
    "objectID": "presentation.html#section-44",
    "href": "presentation.html#section-44",
    "title": "",
    "section": "",
    "text": "Full Summary - Nathan talk about both question findings."
  },
  {
    "objectID": "presentation.html#section-45",
    "href": "presentation.html#section-45",
    "title": "",
    "section": "",
    "text": "In Closing - Nathan"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Audio Alchemy",
    "section": "Introduction",
    "text": "Introduction\nMusic genre classification is a central task in the field of music information retrieval, combining elements of signal processing, machine learning, and deep learning. Accurate genre identification not only enhances music recommendation systems and streaming platforms but also deepens our understanding of audio structure and human perception of sound. Traditional approaches have relied on handcrafted audio features analyzed with machine learning techniques such as Random Forests and Gaussian Mixture Models, offering interpretable yet limited performance[1]. Recent advances, however, leverage deep learning methods—particularly convolutional neural networks (CNNs)—to extract high-level representations directly from spectrograms, achieving state-of-the-art results[2]. This project explores both paradigms: first applying classical machine learning with 5-fold cross-validation, and then advancing to CNN-based classification on spectrogram heat maps, with results evaluated using standard metrics including accuracy, precision, recall, F1-score, confusion matrices, and ROC curves."
  },
  {
    "objectID": "index.html#questions",
    "href": "index.html#questions",
    "title": "Audio Alchemy",
    "section": "Questions",
    "text": "Questions\n\n1. Language Recognition with Separated Vocal & Audio Tracks\n\ninitial formulation\nHow can we leverage statistical and time-frequency features extracted from separated vocal and audio tracks to build effective language recognition models? Specifically, how can traditional machine learning methods — ranging from classical classifiers on simple statistical summaries to Gaussian Mixture Models on richer time-frequency features — be applied in this context?\n\nWhat are the key benefits and limitations of these approaches?\n\nHow can careful feature engineering, feature integration, and thorough model evaluation improve the accuracy and robustness of language recognition systems?\n\nHow do model results compare and contrast when using .wav files versus .mp3 files?\n\n\n\nsecondary formulation\n\nFrom the initial formulation, we refined the question to specifically compare how different ablations of the audio track (complete song, vocal-only, and non-vocal) affect model performance.\n\nHow does model performance differ when predicting song language using features from complete songs, vocal-only tracks, and instrumental-only tracks?\nWhat are the relative strengths and limitations of classical machine learning models (Logistic Regression, Random Forest, SVM) when applied to language recognition?\n\n\n\nFrom the initial problems statement - what did you change?\n\n\n\n2. Recommendation Systems Using Audio Features & User Data\n\ninitial formulation\nHow can user interaction data, combined with basic track metadata and simple audio features, be used to build an effective recommendation system using collaborative filtering and traditional machine learning methods?\n\nFurthermore, how can advanced audio features, dimensionality reduction, and clustering techniques improve personalized recommendations by better capturing user preferences and track characteristics from both vocal and non-vocal components?\n\nHow do recommendation model results compare and contrast when using .wav files versus .mp3 files, considering the potential impact of audio quality and compression artifacts on feature extraction and recommendation performance?\n\n\n\nsecondary formulation\n\nNathan #_to_Do\n\nFrom the initial problems statement - what did you change?"
  },
  {
    "objectID": "index.html#dataset",
    "href": "index.html#dataset",
    "title": "Audio Alchemy",
    "section": "Dataset",
    "text": "Dataset\n\ndata provenance\n\nNathan #_to_Do\n\n\n\nsoftware distriubtion\n\nNathan #_to_Do\n\n\n\ndata collection\n\nNathan #_to_Do\n\n\n\ndata storage\n\nNathan #_to_Do\n\n\n\ngraphs?\n\nNathan #_to_Do\n\n.. any feature graphs I can think of…"
  },
  {
    "objectID": "index.html#analysis-plan",
    "href": "index.html#analysis-plan",
    "title": "Audio Alchemy",
    "section": "Analysis Plan",
    "text": "Analysis Plan\n\nGeneral\n\n\nQ1 - Yashi\n\nYashi #_to_Do - entire section for your problem\n\n\nput any graphs in this section.\ndo you have learning curve, ROC curves? Feature importance graphs?\n\n\nrestate question\ndata collection - data set size/composition (var types) Data was collected through a series of Python scripts. [see slide 3] You had an extra layer of feature extraction - removing vocal/instrumental tracts.\ndata processing - any PCA, correlation, imputation, outlier removal?\nmodel selection - what models, why? (if any reason), what Python libraries did you use?\nmodel validation - what metrics? why?\nmodel evaluation - what metrics? why? Note: the ‘no vocal’ track - behaved at about 50% accuracy - which is what you’d expect for a control group.\nfuture steps/recommendations\n\n\n\nQ2\n\nNathan #_to_Do\n\n\nput any graphs in this section.\ndo you have learning curve, ROC curves? Feature importance graphs?\n\n\nrestate question\ndata collection - data set size/composition (var types) Data was collected through a series of Python scripts. [see slide 3] You had an extra layer of feature extraction - removing vocal/instrumental tracts.\ndata processing - any PCA, correlation, imputation, outlier removal?\nmodel selection - what models, why? (if any reason), what Python libraries did you use?\nmodel validation - what metrics? why?\nmodel evaluation - what metrics? why?\nfuture steps/recommendations"
  },
  {
    "objectID": "index.html#results-conclusion",
    "href": "index.html#results-conclusion",
    "title": "Audio Alchemy",
    "section": "Results & Conclusion",
    "text": "Results & Conclusion\n\nYashi\n\nThe project goal was to investigate how audio feature engineering can support language classification in music. Our analysis demonstrated that vocal-only features drive most of the predictive signal, confirming that separated vocals provide the strongest basis for language recognition. These results demonstrate the potential of classical machine learning for this task, but also highlight its limits，larger datasets and deep learning methods will be needed for stronger multilingual classification in the future.\n\nRestate the project goal, and the goal of your question. What was done in the analysis, and what was found with the features extracted. (1 paragraph)\n\nNathan #_to_Do\n\nRestate the project goal, and the goal of your question. What was done in the analysis, and what was found with the features extracted. (1 paragraph)"
  },
  {
    "objectID": "index.html#video-links",
    "href": "index.html#video-links",
    "title": "Audio Alchemy",
    "section": "Video links",
    "text": "Video links\n\nNathan #_to_Do"
  },
  {
    "objectID": "index.html#audio-player-demo",
    "href": "index.html#audio-player-demo",
    "title": "Audio Alchemy",
    "section": "Audio Player Demo",
    "text": "Audio Player Demo\n\nNathan #_to_Do"
  },
  {
    "objectID": "index.html#team-member-workload",
    "href": "index.html#team-member-workload",
    "title": "Audio Alchemy",
    "section": "Team member workload",
    "text": "Team member workload\n\nOur project workload followed a structured week-by-week workflow, with responsibilities distributed among team members. We began by finalizing and sharing the proposal, followed by the individual collection and organization of ~200 audio files per person. Nathan Herling led the processing and validation of metadata, while each member focused on building machine learning pipelines and conducting iterative testing. The project concluded with a collaborative effort on final model evaluation, report preparation, and presentation development.\n\nJust type 3-4 sentences about workload go off the proposal ‘week work map/individual duties section’ - but, put into into paragraph form."
  },
  {
    "objectID": "index.html#sources",
    "href": "index.html#sources",
    "title": "Audio Alchemy",
    "section": "sources",
    "text": "sources\n\nNathan #_to_Do\n\n[1] https://link.springer.com/chapter/10.1007/978-981-97-4533-3_6\n[2] https://arxiv.org/html/2411.14474v1"
  },
  {
    "objectID": "index.html#a-note-on-spectographic-features-of-.mp3-vs.-.wav",
    "href": "index.html#a-note-on-spectographic-features-of-.mp3-vs.-.wav",
    "title": "Audio Alchemy",
    "section": "A note on spectographic features of .mp3 vs. .wav",
    "text": "A note on spectographic features of .mp3 vs. .wav\n\nNathan #_to_Do\n\nHere i’ll add a graph, and discuss who despite the large disparity in file size the two file types are nearly similar in audio data. Making it not a good research path to compare .mp3 and .wav files."
  },
  {
    "objectID": "index.html#problem-analysis-and-results",
    "href": "index.html#problem-analysis-and-results",
    "title": "Audio Alchemy",
    "section": "Problem analysis and results",
    "text": "Problem analysis and results\n\nGeneral\n\nNathan #_to_Do\n\nDiscuss what the original plan was, and what was done - in terms of easy, med problem design.\n\n\nQ1 - Yashi\n\nHow can we leverage audio features from separated vocal and instrumental tracks to improve language recognition in music?\nData Collection: The dataset consisted of ~200 audio files, preprocessed into three ablations: complete songs, vocal-only tracks, and instrumental-only tracks. Features included time-domain statistics (mean, variance, skewness, kurtosis).\nData Processing: All features were standardized using global scaling. Encoded target variable (language) with LabelEncoder.No major imputation was required as missingness was minimal.\nModel Selection: I evaluated three models: Logistic Regression, Random Forest, and Support Vector Machines with linear kernels. These models were chosen for their balance of interpretability, robustness, and suitability for structured feature data. Training and evaluation were conducted using 5-fold stratified cross-validation to ensure reliable performance comparisons across models.\nValidation & Metrics: Evaluation focused on accuracy, precision, recall, and F1-score. Confusion matrices were used to analyze per-class misclassification patterns.\nModel Evaluation:\n\n\n\n\n\n\n\n\n\n\n\nAblation\nModel\nAccuracy\nPrecision\nRecall\nF1\n\n\n\n\ncomplete_song\nLogReg\n0.399\n0.398\n0.447\n0.387\n\n\ncomplete_song\nRandomForest\n0.626\n0.469\n0.410\n0.401\n\n\ncomplete_song\nSVM_linear\n0.432\n0.443\n0.504\n0.427\n\n\nvocal_only\nLogReg\n0.560\n0.531\n0.567\n0.509\n\n\nvocal_only\nRandomForest\n0.552\n0.426\n0.404\n0.385\n\n\nvocal_only\nSVM_linear\n0.544\n0.542\n0.583\n0.514\n\n\nno_vocal\nLogReg\n0.333\n0.371\n0.364\n0.316\n\n\nno_vocal\nRandomForest\n0.577\n0.436\n0.349\n0.328\n\n\nno_vocal\nSVM_linear\n0.366\n0.418\n0.411\n0.347\n\n\nColumn Min\n-\n0.333\n0.371\n0.349\n0.316\n\n\nColumn Max\n-\n0.626\n0.542\n0.583\n0.514\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResults:\n\nVocal-only tracks: Provided the best classification signal, with SVM achieving ~0.51 macro F1, outperforming Random Forest and Logistic Regression.\nComplete songs: Models achieved moderate performance (~0.40 F1), reflecting a mixture of useful vocal cues diluted by instrumental content.\nNon_vocal tracks: Accuracy dropped to ~0.50 (random baseline), validating the expectation that language recognition requires vocal content.\n\n\n\n\nrestate question\ndata collection - data set size/composition (var types) Data was collected through a series of Python scripts. [see slide 3] You had an extra layer of feature extraction - removing vocal/instrumental tracts.\ndata processing - any PCA, correlation, imputation, outlier removal?\nmodel selection - what models, why? (if any reason), what Python libraries did you use?\nmodel validation - what metrics? why?\nmodel evaluation - what metrics? why? Note: the ‘no vocal’ track - behaved at about 50% accuracy - which is what you’d expect for a control group.\nfuture steps/recommendations\n\n\n\nQ2\n\nNathan #_to_Do\n\n\nput any graphs in this section.\ndo you have learning curve, ROC curves? Feature importance graphs?\n\n\nrestate question\ndata collection - data set size/composition (var types) Data was collected through a series of Python scripts. [see slide 3] You had an extra layer of feature extraction - removing vocal/instrumental tracts.\ndata processing - any PCA, correlation, imputation, outlier removal?\nmodel selection - what models, why? (if any reason), what Python libraries did you use?\nmodel validation - what metrics? why?\nmodel evaluation - what metrics? why?\nfuture steps/recommendations"
  }
]