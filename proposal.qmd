---
title: "Audio Alchemy - with The AudioPhiles"
subtitle: ""
author: 
  - name: "The Audiophiles -  Nathan Herling & Yashi Mi"
    affiliations:
      - name: "College of Information Science, University of Arizona"
description: ""
format:
  html:
    code-tools: true
    code-overflow: wrap
    code-line-numbers: true
    embed-resources: true
editor: visual
code-annotations: hover
execute:
  warning: false
jupyter: python3
---

## Proposal

Our team is developing a machine learning system as part of a larger AI-driven music recommendation service. The primary objective is to build a model capable of recognizing the language(s) spoken in audio files and assessing whether new songs align with a user‚Äôs preferences. Understanding spoken language within music tracks, combined with genre classification, enables more personalized and accurate recommendations. This capability is essential for enhancing user experience by suggesting songs that resonate with individual tastes.

The challenge lies in effectively processing real audio data to extract meaningful features, classify genres, and interpret user listening histories. By leveraging signal processing and machine learning techniques, this project aims to automate and improve the recommendation process. Such advancements not only deepen our understanding of audio analysis but also pave the way for smarter, more intuitive music discovery platforms.

## Python libraries
```{python}
#| label: load-pkgs
#| message: false
import os
import json
import subprocess
import numpy as np
import pandas as pd

# === Machine Learning & Evaluation ===
import sklearn  # Models, preprocessing, cross-validation, metrics
import lightgbm as lgb  # Gradient boosting
import xgboost as xgb  # Gradient boosting
#import surprise  # Consider removing if problematic, see alternatives

# === Deep Learning Frameworks ===
import torch  # PyTorch (used with Demucs, CNNs, etc.)
import tensorflow as tf  # TensorFlow
from tensorflow import keras  # Keras API

# === Audio Processing ===
import librosa  # Feature extraction (ZCR, RMS, tempo, etc.)
import torchaudio  # Audio I/O and transformations with PyTorch
from demucs.apply import apply_model
from demucs.pretrained import get_model  # Vocal separation

# === Visualization ===
import matplotlib.pyplot as plt
import seaborn as sns

# === Display & Formatting (for .qmd / Jupyter) ===
from IPython.display import display, HTML

```
### Vocal track separation with - DEMUCS
Demucs (Deep Extractor for Music Sources) is an open-source tool developed by Meta AI, 
designed for high-quality music source separation. Originally introduced to outperform traditional 
spectral mask methods, it uses a deep learning architecture based on convolutional layers and 
bidirectional LSTM to separate audio into distinct stems like vocals, drums, bass, and others. 
Unlike classical techniques that rely on frequency-domain heuristics, Demucs operates directly in the time 
domain, enabling precise extraction of overlapping audio components. For multilingual analysis, 
Demucs is especially effective because it isolates vocals based purely on acoustic features ‚Äî not 
linguistic content ‚Äî making it an ideal front-end for tasks like spoken language identification or lyric 
classification in machine learning pipelines.

### Minimal scripts - Using System Terminal Commands - DEMUCS
Scripts that run `demucs` using system commands‚Äîtypically through Python‚Äôs `subprocess` or `os` libraries‚Äîoffer a straightforward way to integrate audio separation tools into Python workflows while interacting with the operating system‚Äôs file structure and command-line utilities.

```{python}
#| label: ex_script_1
#| message: false
#| eval: false
import os
import subprocess

# Path to your input audio file
audio_file = r"~\Gloria_Gaynor_I_Will_Survive.wav"

# Optional: check if file exists
if not os.path.exists(audio_file):
    raise FileNotFoundError(f"Audio file not found: {audio_file}")

# Build the Demucs command
# You can change --two-stems to 'drums' or 'bass' if needed
command = [
    "demucs",
    "--two-stems=vocals",  # Extract vocals only
    "--out", "demucs_output",  # Output folder
    audio_file
]

# Run the command
print("üîÑ Running Demucs...")
subprocess.run(command)

print("‚úÖ Separation complete. Check the 'demucs_output' folder for results.")

```


## Dataset
### Data - Provenence
All audio data used in this project was sourced from YouTube, following an automated pipeline designed to gather relevant tracks for training and evaluation:

- A list of artist names was manually curated.

- For each artist, their top five songs were identified by scraping Google search results.

- YouTube links corresponding to those songs were retrieved using a custom scraping script.

- Audio from each YouTube video was downloaded and stored for analysis.

In addition to audio collection, a secondary scraper was developed to gather metadata. This tool extracted the most commonly associated genres and countries of origin for each artist by querying publicly available sources.

### Data - Collection
The data collection process involved several custom Python scripts designed to scrape and download the necessary information and audio files:

`genre_scrape.py` ‚Äî Scrapes top genres associated with artists.

`artist_5_song_list_scrape.py` ‚Äî Retrieves the top five songs per artist from Google search results.

`artist_genre_scrape.py` ‚Äî Gathers genre metadata for each artist from public sources.

`artist_country_of_origin_scrape.py` ‚Äî Extracts the country of origin for each artist.

`audio_scrape_wav_mp3.py` ‚Äî Downloads audio files from YouTube links in WAV and MP3 formats.

Together, these scripts automate the extraction of both audio data and relevant metadata to support training and evaluation of the recommendation system.

### Data - Description
This table provides a comprehensive overview of the audio and metadata features considered during project development. It details each feature‚Äôs definition, data type, output format, and extraction notes. While some features may not be ultimately used, the table serves as a complete reference of potential inputs for modeling musical and audio characteristics. All features requiring extraction have, at least prima facie, been verified against available Python libraries capable of performing the extraction.
```{python}
#| label: feature-summary-py
#| echo: false

import pandas as pd

# Define the simplified table
df = pd.DataFrame({
    "Feature": [
        "Artist", "Song Title", "Genre", "Mean", "Variance", "Skewness", "Kurtosis",
        "Zero Crossing Rate", "RMS Energy", "Loudness", "Energy", "Tempo", "Danceability",
        "Key / Key Name", "Mode / Mode Name", "FFT", "STFT", "Mel-Spectrogram",
        "Freq vs dB Spectrum", "Duration"
    ],
    "Definition": [
        "Name of the performing artist or band.",
        "Title of the song or track.",
        "Musical style or category (may be multiple).",
        "Average value of an audio feature over time.",
        "Spread or variability of the feature.",
        "Asymmetry of the feature distribution.",
        "Heaviness of tails in the feature distribution.",
        "Rate at which waveform crosses zero amplitude.",
        "Root mean square of amplitude (loudness).",
        "Perceived loudness (in dB).",
        "Total signal energy over time.",
        "Beats per minute (BPM).",
        "Suitability of the track for dancing.",
        "Musical key (e.g., C, F#, A minor).",
        "Tonality: major or minor.",
        "Amplitude vs frequency via FFT.",
        "Short-Time Fourier Transform (time-frequency).",
        "Mel-scaled spectrogram representation.",
        "Spectrum in decibel scale.",
        "Length of audio in milliseconds."
    ],
    "Variable Type": [
        "String", "String", "String Array", "Numeric", "Numeric", "Numeric", "Numeric",
        "Numeric", "Numeric", "Numeric", "Numeric", "Numeric", "Numeric",
        "Categorical", "Categorical", "Numeric Array", "Complex Array",
        "Numeric Array", "Numeric Array", "Numeric"
    ],
    "Output Type": [
        "Single string", "Single string", "List of strings", "Single number", "Single number",
        "Single number", "Single number", "Single number", "Single number", "Single number",
        "Single number", "Single number", "Approximate", "Single string", "Single string",
        "1D Array", "2D Matrix", "2D Matrix", "1D/2D Matrix", "Single number"
    ],
    "Notes": [
        "Usually human entered or scraped from metadata.",
        "Usually human entered or scraped from metadata.",
        "Often human-assigned or from databases.",
        "Use numpy/scipy on time-series features.",
        "Use numpy.var() on time-varying data.",
        "Use scipy.stats.skew() on feature arrays.",
        "Use scipy.stats.kurtosis() for tail behavior.",
        "librosa.feature.zero_crossing_rate().",
        "librosa.feature.rms() or similar.",
        "Estimate via RMS or pydub gain.",
        "Mean RMS squared or signal power.",
        "librosa.beat.beat_track().",
        "Estimated with ML or Essentia.",
        "From chroma feature analysis.",
        "Derived from key/chroma analysis.",
        "np.fft.fft() + abs().",
        "librosa.stft().",
        "librosa.feature.melspectrogram().",
        "librosa.power_to_db() on FFT or STFT.",
        "librosa.get_duration() * 1000."
    ]
})

# Style and display the table
df.style.set_table_attributes("class='table table-striped table-hover'").set_caption("Summary of Audio Features (Simplified)")
```
### Machine Learning Nomenclature
This table defines key terms related to the concept of ‚Äútokens‚Äù in the context of music machine learning. It clarifies how raw audio features, sequences, and categorical metadata can be represented as tokens for model input. Understanding these distinctions is essential for designing effective audio classification and recommendation systems.
```{python}
#| label: mL-Vocab-py
#| echo: false
# Define the data
data = {
    "Term": ["Token", "Sequence", "Vocabulary"],
    "Example token in your project": [
        "Single time-window feature vector or metadata category",
        "Ordered series of audio feature vectors (like frames in time)",
        "Set of all possible genres, artists, keys, or discretized audio features"
    ]
}

# Create DataFrame
df = pd.DataFrame(data)

# Style and display the table with Bootstrap classes and caption
styled_table = df.style.set_table_attributes("class='table table-striped table-hover'") \
                       .set_caption("Token Definitions in Music ML Context")

# To render in Jupyter or Quarto, just put `styled_table` on the last line
styled_table
```
### Data - Storage
The example JSON structure below represents a data entry for an artist and their song metadata, including audio file links, user feedback options, and categorical tags. This format organizes information for easy ingestion by machine learning pipelines or user interfaces, capturing both objective metadata (artist, categories, audio features) and subjective user ratings using Likert scales of varying lengths.

Audio files and any complex metadata such as multi-dimensional arrays (e.g., spectrograms) will be stored in separate folders, with their file paths referenced in the JSON under the AudioFile node.

`AudioFile:` Nested object containing lists of links to YouTube, WAV, and MP3 versions of the audio files.

`Region, User:` Lists to hold geographic metadata and user identifiers. The User field, along with the Likert rating arrays, are pre-built placeholders designed to support future features such as user accounts and the collection of personalized likeability or preference ratings.

`Data:` A comprehensive sub-node capturing detailed metadata about the audio, including:

- Artist and song title

- Genre(s)

- Quantitative audio features such as mean, variance, skewness, kurtosis, zero crossing rate, RMS   energy, loudness, energy, tempo, danceability

- Musical attributes like key and mode

- Complex audio representations like FFT, STFT, mel-spectrogram, frequency vs dB spectrum

- Duration in milliseconds

`Likert_2`, `Likert_3`, `Likert_5`: Arrays representing different Likert scales, each with entries for score, label, color, description, and selection status to capture nuanced user feedback.

`category`: An array of genre tags associated with the song.

This JSON schema is designed to be flexible and extensible, accommodating rich metadata and user feedback for building and improving recommendation systems.
```{python}
#| label: json-ex
#| echo: false
# Your JSON dict & display code here
# Your JSON data as a Python dictionary
node = {
    "AudioFile": {
        "yt_link": [],
        "wav_link": [],
        "mp3_link": []
    },
    "Region": ["America"],
    "User": ["Nathan"],
    "Data": {
        "Artist": "Lady Gaga",
        "Song Title": "",
        "Genre": [],
        "Mean (of features)": None,
        "Variance": None,
        "Skewness": None,
        "Kurtosis": None,
        "Zero Crossing Rate": None,
        "RMS Energy": None,
        "Loudness": None,
        "Energy": None,
        "Tempo": None,
        "Danceability": None,
        "Key / Key Name": "",
        "Mode / Mode Name": "",
        "FFT (Amplitude vs Frequency)": None,
        "STFT (Short-Time Fourier Transform)": None,
        "Mel-Spectrogram": None,
        "Frequency vs dB Spectrum": None,
        "Duration (ms)": None
    },
    "Likert_2": [
        {"score": 1, "label": "No", "color": "#FF4C4C", "selected": True},
        {"score": 2, "label": "Yes", "color": "#4CAF50", "selected": False}
    ],
    "Likert_3": [
        {"score": 1, "label": "Dislike", "description": "I do not like this", "color": "#FF4C4C", "selected": False},
        {"score": 2, "label": "Meh", "description": "Neutral or indifferent", "color": "#FFD700", "selected": True},
        {"score": 3, "label": "Like", "description": "I like this", "color": "#4CAF50", "selected": False}
    ],
    "Likert_5": [
        {"score": 1, "label": "Strongly Dislike", "description": "I strongly dislike this genre/song", "color": "#FF4C4C", "selected": False},
        {"score": 2, "label": "Dislike", "description": "I don‚Äôt enjoy this genre/song", "color": "#FF8C00", "selected": False},
        {"score": 3, "label": "Neutral", "description": "Neither like nor dislike", "color": "#FFD700", "selected": True},
        {"score": 4, "label": "Like", "description": "I like this genre/song", "color": "#90EE90", "selected": False},
        {"score": 5, "label": "Strongly Like", "description": "I strongly like or love this genre/song", "color": "#008000", "selected": False}
    ],
    "category": ["rock", "pop", "electronic pop", "jazz pop"]
}


json_str = json.dumps(node, indent=4)
html_code = f"""
<div style="max-height: 400px; overflow: auto; border: 1px solid #ccc; padding: 10px; background: #f9f9f9; white-space: pre-wrap; font-family: monospace;font-size: 11px;">
{json_str}
</div>
"""
display(HTML(html_code))
```


### Data - Second Tier features
The second tier of data features will focus on vocal track extraction, utilizing the DEMUCS library. Currently, the plan is to develop a dedicated script that processes each song‚Äôs audio files‚Äîboth `.mp3` and `.wav` formats‚Äîand stores the resulting vocal isolation data in individual, well-organized folders. As additional processing requirements emerge, the structure of the master JSON file may need to be adapted or reorganized to accommodate these new data components seamlessly. This approach ensures flexibility and scalability in handling complex audio feature sets while maintaining clear data management practices.


### The Audiophiles custom audiofile ui
A custom audio player UI that supports `.wav` and `.mp3` files and dynamically renders decibel versus frequency spectrograms for each song. The player features real-time analysis with frequency, time, and dB spectrogram visualizations synchronized to the playback - a scrolling frequency,time,dB heat map. Additionally, there may be a need to build a JSON reader to facilitate the processing and aggregation of Likert scale scores associated with the songs.

![](_extra/_images/audio_player.png){fig-alt="Custom audio player UI showing spectrogram visualizations" fig-align="center"}

*Figure 1: 'Mk_1' - Custom audio player UI displaying real-time frequency, time, and dB spectrograms synchronized to playback.*


### Additional Script info, Data Storage, Data Extraction:
All additional processing tasks are handled via custom Python scripts. These scripts include tools for downloading `.mp3` and `.mp4` files, and will be maintained individually by each team member. Due to the volume of data involved‚Äîapproximately `200 .wav` files and `200 .mp3` files‚Äîthese audio assets will be stored locally on each user‚Äôs machine and not uploaded to GitHub.

Once metadata has been successfully extracted and organized, it can be safely stored and versioned within the GitHub repository. Similarly, any machine learning models developed throughout the project will be saved in the GitHub repo for reproducibility and collaboration.

Scripts for metadata extraction will be developed by Nathan and distributed to the rest of the team. Additional scripts and processing pipelines required for addressing specific research tasks will be created by individual team members as needed.


## Individual Duties

**Nathan ‚Äì Problem #2: User Song Recommendation**

- **Metadata JSON Schema Design**  
  Designed and implemented the nested `.json` structure to store song metadata, audio links, user ratings, and extracted audio features.

- **Metadata Extraction Scripts**  
  Responsible for writing and maintaining Python scripts to:
  - Scrape artist genres, countries of origin, and song lists  
  - Retrieve YouTube links  
  - Organize metadata for ingestion and storage  

- **Audio Visualization Interface**  
  Developed a custom `.wav/.mp3` player with:
  - Real-time frequency vs. dB spectrograms  
  - Static visualizations for spectral energy distribution  
  - Interactive time-frequency-dB spectrograms during playback  

- **Demucs Integration and Vocal Separation**  
  Writing scripts to:
  - Apply the Demucs library for isolating vocal tracks  
  - Store separated `.wav` and `.mp3` files in structured folders for analysis  

- **User Interface & Data Display**  
  Building HTML/Quarto-styled displays for rendering `.json` metadata and user feedback in a human-readable format.  

---

**Yashi ‚Äì Problem #1: Language Recognition from Audio**

- **Machine Learning Models**  
  Leading development of:
  - Language classification models using vocal segments  
  - Pipelines for training, validation, and testing  

- **Audio Feature Extraction**  
  Responsible for extracting statistical features such as:
  - Zero Crossing Rate (ZCR), Root Mean Square (RMS), tempo, and FFT  
  - Transforming raw waveform data into usable feature vectors  

- **Recommendation System Components**  
  Assisting in the design of:
  - Feature-based comparison systems for future personalized recommendations  
  - Methods for encoding user preferences and behavior patterns  

- **Model Evaluation and Testing**  
  Conducting:
  - Accuracy and performance evaluations of models  
  - Error analysis using tools such as confusion matrices and cross-validation  

---

**Joint Responsibilities**  
Both team members will independently construct their machine learning pipelines. Each pipeline will be trained, tested, evaluated, and iteratively improved based on problem-specific goals. Coordination will ensure that feature extraction and data preprocessing remain compatible across both tasks.

### üóÇÔ∏è Workflow Plan (Final 2.25 Weeks)

| **Phase** | **Dates**         | **Tasks** |
|-----------|------------------|-----------|
| **Phase 1: Script Finalization & Distribution** | Aug 1 ‚Äì Aug 3 | - Finalize all data scraping & audio processing scripts<br>- Distribute scripts to team members<br>- Confirm runtime & environment setup |
| **Phase 2: Data Collection & Organization** | Aug 4 ‚Äì Aug 6 | - Each user runs scripts locally<br>- Collect ~200 `.mp3` and `.wav` files per user<br>- Store audio and metadata in standardized folder structure |
| **Phase 3: Metadata Processing** | Aug 7 ‚Äì Aug 8 | - Parse, clean, and validate `.json` metadata<br>- Integrate new entries into master metadata files<br>- Ensure feature coverage (e.g., genre, tempo, duration) |
| **Phase 4: ML Pipeline Construction** | Aug 9 ‚Äì Aug 10 | - Each user builds their custom ML pipeline<br>- Define preprocessing, feature extraction, model architecture |
| **Phase 5: ML Testing & Iteration ‚Äì Round 1** | Aug 11 ‚Äì Aug 13 | - Run training and validation pipelines<br>- Tune hyperparameters<br>- Log and assess intermediate results |
| **Phase 6: ML Testing & Iteration ‚Äì Round 2** | Aug 14 ‚Äì Aug 16 | - Refine pipelines based on feedback<br>- Add second-tier features (e.g., vocal-only inputs)<br>- Evaluate early model performance |
| **Phase 7: Final Evaluation & Model Selection** | Aug 17 ‚Äì Aug 18 | - Select best models per user task<br>- Create evaluation reports<br>- Generate confusion matrices, ROC curves, etc. |
| **Phase 8: Write-Up & Presentation Prep** | Aug 19 ‚Äì Aug 21 | - Complete final Quarto write-up<br>- Polish visualizations and tables<br>- Build and rehearse project presentation |


## Questions
### 1. Language Recognition with Separated Vocal & Audio Tracks
How can we leverage **statistical** and **time-frequency features** extracted from separated vocal and audio tracks to build effective language recognition models? Specifically, how can traditional machine learning methods ‚Äî ranging from **classical classifiers** on simple statistical summaries to **Gaussian Mixture Models** on richer time-frequency features ‚Äî be applied in this context?

- What are the key **benefits** and **limitations** of these approaches?  
- How can **careful feature engineering**, **feature integration**, and **thorough model evaluation** improve the accuracy and robustness of language recognition systems?  
- How do model results compare and contrast when using **.wav** files versus **.mp3** files?

---

### 2. Recommendation Systems Using Audio Features & User Data
How can **user interaction data**, combined with basic track metadata and simple audio features, be used to build an effective recommendation system using **collaborative filtering** and traditional machine learning methods?

- Furthermore, how can **advanced audio features**, **dimensionality reduction**, and **clustering techniques** improve personalized recommendations by better capturing user preferences and track characteristics from both vocal and non-vocal components?  
- How do recommendation model results compare and contrast when using **.wav** files versus **.mp3** files, considering the potential impact of audio quality and compression artifacts on feature extraction and recommendation performance?

## Analysis plan

### Preamble: Easy and Medium Paths
The Easy Path serves as a minimal, foundational implementation aimed at quickly establishing a baseline for language recognition using straightforward statistical features extracted from separated vocal and audio tracks. It relies on classical machine learning models such as Logistic Regression, Random Forest, and Support Vector Machines, which are simple to train and interpret.

The Medium Path provides a more detailed approach that extends beyond simple statistics by incorporating time-frequency features such as Mel-Frequency Cepstral Coefficients (MFCCs), spectral centroid, and bandwidth from both vocal and non-vocal tracks. Instead of deep learning, this path uses classical probabilistic models like Gaussian Mixture Models (GMMs), Hidden Markov Models (HMMs), or advanced classical classifiers trained on aggregated time-frequency features. This allows capturing richer audio characteristics while maintaining interpretability and computational efficiency.


### Analysis Plan for Problem 1: Language Recognition  

### 1. Data Preparation & Feature Extraction - problem 1  
- Load separated vocal and instrumental audio tracks for each sample in both **.wav** and **.mp3** formats.  
- **Easy Path:** Extract statistical features (mean, variance, skewness, kurtosis, RMS energy, zero crossing rate, tempo, loudness) separately from vocal and audio tracks.  
- **Medium Path:** Extract time-frequency features such as MFCCs, spectral centroid, bandwidth, or STFT for vocal and audio tracks. Aggregate these features by computing summary statistics (mean, variance).  
- Normalize numerical features (StandardScaler or Min-Max scaling).  
- Encode categorical metadata if available (e.g., one-hot encoding for language labels or artist).  

### 2. Model Construction & Training - problem 1  
- **Easy Path:** Train classical classifiers ‚Äî Logistic Regression, Random Forest, Support Vector Machines ‚Äî on statistical feature vectors.  
- **Medium Path:**  
  - Apply **K-Means clustering** on aggregated time-frequency features to group similar audio patterns unsupervised, enhancing feature representation.  
  - Train classical models suited for time-frequency data ‚Äî Gaussian Mixture Models (GMMs), Hidden Markov Models (HMMs) (optional), or classical classifiers on combined original and cluster-based features.  
- Perform hyperparameter tuning via grid or random search where applicable.  

### 3. Validation - problem 1  
- Use stratified K-fold cross-validation to ensure balanced representation of languages in training and test splits.  
- Evaluate model performance on validation folds.  

### 4. Performance Evaluation - problem 1  
- Metrics: Accuracy, Precision, Recall, F1-score.  
- Generate confusion matrices to analyze language-specific errors.  
- Conduct ablation studies comparing vocal-only, audio-only, and combined vocal + audio features.  
- Compare model performance and feature extraction quality between **.wav** and **.mp3** audio formats to assess the impact of audio compression and quality differences on recognition accuracy.  


### Putative Machine Learning Techniques - problem 1

- Logistic Regression, Random Forest, Support Vector Machines (SVM)  
- Gaussian Mixture Models (GMM), Hidden Markov Models (HMM) (optional)  
- K-Means Clustering (unsupervised feature grouping)  
- Feature normalization/scaling (StandardScaler, Min-Max)  
- Encoding categorical features (One-hot, Label Encoding)  
- Stratified K-Fold cross-validation  
- Hyperparameter tuning (Grid Search, Random Search)  
- Evaluation metrics: Accuracy, Precision, Recall, F1-score, Confusion Matrix  
- Ablation analysis for feature contribution  

---

### Analysis Plan for Problem 2: User Song Recommendation  

### 1. Data Preparation & Feature Extraction - problem 2  
- Load user interaction data combined with track metadata (artist, genre, audio features) in both **.wav** and **.mp3** formats.  
- Encode categorical metadata (one-hot, label encoding, or embeddings).  
- Normalize numerical features (Min-Max scaling, StandardScaler).  
- Construct user-item interaction matrix from implicit feedback or ratings.  

### 2. Model Construction - problem 2  
- **Easy Path:** Collaborative filtering using K-Nearest Neighbors or Logistic Regression leveraging metadata and user preferences.  
- **Medium Path:**  
  - Use **K-Means clustering** or Hierarchical clustering on track features to identify similar groups of songs or users unsupervised.  
  - Build tree-based classifiers (Random Forest, Gradient Boosting Machines) on clustered feature groups for content-based filtering.  

### 3. Training & Validation - problem 2  
- Train-Test splits or Stratified K-Fold cross-validation.  
- Hyperparameter tuning via grid or random search.  

### 4. Performance Evaluation - problem 2  
- Metrics: Precision@K, Recall@K, Accuracy, F1-score.  
- Offline validation on held-out test sets.  
- Analyze recommendation relevance and diversity.  
- Perform ablation studies comparing models built with vocal-only features vs combined vocal + audio features.  
- Compare recommendation system performance using features extracted from **.wav** versus **.mp3** files to understand the effect of audio quality and compression artifacts.  


### Putative Machine Learning Techniques - problem 2  

- Collaborative Filtering: K-Nearest Neighbors, Logistic Regression for implicit feedback  
- Content-Based Filtering: K-Means, Hierarchical Clustering (unsupervised grouping), Random Forest, Gradient Boosting Machines  
- Cross-validation: Train-Test Split, Stratified K-Fold CV  
- Feature Engineering: One-hot encoding, embeddings, normalization (Min-Max, StandardScaler)  
- Evaluation Metrics: Precision@K, Recall@K, Accuracy, F1-score  
- Ablation Analysis: Assess impact of vocal vs non-vocal feature inclusion  



## Repo Oraganization
[Yashi - #ToDo]

::: {.callout-note}
**Note:** Current Proposal 1.0.0. Subject to change.  
(version.feature.patch notation)
:::

