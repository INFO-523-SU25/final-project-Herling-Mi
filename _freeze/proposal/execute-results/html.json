{
  "hash": "2fb7cc8f42fcc2b2336b9b22d212064b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Audio Alchemy - with The AudioPhiles\"\nsubtitle: \"\"\nauthor: \n  - name: \"The Audiophiles -  Nathan Herling & Yashi Mi\"\n    affiliations:\n      - name: \"College of Information Science, University of Arizona\"\ndescription: \"\"\nformat:\n  html:\n    code-tools: true\n    code-overflow: wrap\n    code-line-numbers: true\n    embed-resources: true\neditor: visual\ncode-annotations: hover\nexecute:\n  warning: false\njupyter: python3\n---\n\n\n\n\n## Proposal\n\nOur team is developing a machine learning system as part of a larger AI-driven music recommendation service. The primary objectives are to build a model capable of recognizing the language(s) spoken in audio files and assessing whether new songs align with a user’s preferences. Understanding spoken language within music tracks, combined with genre classification, enables more personalized and accurate recommendations. This capability is essential for enhancing user experience by suggesting songs that resonate with individual tastes.\n\nThe challenge lies in effectively processing real audio data to extract meaningful features, classify genres, and interpret user listening histories. By leveraging signal processing and machine learning techniques, this project aims to automate and improve the recommendation process. Such advancements not only deepen our understanding of audio analysis but also pave the way for smarter, more intuitive music discovery platforms.\n\n## Python libraries\n\n::: {#load-pkgs .cell message='false' execution_count=1}\n``` {.python .cell-code}\nimport os\nimport json\nimport subprocess\nimport numpy as np\nimport pandas as pd\n\n# === Machine Learning & Evaluation ===\nimport sklearn  # Models, preprocessing, cross-validation, metrics\nimport lightgbm as lgb  # Gradient boosting\nimport xgboost as xgb  # Gradient boosting\n#import surprise  # Consider removing if problematic, see alternatives\n\n# === Deep Learning Frameworks ===\nimport torch  # PyTorch (used with Demucs, CNNs, etc.)\nimport tensorflow as tf  # TensorFlow\nfrom tensorflow import keras  # Keras API\n\n# === Audio Processing ===\nimport librosa  # Feature extraction (ZCR, RMS, tempo, etc.)\nimport torchaudio  # Audio I/O and transformations with PyTorch\nfrom demucs.apply import apply_model\nfrom demucs.pretrained import get_model  # Vocal separation\n\n# === Visualization ===\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# === Display & Formatting (for .qmd / Jupyter) ===\nfrom IPython.display import display, HTML\n```\n:::\n\n\n### Vocal track separation with - DEMUCS\n\nDemucs (Deep Extractor for Music Sources) is an open-source tool developed by Meta AI, designed for high-quality music source separation. Originally introduced to outperform traditional spectral mask methods, it uses a deep learning architecture based on convolutional layers and bidirectional LSTM (Long Short-Term Memory) to separate audio into distinct stems like vocals, drums, bass, and others. Unlike classical techniques that rely on frequency-domain heuristics, Demucs operates directly in the time domain, enabling precise extraction of overlapping audio components. For multilingual analysis, Demucs is especially effective because it isolates vocals based purely on acoustic features — not linguistic content — making it an ideal front-end for tasks like spoken language identification or lyric classification in machine learning pipelines.\n\n### Minimal scripts - Using System Terminal Commands - DEMUCS\n\nScripts that run `demucs` using system commands—typically through Python’s `subprocess` or `os` libraries—offer a straightforward way to integrate audio separation tools into Python workflows while interacting with the operating system’s file structure and command-line utilities.\n\n::: {#ex_script_1 .cell message='false' execution_count=2}\n``` {.python .cell-code}\nimport os\nimport subprocess\n\n# Path to your input audio file\naudio_file = r\"~\\Gloria_Gaynor_I_Will_Survive.wav\"\n\n# Optional: check if file exists\nif not os.path.exists(audio_file):\n    raise FileNotFoundError(f\"Audio file not found: {audio_file}\")\n\n# Build the Demucs command\n# You can change --two-stems to 'drums' or 'bass' if needed\ncommand = [\n    \"demucs\",\n    \"--two-stems=vocals\",  # Extract vocals only\n    \"--out\", \"demucs_output\",  # Output folder\n    audio_file\n]\n\n# Run the command\nprint(\"🔄 Running Demucs...\")\nsubprocess.run(command)\n\nprint(\"✅ Separation complete. Check the 'demucs_output' folder for results.\")\n```\n:::\n\n\n## Dataset\n\n### Data - Provenence\n\nAll audio data used in this project was sourced from YouTube, following an automated pipeline designed to gather relevant tracks for training and evaluation:\n\n-   A list of artist names was manually curated.\n\n-   For each artist, their top five songs were identified by scraping Google search results.\n\n-   YouTube links corresponding to those songs were retrieved using a custom scraping script.\n\n-   Audio from each YouTube video was downloaded and stored for analysis.\n\nIn addition to audio collection, a secondary scraper was developed to gather metadata. This tool extracted the most commonly associated genres and countries of origin for each artist by querying publicly available sources.\n\n### Data - Collection\n\nThe data collection process involved several custom Python scripts designed to scrape and download the necessary information and audio files:\n\n`artist_5_song_list_scrape.py` — Retrieves the top five songs per artist from Google search results.\n\n`artist_genre_scrape.py` — Gathers genre metadata for each artist from public sources.\n\n`artist_country_of_origin_scrape.py` — Extracts the country of origin for each artist.\n\n`audio_scrape_wav_mp3.py` — Downloads audio files from YouTube links in WAV and MP3 formats.\n\nTogether, these scripts automate the extraction of both audio data and relevant metadata to support training and evaluation of the recommendation system.\n\n### Data - Description\n\nThis table provides a comprehensive overview of the audio and metadata features considered during project development. It details each feature’s definition, data type, output format, and extraction notes. While some features may not be ultimately used, the table serves as a complete reference of potential inputs for modeling musical and audio characteristics. All features requiring extraction have, at least prima facie, been verified against available Python libraries capable of performing the extraction.\n\n::: {#cell-feature-summary-py .cell execution_count=3}\n\n::: {#feature-summary-py .cell-output .cell-output-display execution_count=2}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_b39d9\" class='table table-striped table-hover'>\n  <caption>Summary of Audio Features (Simplified)</caption>\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_b39d9_level0_col0\" class=\"col_heading level0 col0\" >Feature</th>\n      <th id=\"T_b39d9_level0_col1\" class=\"col_heading level0 col1\" >Definition</th>\n      <th id=\"T_b39d9_level0_col2\" class=\"col_heading level0 col2\" >Variable Type</th>\n      <th id=\"T_b39d9_level0_col3\" class=\"col_heading level0 col3\" >Output Type</th>\n      <th id=\"T_b39d9_level0_col4\" class=\"col_heading level0 col4\" >Notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_b39d9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_b39d9_row0_col0\" class=\"data row0 col0\" >Artist</td>\n      <td id=\"T_b39d9_row0_col1\" class=\"data row0 col1\" >Name of the performing artist or band.</td>\n      <td id=\"T_b39d9_row0_col2\" class=\"data row0 col2\" >String</td>\n      <td id=\"T_b39d9_row0_col3\" class=\"data row0 col3\" >Single string</td>\n      <td id=\"T_b39d9_row0_col4\" class=\"data row0 col4\" >Usually human entered or scraped from metadata.</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_b39d9_row1_col0\" class=\"data row1 col0\" >Song Title</td>\n      <td id=\"T_b39d9_row1_col1\" class=\"data row1 col1\" >Title of the song or track.</td>\n      <td id=\"T_b39d9_row1_col2\" class=\"data row1 col2\" >String</td>\n      <td id=\"T_b39d9_row1_col3\" class=\"data row1 col3\" >Single string</td>\n      <td id=\"T_b39d9_row1_col4\" class=\"data row1 col4\" >Usually human entered or scraped from metadata.</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_b39d9_row2_col0\" class=\"data row2 col0\" >Genre</td>\n      <td id=\"T_b39d9_row2_col1\" class=\"data row2 col1\" >Musical style or category (may be multiple).</td>\n      <td id=\"T_b39d9_row2_col2\" class=\"data row2 col2\" >String Array</td>\n      <td id=\"T_b39d9_row2_col3\" class=\"data row2 col3\" >List of strings</td>\n      <td id=\"T_b39d9_row2_col4\" class=\"data row2 col4\" >Often human-assigned or from databases.</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_b39d9_row3_col0\" class=\"data row3 col0\" >Mean</td>\n      <td id=\"T_b39d9_row3_col1\" class=\"data row3 col1\" >Average value of an audio feature over time.</td>\n      <td id=\"T_b39d9_row3_col2\" class=\"data row3 col2\" >Numeric</td>\n      <td id=\"T_b39d9_row3_col3\" class=\"data row3 col3\" >Single number</td>\n      <td id=\"T_b39d9_row3_col4\" class=\"data row3 col4\" >Use numpy/scipy on time-series features.</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_b39d9_row4_col0\" class=\"data row4 col0\" >Variance</td>\n      <td id=\"T_b39d9_row4_col1\" class=\"data row4 col1\" >Spread or variability of the feature.</td>\n      <td id=\"T_b39d9_row4_col2\" class=\"data row4 col2\" >Numeric</td>\n      <td id=\"T_b39d9_row4_col3\" class=\"data row4 col3\" >Single number</td>\n      <td id=\"T_b39d9_row4_col4\" class=\"data row4 col4\" >Use numpy.var() on time-varying data.</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n      <td id=\"T_b39d9_row5_col0\" class=\"data row5 col0\" >Skewness</td>\n      <td id=\"T_b39d9_row5_col1\" class=\"data row5 col1\" >Asymmetry of the feature distribution.</td>\n      <td id=\"T_b39d9_row5_col2\" class=\"data row5 col2\" >Numeric</td>\n      <td id=\"T_b39d9_row5_col3\" class=\"data row5 col3\" >Single number</td>\n      <td id=\"T_b39d9_row5_col4\" class=\"data row5 col4\" >Use scipy.stats.skew() on feature arrays.</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n      <td id=\"T_b39d9_row6_col0\" class=\"data row6 col0\" >Kurtosis</td>\n      <td id=\"T_b39d9_row6_col1\" class=\"data row6 col1\" >Heaviness of tails in the feature distribution.</td>\n      <td id=\"T_b39d9_row6_col2\" class=\"data row6 col2\" >Numeric</td>\n      <td id=\"T_b39d9_row6_col3\" class=\"data row6 col3\" >Single number</td>\n      <td id=\"T_b39d9_row6_col4\" class=\"data row6 col4\" >Use scipy.stats.kurtosis() for tail behavior.</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n      <td id=\"T_b39d9_row7_col0\" class=\"data row7 col0\" >Zero Crossing Rate</td>\n      <td id=\"T_b39d9_row7_col1\" class=\"data row7 col1\" >Rate at which waveform crosses zero amplitude.</td>\n      <td id=\"T_b39d9_row7_col2\" class=\"data row7 col2\" >Numeric</td>\n      <td id=\"T_b39d9_row7_col3\" class=\"data row7 col3\" >Single number</td>\n      <td id=\"T_b39d9_row7_col4\" class=\"data row7 col4\" >librosa.feature.zero_crossing_rate().</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n      <td id=\"T_b39d9_row8_col0\" class=\"data row8 col0\" >RMS Energy</td>\n      <td id=\"T_b39d9_row8_col1\" class=\"data row8 col1\" >Root mean square of amplitude (loudness).</td>\n      <td id=\"T_b39d9_row8_col2\" class=\"data row8 col2\" >Numeric</td>\n      <td id=\"T_b39d9_row8_col3\" class=\"data row8 col3\" >Single number</td>\n      <td id=\"T_b39d9_row8_col4\" class=\"data row8 col4\" >librosa.feature.rms() or similar.</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n      <td id=\"T_b39d9_row9_col0\" class=\"data row9 col0\" >Loudness</td>\n      <td id=\"T_b39d9_row9_col1\" class=\"data row9 col1\" >Perceived loudness (in dB).</td>\n      <td id=\"T_b39d9_row9_col2\" class=\"data row9 col2\" >Numeric</td>\n      <td id=\"T_b39d9_row9_col3\" class=\"data row9 col3\" >Single number</td>\n      <td id=\"T_b39d9_row9_col4\" class=\"data row9 col4\" >Estimate via RMS or pydub gain.</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n      <td id=\"T_b39d9_row10_col0\" class=\"data row10 col0\" >Energy</td>\n      <td id=\"T_b39d9_row10_col1\" class=\"data row10 col1\" >Total signal energy over time.</td>\n      <td id=\"T_b39d9_row10_col2\" class=\"data row10 col2\" >Numeric</td>\n      <td id=\"T_b39d9_row10_col3\" class=\"data row10 col3\" >Single number</td>\n      <td id=\"T_b39d9_row10_col4\" class=\"data row10 col4\" >Mean RMS squared or signal power.</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n      <td id=\"T_b39d9_row11_col0\" class=\"data row11 col0\" >Tempo</td>\n      <td id=\"T_b39d9_row11_col1\" class=\"data row11 col1\" >Beats per minute (BPM).</td>\n      <td id=\"T_b39d9_row11_col2\" class=\"data row11 col2\" >Numeric</td>\n      <td id=\"T_b39d9_row11_col3\" class=\"data row11 col3\" >Single number</td>\n      <td id=\"T_b39d9_row11_col4\" class=\"data row11 col4\" >librosa.beat.beat_track().</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n      <td id=\"T_b39d9_row12_col0\" class=\"data row12 col0\" >Danceability</td>\n      <td id=\"T_b39d9_row12_col1\" class=\"data row12 col1\" >Suitability of the track for dancing.</td>\n      <td id=\"T_b39d9_row12_col2\" class=\"data row12 col2\" >Numeric</td>\n      <td id=\"T_b39d9_row12_col3\" class=\"data row12 col3\" >Approximate</td>\n      <td id=\"T_b39d9_row12_col4\" class=\"data row12 col4\" >Estimated with ML or Essentia.</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n      <td id=\"T_b39d9_row13_col0\" class=\"data row13 col0\" >Key / Key Name</td>\n      <td id=\"T_b39d9_row13_col1\" class=\"data row13 col1\" >Musical key (e.g., C, F#, A minor).</td>\n      <td id=\"T_b39d9_row13_col2\" class=\"data row13 col2\" >Categorical</td>\n      <td id=\"T_b39d9_row13_col3\" class=\"data row13 col3\" >Single string</td>\n      <td id=\"T_b39d9_row13_col4\" class=\"data row13 col4\" >From chroma feature analysis.</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n      <td id=\"T_b39d9_row14_col0\" class=\"data row14 col0\" >Mode / Mode Name</td>\n      <td id=\"T_b39d9_row14_col1\" class=\"data row14 col1\" >Tonality: major or minor.</td>\n      <td id=\"T_b39d9_row14_col2\" class=\"data row14 col2\" >Categorical</td>\n      <td id=\"T_b39d9_row14_col3\" class=\"data row14 col3\" >Single string</td>\n      <td id=\"T_b39d9_row14_col4\" class=\"data row14 col4\" >Derived from key/chroma analysis.</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n      <td id=\"T_b39d9_row15_col0\" class=\"data row15 col0\" >FFT</td>\n      <td id=\"T_b39d9_row15_col1\" class=\"data row15 col1\" >Amplitude vs frequency via FFT.</td>\n      <td id=\"T_b39d9_row15_col2\" class=\"data row15 col2\" >Numeric Array</td>\n      <td id=\"T_b39d9_row15_col3\" class=\"data row15 col3\" >1D Array</td>\n      <td id=\"T_b39d9_row15_col4\" class=\"data row15 col4\" >np.fft.fft() + abs().</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n      <td id=\"T_b39d9_row16_col0\" class=\"data row16 col0\" >STFT</td>\n      <td id=\"T_b39d9_row16_col1\" class=\"data row16 col1\" >Short-Time Fourier Transform (time-frequency).</td>\n      <td id=\"T_b39d9_row16_col2\" class=\"data row16 col2\" >Complex Array</td>\n      <td id=\"T_b39d9_row16_col3\" class=\"data row16 col3\" >2D Matrix</td>\n      <td id=\"T_b39d9_row16_col4\" class=\"data row16 col4\" >librosa.stft().</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n      <td id=\"T_b39d9_row17_col0\" class=\"data row17 col0\" >Mel-Spectrogram</td>\n      <td id=\"T_b39d9_row17_col1\" class=\"data row17 col1\" >Mel-scaled spectrogram representation.</td>\n      <td id=\"T_b39d9_row17_col2\" class=\"data row17 col2\" >Numeric Array</td>\n      <td id=\"T_b39d9_row17_col3\" class=\"data row17 col3\" >2D Matrix</td>\n      <td id=\"T_b39d9_row17_col4\" class=\"data row17 col4\" >librosa.feature.melspectrogram().</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n      <td id=\"T_b39d9_row18_col0\" class=\"data row18 col0\" >Freq vs dB Spectrum</td>\n      <td id=\"T_b39d9_row18_col1\" class=\"data row18 col1\" >Spectrum in decibel scale.</td>\n      <td id=\"T_b39d9_row18_col2\" class=\"data row18 col2\" >Numeric Array</td>\n      <td id=\"T_b39d9_row18_col3\" class=\"data row18 col3\" >1D/2D Matrix</td>\n      <td id=\"T_b39d9_row18_col4\" class=\"data row18 col4\" >librosa.power_to_db() on FFT or STFT.</td>\n    </tr>\n    <tr>\n      <th id=\"T_b39d9_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n      <td id=\"T_b39d9_row19_col0\" class=\"data row19 col0\" >Duration</td>\n      <td id=\"T_b39d9_row19_col1\" class=\"data row19 col1\" >Length of audio in milliseconds.</td>\n      <td id=\"T_b39d9_row19_col2\" class=\"data row19 col2\" >Numeric</td>\n      <td id=\"T_b39d9_row19_col3\" class=\"data row19 col3\" >Single number</td>\n      <td id=\"T_b39d9_row19_col4\" class=\"data row19 col4\" >librosa.get_duration() * 1000.</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n### Machine Learning Nomenclature\n\nThis table defines key terms related to the concept of “tokens” in the context of music machine learning. It clarifies how raw audio features, sequences, and categorical metadata can be represented as tokens for model input. Understanding these distinctions is essential for designing effective audio classification and recommendation systems.\n\n::: {#cell-mL-Vocab-py .cell execution_count=4}\n\n::: {#ml-vocab-py .cell-output .cell-output-display execution_count=3}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_6fa29\" class='table table-striped table-hover'>\n  <caption>Token Definitions in Music ML Context</caption>\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_6fa29_level0_col0\" class=\"col_heading level0 col0\" >Term</th>\n      <th id=\"T_6fa29_level0_col1\" class=\"col_heading level0 col1\" >Example token in your project</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_6fa29_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_6fa29_row0_col0\" class=\"data row0 col0\" >Token</td>\n      <td id=\"T_6fa29_row0_col1\" class=\"data row0 col1\" >Single time-window feature vector or metadata category</td>\n    </tr>\n    <tr>\n      <th id=\"T_6fa29_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_6fa29_row1_col0\" class=\"data row1 col0\" >Sequence</td>\n      <td id=\"T_6fa29_row1_col1\" class=\"data row1 col1\" >Ordered series of audio feature vectors (like frames in time)</td>\n    </tr>\n    <tr>\n      <th id=\"T_6fa29_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_6fa29_row2_col0\" class=\"data row2 col0\" >Vocabulary</td>\n      <td id=\"T_6fa29_row2_col1\" class=\"data row2 col1\" >Set of all possible genres, artists, keys, or discretized audio features</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n### Data - Storage\n\nThe example JSON structure below represents a data entry for an artist and their song metadata, including audio file links, user feedback options, and categorical tags. This format organizes information for easy ingestion by machine learning pipelines or user interfaces, capturing both objective metadata (artist, categories, audio features) and subjective user ratings using Likert scales of varying lengths.\n\nAudio files and any complex metadata such as multi-dimensional arrays (e.g., spectrograms) will be stored in separate folders, with their file paths referenced in the JSON under the AudioFile node.\n\n**JSON Node Storage Categories**\n\nIn hierarchical data structures like JSON, a **node** represents a single element in the tree. Nodes can be categorized by the type of data they store and their structural role. Here is a list of the strucured schemea in our JSON:\n\n`AudioFile:` Nested object containing lists of links to YouTube, WAV, and MP3 versions of the audio files.\n\n`Region, User:` Lists to hold geographic metadata and user identifiers. The User field, along with the Likert rating arrays, are pre-built placeholders designed to support future features such as user accounts and the collection of personalized likeability or preference ratings.\n\n`Data:` A comprehensive sub-node capturing detailed metadata about the audio, including:\n\n-   Artist and song title\n\n-   Genre(s)\n\n-   Quantitative audio features such as mean, variance, skewness, kurtosis, zero crossing rate, RMS energy, loudness, energy, tempo, danceability\n\n-   Musical attributes like key and mode\n\n-   Complex audio representations like FFT, STFT, mel-spectrogram, frequency vs dB spectrum\n\n-   Duration in milliseconds\n\n`Likert_2`, `Likert_3`, `Likert_5`: Arrays representing different Likert scales, each with entries for score, label, color, description, and selection status to capture nuanced user feedback.\n\n`category`: An array of genre tags associated with the song.\n\nThis JSON schema is designed to be flexible and extensible, accommodating rich metadata and user feedback for building and improving recommendation systems. Below is a partially filled Node, with all schema present.\n\n::: {#cell-json-ex .cell execution_count=5}\n\n::: {#json-ex .cell-output .cell-output-display}\n```{=html}\n\n<div style=\"max-height: 400px; overflow: auto; border: 1px solid #ccc; padding: 10px; background: #f9f9f9; white-space: pre-wrap; font-family: monospace;font-size: 11px;\">\n{\n    \"AudioFile\": {\n        \"yt_link\": [],\n        \"wav_link\": [],\n        \"mp3_link\": []\n    },\n    \"Region\": [\n        \"America\"\n    ],\n    \"User\": [\n        \"Nathan\"\n    ],\n    \"Data\": {\n        \"Artist\": \"Lady Gaga\",\n        \"Song Title\": \"\",\n        \"Genre\": [],\n        \"Mean (of features)\": null,\n        \"Variance\": null,\n        \"Skewness\": null,\n        \"Kurtosis\": null,\n        \"Zero Crossing Rate\": null,\n        \"RMS Energy\": null,\n        \"Loudness\": null,\n        \"Energy\": null,\n        \"Tempo\": null,\n        \"Danceability\": null,\n        \"Key / Key Name\": \"\",\n        \"Mode / Mode Name\": \"\",\n        \"FFT (Amplitude vs Frequency)\": null,\n        \"STFT (Short-Time Fourier Transform)\": null,\n        \"Mel-Spectrogram\": null,\n        \"Frequency vs dB Spectrum\": null,\n        \"Duration (ms)\": null\n    },\n    \"Likert_2\": [\n        {\n            \"score\": 1,\n            \"label\": \"No\",\n            \"color\": \"#FF4C4C\",\n            \"selected\": true\n        },\n        {\n            \"score\": 2,\n            \"label\": \"Yes\",\n            \"color\": \"#4CAF50\",\n            \"selected\": false\n        }\n    ],\n    \"Likert_3\": [\n        {\n            \"score\": 1,\n            \"label\": \"Dislike\",\n            \"description\": \"I do not like this\",\n            \"color\": \"#FF4C4C\",\n            \"selected\": false\n        },\n        {\n            \"score\": 2,\n            \"label\": \"Meh\",\n            \"description\": \"Neutral or indifferent\",\n            \"color\": \"#FFD700\",\n            \"selected\": true\n        },\n        {\n            \"score\": 3,\n            \"label\": \"Like\",\n            \"description\": \"I like this\",\n            \"color\": \"#4CAF50\",\n            \"selected\": false\n        }\n    ],\n    \"Likert_5\": [\n        {\n            \"score\": 1,\n            \"label\": \"Strongly Dislike\",\n            \"description\": \"I strongly dislike this genre/song\",\n            \"color\": \"#FF4C4C\",\n            \"selected\": false\n        },\n        {\n            \"score\": 2,\n            \"label\": \"Dislike\",\n            \"description\": \"I don\\u2019t enjoy this genre/song\",\n            \"color\": \"#FF8C00\",\n            \"selected\": false\n        },\n        {\n            \"score\": 3,\n            \"label\": \"Neutral\",\n            \"description\": \"Neither like nor dislike\",\n            \"color\": \"#FFD700\",\n            \"selected\": true\n        },\n        {\n            \"score\": 4,\n            \"label\": \"Like\",\n            \"description\": \"I like this genre/song\",\n            \"color\": \"#90EE90\",\n            \"selected\": false\n        },\n        {\n            \"score\": 5,\n            \"label\": \"Strongly Like\",\n            \"description\": \"I strongly like or love this genre/song\",\n            \"color\": \"#008000\",\n            \"selected\": false\n        }\n    ],\n    \"category\": [\n        \"rock\",\n        \"pop\",\n        \"electronic pop\",\n        \"jazz pop\"\n    ]\n}\n</div>\n```\n:::\n:::\n\n\n### Likert Scale Feature - indepth explanation\n\nThe Likert score will most likely be implemented in the second phase of the project. It was included in the initial design of the `.JSON` file with the idea that incorporating it early would enable potential use in the current phase and support software design aligned with future phases or iterations of the project. The intended use of the Likert score is as a UI/UX feature for users. The initial approach will involve encoding the Likert scores and using them as input features, rather than as target variables. The machine learning component will aim to provide a gradient of score values, allowing users to express preferences (likes/dislikes) with greater precision. The goal is to develop a model capable of handling varying Likert scale gradients and their corresponding precision for more accurate recommendations..\n\nIn our design, each user will have their own Likert score database (or user-specific rating entries) linked to their unique user ID. This ensures that preference modeling and recommendation outputs are truly personalized. The idea is to allow users to “score” their like or dislike of individual songs and then see how the recommendation model adapts to those inputs. To make the interaction more engaging, users can choose between a 2-, 3-, or 5-point Likert scale.\n\n### Data - Second Tier features\n\nThe second tier of data features will focus on vocal track extraction, utilizing the DEMUCS library. Currently, the plan is to develop a dedicated script that processes each song’s audio files—both `.mp3` and `.wav` formats—and stores the resulting vocal isolation data in individual, well-organized folders. As additional processing requirements emerge, the structure of the master JSON file may need to be adapted or reorganized to accommodate these new data components seamlessly. This approach ensures flexibility and scalability in handling complex audio feature sets while maintaining clear data management practices.\n\n### The Audiophiles custom audiofile ui\n\nA custom audio player UI that supports `.wav` and `.mp3` files and dynamically renders decibel versus frequency spectrograms for each song. The player features real-time analysis with frequency, time, and dB spectrogram visualizations synchronized to the playback - i.e., a scrolling frequency,time,dB heat map. Additionally, there may be a need to build a JSON reader to facilitate the processing and aggregation of Likert scale scores associated with the songs.\n\n![](_extra/_images/audio_player.png){fig-alt=\"Custom audio player UI showing spectrogram visualizations\" fig-align=\"center\"}\n\n*Figure 1: 'Mk_1' - Custom audio player UI displaying real-time frequency, time, and dB spectrograms synchronized to playback.*\n\n### Additional Script info, Data Storage, Data Extraction:\n\nAll additional processing tasks are handled via custom Python scripts. These scripts include tools for downloading `.mp3` and `.mp4` files, and will be maintained individually by each team member. Due to the volume of data involved—approximately `200 .wav` files and `200 .mp3` files—these audio assets will be stored locally on each user’s machine and not uploaded to GitHub.\n\nOnce metadata has been successfully extracted and organized, it can be safely stored and versioned within the GitHub repository. Similarly, any machine learning models developed throughout the project will be saved in the GitHub repo for reproducibility and collaboration.\n\nScripts for metadata extraction will be developed by Nathan and distributed to the rest of the team. Additional scripts and processing pipelines required for addressing specific research tasks will be created by individual team members as needed.\n\n## Individual Duties\n\n**Nathan – Problem #2: User Song Recommendation**\n\n-   **Metadata JSON Schema Design**\\\n    Designed and implemented the nested `.json` structure to store song metadata, audio links, user ratings, and extracted audio features.\n\n-   **Metadata Extraction Scripts**\\\n    Responsible for writing and maintaining Python scripts to:\n\n    -   Scrape artist genres, countries of origin, and song lists\\\n    -   Retrieve YouTube links\\\n    -   Organize metadata for ingestion and storage\n\n-   **Audio Visualization Interface**\\\n    Developed a custom `.wav/.mp3` player with:\n\n    -   Real-time frequency vs. dB spectrograms\\\n    -   Static visualizations for spectral energy distribution\\\n    -   Interactive time-frequency-dB spectrograms during playback\n\n-   **Demucs Integration and Vocal Separation**\\\n    Writing scripts to:\n\n    -   Apply the Demucs library for isolating vocal tracks\\\n    -   Store separated `.wav` and `.mp3` files in structured folders for analysis\n\n-   **User Interface & Data Display**\\\n    Building HTML/Quarto-styled displays for rendering `.json` metadata and user feedback in a human-readable format.\n\n------------------------------------------------------------------------\n\n**Yashi – Problem #1: Language Recognition from Audio**\n\n-   **Machine Learning Models**\\\n    Leading development of:\n    -   Language classification models using vocal segments\\\n    -   Pipelines for training, validation, and testing\n-   **Audio Feature Extraction**\\\n    Responsible for extracting statistical features such as:\n    -   Zero Crossing Rate (ZCR), Root Mean Square (RMS), tempo, and FFT\\\n    -   Transforming raw waveform data into usable feature vectors\n-   **Recommendation System Components**\\\n    Assisting in the design of:\n    -   Feature-based comparison systems for future personalized recommendations\\\n    -   Methods for encoding user preferences and behavior patterns\n-   **Model Evaluation and Testing**\\\n    Conducting:\n    -   Accuracy and performance evaluations of models\\\n    -   Error analysis using tools such as confusion matrices and cross-validation\n\n------------------------------------------------------------------------\n\n**Joint Responsibilities**\\\nBoth team members will independently construct their machine learning pipelines. Each pipeline will be trained, tested, evaluated, and iteratively improved based on problem-specific goals. Coordination will ensure that feature extraction and data preprocessing remain compatible across both tasks.\n\n### 🗂️ Workflow Plan (Final 2.25 Weeks)\n\n| **Phase** | **Dates** | **Tasks** |\n|--------------------|--------------------------------|--------------------|\n| **Phase 1: Script Finalization & Distribution** | Aug 1 – Aug 3 | \\- Finalize all data scraping & audio processing scripts<br>- Distribute scripts to team members<br>- Confirm runtime & environment setup |\n| **Phase 2: Data Collection & Organization** | Aug 4 – Aug 6 | \\- Each user runs scripts locally<br>- Collect \\~200 `.mp3` and `.wav` files per user<br>- Store audio and metadata in standardized folder structure |\n| **Phase 3: Metadata Processing** | Aug 7 – Aug 8 | \\- Parse, clean, and validate `.json` metadata<br>- Integrate new entries into master metadata files<br>- Ensure feature coverage (e.g., genre, tempo, duration) |\n| **Phase 4: ML Pipeline Construction** | Aug 9 – Aug 10 | \\- Each user builds their custom ML pipeline<br>- Define preprocessing, feature extraction, model architecture |\n| **Phase 5: ML Testing & Iteration – Round 1** | Aug 11 – Aug 13 | \\- Run training and validation pipelines<br>- Tune hyperparameters<br>- Log and assess intermediate results |\n| **Phase 6: ML Testing & Iteration – Round 2** | Aug 14 – Aug 16 | \\- Refine pipelines based on feedback<br>- Add second-tier features (e.g., vocal-only inputs)<br>- Evaluate early model performance |\n| **Phase 7: Final Evaluation & Model Selection** | Aug 17 – Aug 18 | \\- Select best models per user task<br>- Create evaluation reports<br>- Generate confusion matrices, ROC curves, etc. |\n| **Phase 8: Write-Up & Presentation Prep** | Aug 19 – Aug 21 | \\- Complete final Quarto write-up<br>- Polish visualizations and tables<br>- Build and rehearse project presentation |\n\n## Questions\n\n### 1. Language Recognition with Separated Vocal & Audio Tracks\n\nHow can we leverage **statistical** and **time-frequency features** extracted from separated vocal and audio tracks to build effective language recognition models? Specifically, how can traditional machine learning methods — ranging from **classical classifiers** on simple statistical summaries to **Gaussian Mixture Models** on richer time-frequency features — be applied in this context?\n\n-   What are the key **benefits** and **limitations** of these approaches?\\\n-   How can **careful feature engineering**, **feature integration**, and **thorough model evaluation** improve the accuracy and robustness of language recognition systems?\\\n-   How do model results compare and contrast when using **.wav** files versus **.mp3** files?\n\n------------------------------------------------------------------------\n\n### 2. Recommendation Systems Using Audio Features & User Data\n\nHow can **user interaction data**, combined with basic track metadata and simple audio features, be used to build an effective recommendation system using **collaborative filtering** and traditional machine learning methods?\n\n-   Furthermore, how can **advanced audio features**, **dimensionality reduction**, and **clustering techniques** improve personalized recommendations by better capturing user preferences and track characteristics from both vocal and non-vocal components?\\\n-   How do recommendation model results compare and contrast when using **.wav** files versus **.mp3** files, considering the potential impact of audio quality and compression artifacts on feature extraction and recommendation performance?\n\n## Analysis plan\n\n### Preamble: Easy and Medium Paths\n\nThe Easy Path serves as a minimal, foundational implementation aimed at quickly establishing a baseline for language recognition using straightforward statistical features extracted from separated vocal and audio tracks. It relies on classical machine learning models such as Logistic Regression, Random Forest, and Support Vector Machines, which are simple to train and interpret.\n\nThe Medium Path provides a more detailed approach that extends beyond simple statistics by incorporating time-frequency features such as Mel-Frequency Cepstral Coefficients (MFCCs), spectral centroid, and bandwidth from both vocal and non-vocal tracks. Instead of deep learning, this path uses classical probabilistic models like Gaussian Mixture Models (GMMs), Hidden Markov Models (HMMs), or advanced classical classifiers trained on aggregated time-frequency features. This allows capturing richer audio characteristics while maintaining interpretability and computational efficiency.\n\n### Analysis Plan for Problem 1: Language Recognition\n\n### 1. Data Preparation & Feature Extraction - problem 1\n\n-   Load separated vocal and instrumental audio tracks for each sample in both **.wav** and **.mp3** formats.\\\n-   **Easy Path:** Extract statistical features (mean, variance, skewness, kurtosis, RMS energy, zero crossing rate, tempo, loudness) separately from vocal and audio tracks.\\\n-   **Medium Path:** Extract time-frequency features such as MFCCs, spectral centroid, bandwidth, or STFT for vocal and audio tracks. Aggregate these features by computing summary statistics (mean, variance).\\\n-   Normalize numerical features (StandardScaler or Min-Max scaling).\\\n-   Encode categorical metadata if available (e.g., one-hot encoding for language labels or artist).\n\n### 2. Model Construction & Training - problem 1\n\n-   **Easy Path:** Train classical classifiers — Logistic Regression, Random Forest, Support Vector Machines — on statistical feature vectors.\\\n-   **Medium Path:**\n    -   Apply **K-Means clustering** on aggregated time-frequency features to group similar audio patterns unsupervised, enhancing feature representation.\\\n    -   Train classical models suited for time-frequency data — Gaussian Mixture Models (GMMs), Hidden Markov Models (HMMs) (optional), or classical classifiers on combined original and cluster-based features.\\\n-   Perform hyperparameter tuning via grid or random search where applicable.\n\n### 3. Validation - problem 1\n\n-   Use stratified K-fold cross-validation to ensure balanced representation of languages in training and test splits.\\\n-   Evaluate model performance on validation folds.\n\n### 4. Performance Evaluation - problem 1\n\n-   Metrics: Accuracy, Precision, Recall, F1-score.\\\n-   Generate confusion matrices to analyze language-specific errors.\\\n-   Conduct ablation studies comparing vocal-only, audio-only, and combined vocal + audio features.\\\n-   Compare model performance and feature extraction quality between **.wav** and **.mp3** audio formats to assess the impact of audio compression and quality differences on recognition accuracy.\n\n### Putative Machine Learning Techniques - problem 1\n\n-   Logistic Regression, Random Forest, Support Vector Machines (SVM)\\\n-   Gaussian Mixture Models (GMM), Hidden Markov Models (HMM) (optional)\\\n-   K-Means Clustering (unsupervised feature grouping)\\\n-   Feature normalization/scaling (StandardScaler, Min-Max)\\\n-   Encoding categorical features (One-hot, Label Encoding)\\\n-   Stratified K-Fold cross-validation\\\n-   Hyperparameter tuning (Grid Search, Random Search)\\\n-   Evaluation metrics: Accuracy, Precision, Recall, F1-score, Confusion Matrix\\\n-   Ablation analysis for feature contribution\n\n------------------------------------------------------------------------\n\n### Analysis Plan for Problem 2: User Song Recommendation\n\n### 1. Data Preparation & Feature Extraction - problem 2\n\n-   Load user interaction data combined with track metadata (artist, genre, audio features) in both **.wav** and **.mp3** formats.\\\n-   Encode categorical metadata (one-hot, label encoding, or embeddings).\\\n-   Normalize numerical features (Min-Max scaling, StandardScaler).\\\n-   Construct user-item interaction matrix from implicit feedback or ratings.\n\n### 2. Model Construction - problem 2\n\n-   **Easy Path:** Collaborative filtering using K-Nearest Neighbors or Logistic Regression leveraging metadata and user preferences.\\\n-   **Medium Path:**\n    -   Use **K-Means clustering** or Hierarchical clustering on track features to identify similar groups of songs or users unsupervised.\\\n    -   Build tree-based classifiers (Random Forest, Gradient Boosting Machines) on clustered feature groups for content-based filtering.\n\n### 3. Training & Validation - problem 2\n\n-   Train-Test splits or Stratified K-Fold cross-validation.\\\n-   Hyperparameter tuning via grid or random search.\n\n### 4. Performance Evaluation - problem 2\n\n-   Metrics: Precision\\@K, Recall\\@K, Accuracy, F1-score.\\\n-   Offline validation on held-out test sets.\\\n-   Analyze recommendation relevance and diversity.\\\n-   Perform ablation studies comparing models built with vocal-only features vs combined vocal + audio features.\\\n-   Compare recommendation system performance using features extracted from **.wav** versus **.mp3** files to understand the effect of audio quality and compression artifacts.\n\n### Putative Machine Learning Techniques - problem 2\n\n-   Collaborative Filtering: K-Nearest Neighbors, Logistic Regression for implicit feedback\\\n-   Content-Based Filtering: K-Means, Hierarchical Clustering (unsupervised grouping), Random Forest, Gradient Boosting Machines\\\n-   Cross-validation: Train-Test Split, Stratified K-Fold CV\\\n-   Feature Engineering: One-hot encoding, embeddings, normalization (Min-Max, StandardScaler)\\\n-   Evaluation Metrics: Precision\\@K, Recall\\@K, Accuracy, F1-score\\\n-   Ablation Analysis: Assess impact of vocal vs non-vocal feature inclusion\n\n## Repo Oraganization\n\n-   **\\_extra:** Houses supplementary project materials such as problem statements, ML library documentation, and feature lists. Serves as a flexible space for reference materials.\n\n    -   **0_problem_statements：**Contains structured descriptions of the project’s problem statements, including tiered Easy/Medium/Hard pipelines.\n\n    -   **mL_lib_info：**Holds reference documents describing the machine learning techniques considered for the project.\n\n    -   **audio_features_mk1.csv：**A CSV file listing all audio features to be extracted, including definitions, variable types, and extraction methods.\n\n    -   **code.qmd：**A Quarto markdown file containing core code and documentation for the project.\n\n    -   **example_web_site.md:** Example documentation for a project website setup.\n\n    -   **og_data_vars_defs.py:** Python definitions for handling and processing project data variables.\n\n    -   **README.md:** Main project overview, setup instructions, and usage guidelines.\n\n-   **\\_freeze:** Contains frozen Quarto document builds for reproducibility, organized by document (`about`, `index`, `presentation`, `proposal`).\n\n-   **github:** Contains GitHub-specific configuration, including:\n\n    -   **ISSUE_TEMPLATE**: Templates for creating consistent GitHub issues.\n\n    -   **workflows**: GitHub Actions workflows for automation (e.g., building Quarto site).\n\n-   **data:** Houses datasets and associated documentation:\n\n    -   **customtheming.scss**: Custom SCSS styling for the Quarto output.\n\n    -   **README.md**: Data usage description.\n\n-   **docs:** Contains rendered Quarto output for deployment (e.g., GitHub Pages):\n\n    -   **\\_extra**: Supplementary files included in documentation.\n\n    -   **site_libs**: JavaScript and CSS libraries for the generated site.\n\n    -   **index.html**: Rendered project index page.\n\n    -   **proposal.html**: Rendered proposal page.\n\n    -   **search.json**: Search index for the site.\n\n-   **images：**Contains project image assets, including visualizations, diagrams, and decorative images for presentation\n\n-   **presentation_files：**Stores materials supporting the final presentation, such as figures and supplementary assets.\n\n-   **gitignore：**Specifies files and folders to be excluded from Git version control.\n\n-   **about.qmd：**A Quarto document providing background on the project purpose and introducing team members.\n\n-   **index.qmd：**The main Quarto page for the project, containing the core narrative, methodology, code, visualizations, and results.\n\n-   **presentation.qmd：**A Quarto file containing the final presentation slides for the project results.\n\n-   **proposal.qmd：**A Quarto file for the project proposal, including dataset descriptions, problem statements, analysis plan, and weekly plan.\n\n-   **README.md：**The main project README file, summarizing project objectives, setup instructions, and usage guidelines.\n\n::: callout-note\n**Note:** Current Proposal 1.0.0. Subject to change.\\\n(version.feature.patch notation)\n:::\n\n",
    "supporting": [
      "proposal_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}