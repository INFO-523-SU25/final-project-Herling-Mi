---
title: "Audio Alchemy"
subtitle: "INFO 523 - Final Project"
author:
  - name: "Nathan Herling & Yashi Mi "
    affiliations:
      - name: "College of Information Science, University of Arizona"
description: "Project description"
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
jupyter: python3
---

## Abstract

Music recommendation systems increasingly rely on machine learning to capture the complexity of user preferences, yet existing models struggle to account for language diversity and nuanced audio features in songs. This project applies signal processing, vocal separation (DEMUCS library), and machine learning techniques to classify song languages and integrate them with genre metadata for improved personalization. By combining automated data collection with advanced audio analysis, the system provides a foundation for smarter, more inclusive recommendation platforms that enhance user experience across diverse musical contexts. The project first applied Random Forests and Gaussian Mixture Models with 5-fold cross-validation for audio genre identification, then advanced to CNNs on spectrogram heat maps validated via a train/validation/test split with early stopping, evaluated through accuracy, precision, recall, F1-score, confusion matrices, and ROC curves.

::: {style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;"}
<b>Nathan #\_to_Do</b>
:::

(Nathan) - sentence about results.

::: {style="background-color: orange; color: black; margin: 0px 0px 0px 0px;"}
<b>By applying statistical and time-frequency features to separated vocal and instrumental tracks, we evaluated the feasibility of machine learning models in song language recognition. Classical approaches such as Logistic Regression, Random Forests, and SVMs were trained with 5-fold cross-validation. Results showed that vocal-only features provided the strongest signals for classification. The analysis demonstrated that language prediction from raw audio is viable when leveraging targeted feature engineering.</b>
:::

(Yashi) - sentence about Part 2. <br>(Yashi) - sentence about results.

## Introduction

Music genre classification is a central task in the field of music information retrieval, combining elements of signal processing, machine learning, and deep learning. Accurate genre identification not only enhances music recommendation systems and streaming platforms but also deepens our understanding of audio structure and human perception of sound. Traditional approaches have relied on handcrafted audio features analyzed with machine learning techniques such as Random Forests and Gaussian Mixture Models, offering interpretable yet limited performance\[1\]. Recent advances, however, leverage deep learning methods—particularly convolutional neural networks (CNNs)—to extract high-level representations directly from spectrograms, achieving state-of-the-art results\[2\]. This project explores both paradigms: first applying classical machine learning with 5-fold cross-validation, and then advancing to CNN-based classification on spectrogram heat maps, with results evaluated using standard metrics including accuracy, precision, recall, F1-score, confusion matrices, and ROC curves.

## A note on spectographic features of <code>.mp3</code> vs. <code>.wav</code>

::: {style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;"}
<b>Nathan #\_to_Do</b>
:::

Here i'll add a graph, and discuss who despite the large disparity in file size the two file types are nearly similar in audio data. Making it not a good research path to compare .mp3 and .wav files.

## Questions

### 1. Language Recognition with Separated Vocal & Audio Tracks

#### <b>initial formulation</b>

How can we leverage **statistical** and **time-frequency features** extracted from separated vocal and audio tracks to build effective language recognition models? Specifically, how can traditional machine learning methods — ranging from **classical classifiers** on simple statistical summaries to **Gaussian Mixture Models** on richer time-frequency features — be applied in this context?

-   What are the key **benefits** and **limitations** of these approaches?\
-   How can **careful feature engineering**, **feature integration**, and **thorough model evaluation** improve the accuracy and robustness of language recognition systems?\
-   How do model results compare and contrast when using **.wav** files versus **.mp3** files?

#### <b>secondary formulation</b>

::: {style="background-color: orange; color: black; margin: 0px 0px 0px 0px;"}
<b>From the initial formulation, we refined the question to specifically compare how different ablations of the audio track (complete song, vocal-only, and non-vocal) affect model performance.

-   How does model performance differ when predicting song language using features from complete songs, vocal-only tracks, and instrumental-only tracks?

-   What are the relative strengths and limitations of classical machine learning models (Logistic Regression, Random Forest, SVM) when applied to language recognition?

</b>
:::

From the initial problems statement - what did you change?

### 2. Recommendation Systems Using Audio Features & User Data

#### <b>initial formulation</b>

How can **user interaction data**, combined with basic track metadata and simple audio features, be used to build an effective recommendation system using **collaborative filtering** and traditional machine learning methods?

-   Furthermore, how can **advanced audio features**, **dimensionality reduction**, and **clustering techniques** improve personalized recommendations by better capturing user preferences and track characteristics from both vocal and non-vocal components?\
-   How do recommendation model results compare and contrast when using **.wav** files versus **.mp3** files, considering the potential impact of audio quality and compression artifacts on feature extraction and recommendation performance?

#### <b>secondary formulation</b>

::: {style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;"}
<b>Nathan #\_to_Do</b>
:::

From the initial problems statement - what did you change?

## Dataset

### data provenance

::: {style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;"}
<b>Nathan #\_to_Do</b>
:::

### software distriubtion

::: {style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;"}
<b>Nathan #\_to_Do</b>
:::

### data collection

::: {style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;"}
<b>Nathan #\_to_Do</b>
:::

### data storage

::: {style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;"}
<b>Nathan #\_to_Do</b>
:::

### graphs?

::: {style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;"}
<b>Nathan #\_to_Do</b>
:::

<br><b>.. any feature graphs I can think of...</b>

## Team member workload

::: {style="background-color: orange; color: black; margin: 0px 0px 0px 0px;"}
<b>Our project workload followed a structured week-by-week workflow, with responsibilities distributed among team members. We began by finalizing and sharing the proposal, followed by the individual collection and organization of \~200 audio files per person. Nathan Herling led the processing and validation of metadata, while each member focused on building machine learning pipelines and conducting iterative testing. The project concluded with a collaborative effort on final model evaluation, report preparation, and presentation development.</b>
:::

Just type 3-4 sentences about workload go off the proposal 'week work map/individual duties section' - but, put into into paragraph form.

## Problem analysis and results

### General

::: {style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;"}
<b>Nathan #\_to_Do</b>
:::

Discuss what the original plan was, and what was done - in terms of easy, med problem design.

### Q1 - Yashi

:::: {style="background-color: orange; color: black; margin: 0px 0px 0px 0px;"}
<b>**How can we leverage audio features from separated vocal and instrumental tracks to improve language recognition in music?**

**Data Collection:** The dataset consisted of \~200 audio files, preprocessed into three ablations: complete songs, vocal-only tracks, and instrumental-only tracks. Features included time-domain statistics (mean, variance, skewness, kurtosis).

**Data Processing:** All features were standardized using global scaling. Encoded target variable (language) with LabelEncoder.No major imputation was required as missingness was minimal.

**Model Selection:** I evaluated three models: Logistic Regression, Random Forest, and Support Vector Machines with linear kernels. These models were chosen for their balance of interpretability, robustness, and suitability for structured feature data. Training and evaluation were conducted using 5-fold stratified cross-validation to ensure reliable performance comparisons across models.

**Validation & Metrics:** Evaluation focused on accuracy, precision, recall, and F1-score. Confusion matrices were used to analyze per-class misclassification patterns.

**Model Evaluation:**</b>

| Ablation       | Model        | Accuracy | Precision | Recall | F1    |
|----------------|--------------|----------|-----------|--------|-------|
| complete_song  | LogReg       | 0.399    | 0.398     | 0.447  | 0.387 |
| complete_song  | RandomForest | 0.626    | 0.469     | 0.410  | 0.401 |
| complete_song  | SVM_linear   | 0.432    | 0.443     | 0.504  | 0.427 |
| vocal_only     | LogReg       | 0.560    | 0.531     | 0.567  | 0.509 |
| vocal_only     | RandomForest | 0.552    | 0.426     | 0.404  | 0.385 |
| vocal_only     | SVM_linear   | 0.544    | 0.542     | 0.583  | 0.514 |
| no_vocal       | LogReg       | 0.333    | 0.371     | 0.364  | 0.316 |
| no_vocal       | RandomForest | 0.577    | 0.436     | 0.349  | 0.328 |
| no_vocal       | SVM_linear   | 0.366    | 0.418     | 0.411  | 0.347 |
| **Column Min** | \-           | 0.333    | 0.371     | 0.349  | 0.316 |
| **Column Max** | \-           | 0.626    | 0.542     | 0.583  | 0.514 |

-   

    ```{python}
    #| echo: false
    #| fig-cap: ""
    #| fig-width: 7
    #| fig-height: 4
    #| out-width: 70%
    #| fig-align: center
    #| fig-format: svg

    import pandas as pd
    import numpy as np
    import seaborn as sns
    import matplotlib.pyplot as plt
    data = [ {"ablation": "complete_song", "model": "LogReg", "f1": 0.387},
    {"ablation": "complete_song", "model": "RandomForest", "f1": 0.401},
    {"ablation": "complete_song", "model": "SVM_linear", "f1": 0.427},
    {"ablation": "vocal_only", "model": "LogReg", "f1": 0.509},
    {"ablation": "vocal_only", "model": "RandomForest", "f1": 0.385},
    {"ablation": "vocal_only", "model": "SVM_linear", "f1": 0.514},
    {"ablation": "no_vocal", "model": "LogReg", "f1": 0.316},
    {"ablation": "no_vocal", "model": "RandomForest", "f1": 0.328},
    {"ablation": "no_vocal", "model": "SVM_linear", "f1": 0.347}]
    df_f1 = pd.DataFrame(data)
    order_ablation = ["vocal_only", "complete_song", "no_vocal"]
    order_model = ["SVM_linear", "LogReg", "RandomForest"]
    df_f1["ablation"] = pd.Categorical(df_f1["ablation"], categories=order_ablation, ordered=True)
    df_f1["model"] = pd.Categorical(df_f1["model"], categories=order_model, ordered=True)
    plt.figure(figsize=(8,4))
    ax = sns.barplot(data=df_f1, x="ablation", y="f1", hue="model")
    ax.set_xlabel("Track Type (Ablation)")
    ax.set_ylabel("F1 (macro)")
    ax.set_title("F1 (macro) by Track Type and Model")
    plt.show()
    ```

::: {style="background-color: orange; color: black; margin: 0px 0px 0px 0px;"}
<b>**Results:**

-   Vocal-only tracks: Provided the best classification signal, with SVM achieving \~0.51 macro F1, outperforming Random Forest and Logistic Regression.

-   Complete songs: Models achieved moderate performance (\~0.40 F1), reflecting a mixture of useful vocal cues diluted by instrumental content.

-   Non_vocal tracks: Accuracy dropped to \~0.50 (random baseline), validating the expectation that language recognition requires vocal content.
:::
::::

(1) restate question
(2) data collection - data set size/composition (var types) Data was collected through a series of Python scripts. \[see slide 3\] You had an extra layer of feature extraction - removing vocal/instrumental tracts.
(3) data processing - any PCA, correlation, imputation, outlier removal?
(4) model selection - what models, why? (if any reason), what Python libraries did you use?
(5) model validation - what metrics? why?
(6) model evaluation - what metrics? why? **Note: the 'no vocal' track - behaved at about 50% accuracy - which is what you'd expect for a control group.**
(7) future steps/recommendations

### Q2

::: {style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;"}
<b>Nathan #\_to_Do</b>
:::

-   put any graphs in this section.
-   do you have learning curve, ROC curves? Feature importance graphs?

(1) restate question
(2) data collection - data set size/composition (var types) Data was collected through a series of Python scripts. \[see slide 3\] You had an extra layer of feature extraction - removing vocal/instrumental tracts.
(3) data processing - any PCA, correlation, imputation, outlier removal?
(4) model selection - what models, why? (if any reason), what Python libraries did you use?
(5) model validation - what metrics? why?
(6) model evaluation - what metrics? why?
(7) future steps/recommendations

## Results & Conclusion

### Yashi

::: {style="background-color: orange; color: black; margin: 0px 0px 0px 0px;"}
<b>The project goal was to investigate how audio feature engineering can support language classification in music. Our analysis demonstrated that vocal-only features drive most of the predictive signal, confirming that separated vocals provide the strongest basis for language recognition. These results demonstrate the potential of classical machine learning for this task, but also highlight its limits，larger datasets and deep learning methods will be needed for stronger multilingual classification in the future.</b>
:::

Restate the project goal, and the goal of your question. What was done in the analysis, and what was found with the features extracted. (1 paragraph)

::: {style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;"}
<b>Nathan #\_to_Do</b>
:::

Restate the project goal, and the goal of your question. What was done in the analysis, and what was found with the features extracted. (1 paragraph)

## Video links

::: {style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;"}
<b>Nathan #\_to_Do</b>
:::

## Audio Player Demo

::: {style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;"}
<b>Nathan #\_to_Do</b>
:::

## sources

::: {style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;"}
<b>Nathan #\_to_Do</b>
:::

\[1\] https://link.springer.com/chapter/10.1007/978-981-97-4533-3_6

\[2\] https://arxiv.org/html/2411.14474v1