---
title: "Audio Alchemy"
subtitle: "INFO 523 - Final Project"
author:
  - name: "Yashi Mi & Nathan Herling"
    affiliations:
      - name: "College of Information Science, University of Arizona"
description: "Project description"
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
jupyter: python3
---

## Abstract

Music recommendation systems increasingly rely on machine learning to capture the complexity of user preferences, yet existing models struggle to account for language diversity and nuanced audio features in songs. This project applies signal processing, vocal separation (DEMUCS library), and machine learning techniques to classify song languages and integrate them with genre metadata for improved personalization. By combining automated data collection with advanced audio analysis, the system provides a foundation for smarter, more inclusive recommendation platforms that enhance user experience across diverse musical contexts.
The project first applied Random Forests and Gaussian Mixture Models with 5-fold cross-validation for audio genre identification, then advanced to CNNs on spectrogram heat maps validated via a train/validation/test split with early stopping, evaluated through accuracy, precision, recall, F1-score, confusion matrices, and ROC curves.
<div style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;">
  <b>Nathan #_to_Do</b>
</div>
(Nathan) - sentence about results.
<div style="background-color: orange; color: black; margin: 0px 0px 0px 0px;">
  <b>Yashi #_to_Do</b>
</div>
(Yashi) - sentence about Part 2.
<br>(Yashi) - sentence about results.

## Introduction
Music genre classification is a central task in the field of music information retrieval, combining elements of signal processing, machine learning, and deep learning. Accurate genre identification not only enhances music recommendation systems and streaming platforms but also deepens our understanding of audio structure and human perception of sound. Traditional approaches have relied on handcrafted audio features analyzed with machine learning techniques such as Random Forests and Gaussian Mixture Models, offering interpretable yet limited performance[1]. Recent advances, however, leverage deep learning methods—particularly convolutional neural networks (CNNs)—to extract high-level representations directly from spectrograms, achieving state-of-the-art results[2]. This project explores both paradigms: first applying classical machine learning with 5-fold cross-validation, and then advancing to CNN-based classification on spectrogram heat maps, with results evaluated using standard metrics including accuracy, precision, recall, F1-score, confusion matrices, and ROC curves.

## A note on spectographic features of <code>.mp3</code> vs. <code>.wav</code>
<div style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;">
  <b>Nathan #_to_Do</b>
</div>
Here i'll add a graph, and discuss who despite the large disparity in file size the two file types are nearly similar in audio data. Making it not a good research path to compare .mp3 and .wav files.

## Questions
### 1. Language Recognition with Separated Vocal & Audio Tracks

#### <b>initial formulation</b> 
How can we leverage **statistical** and **time-frequency features** extracted from separated vocal and audio tracks to build effective language recognition models? Specifically, how can traditional machine learning methods — ranging from **classical classifiers** on simple statistical summaries to **Gaussian Mixture Models** on richer time-frequency features — be applied in this context?

-   What are the key **benefits** and **limitations** of these approaches?\
-   How can **careful feature engineering**, **feature integration**, and **thorough model evaluation** improve the accuracy and robustness of language recognition systems?\
-   How do model results compare and contrast when using **.wav** files versus **.mp3** files?

#### <b>secondary formulation</b>
<div style="background-color: orange; color: black; margin: 0px 0px 0px 0px;">
  <b>Yashi #_to_Do</b>
</div>
From the initial problems statement - what did you change?

### 2. Recommendation Systems Using Audio Features & User Data

#### <b>initial formulation</b> 
How can **user interaction data**, combined with basic track metadata and simple audio features, be used to build an effective recommendation system using **collaborative filtering** and traditional machine learning methods?

-   Furthermore, how can **advanced audio features**, **dimensionality reduction**, and **clustering techniques** improve personalized recommendations by better capturing user preferences and track characteristics from both vocal and non-vocal components?\
-   How do recommendation model results compare and contrast when using **.wav** files versus **.mp3** files, considering the potential impact of audio quality and compression artifacts on feature extraction and recommendation performance?

#### <b>secondary formulation</b>
<div style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;">
  <b>Nathan #_to_Do</b>
</div>
From the initial problems statement - what did you change?


## Dataset
### data provenance
<div style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;">
  <b>Nathan #_to_Do</b>
</div>
### software distriubtion
<div style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;">
  <b>Nathan #_to_Do</b>
</div>
### data collection
<div style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;">
  <b>Nathan #_to_Do</b>
</div>
### data storage
<div style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;">
  <b>Nathan #_to_Do</b>
</div>
### graphs?
<div style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;">
  <b>Nathan #_to_Do</b>
</div>
<br><b>.. any feature graphs I can think of...</b>

## Team member workload
<div style="background-color: orange; color: black; margin: 0px 0px 0px 0px;">
  <b>Yashi #_to_Do</b>
</div>
 
Just type 3-4 sentences about workload go off the proposal 'week work map/individual duties section' - but, put into into paragraph form.

## Problem analysis and results
### General

<div style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;">
  <b>Nathan #_to_Do</b>
</div>
Discuss what the original plan was, and what was done - in terms of easy, med problem design.

### Q1 - Yashi

<div style="background-color: orange; color: black; margin: 0px 0px 0px 0px;">
  <b>Yashi #_to_Do - entire section for your problem</b>
</div>

- put any graphs in this section.
- do you have learning curve, ROC curves? Feature importance graphs?
(1) restate question
(2) data collection - data set size/composition (var types)
Data was collected through a series of Python scripts.
[see slide 3] You had an extra layer of feature extraction - removing vocal/instrumental tracts.
(3) data processing - any PCA, correlation, imputation, outlier removal?
(4) model selection - what models, why? (if any reason), what Python libraries did you use?
(5) model validation - what metrics? why?
(6) model evaluation - what metrics? why?
**Note: the 'no vocal' track - behaved at about 50% accuracy - which is what you'd expect for a control group.**
(7) future steps/recommendations

### Q2
<div style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;">
  <b>Nathan #_to_Do</b>
</div>
- put any graphs in this section.
- do you have learning curve, ROC curves? Feature importance graphs?
(1) restate question
(2) data collection - data set size/composition (var types)
Data was collected through a series of Python scripts.
[see slide 3] You had an extra layer of feature extraction - removing vocal/instrumental tracts.
(3) data processing - any PCA, correlation, imputation, outlier removal?
(4) model selection - what models, why? (if any reason), what Python libraries did you use?
(5) model validation - what metrics? why?
(6) model evaluation - what metrics? why?
(7) future steps/recommendations


## Results & Conclusion
### Yashi
<div style="background-color: orange; color: black; margin: 0px 0px 0px 0px;">
  <b>Yashi #_to_Do</b>
</div>
Restate the project goal, and the goal of your question. What was done in the analysis, and what was found with the features extracted. (1 paragraph)

<div style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;">
  <b>Nathan #_to_Do</b>
</div>
Restate the project goal, and the goal of your question. What was done in the analysis, and what was found with the features extracted. (1 paragraph)


## Video links
<div style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;">
  <b>Nathan #_to_Do</b>
</div>


## Audio Player Demo
<div style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;">
  <b>Nathan #_to_Do</b>
</div>

## sources
<div style="background-color: #4CBB17; color: black; margin: 0px 0px 0px 0px;">
  <b>Nathan #_to_Do</b>
</div>

[1] https://link.springer.com/chapter/10.1007/978-981-97-4533-3_6

[2] https://arxiv.org/html/2411.14474v1
